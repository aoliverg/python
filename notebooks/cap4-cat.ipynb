{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Anàlisi textual i processament de corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Accés als corpus de l'NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’NLTK es distribueix amb una gran quantitat de corpus. Tot i que una gran part d’aquests corpus estan en anglès, n’hi ha també en altres llengües, com el català i el castellà. La llista completa i actualitzada dels corpus disponibles a l'NLTK es pot trobar en el següent enllaç: http://www.nltk.org/nltk_data/\n",
    "\n",
    "Per exemple, NLTK inclou una selecció de textos del Projecte Gutenberg. Aquest projecte recopila llibres en format electrònic (llibres en domini públic, és a dir, sense drets d'autor i per tant legalment descarregables). Si volem carregar aquest corpus i observar els arxius que conté, podem escriure el següent codi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "llibres=gutenberg.fileids()\n",
    "print(llibres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem accedir a les paraules d'un determinat llibre de la col·lecció fent el següent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "mobi=nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "print(mobi)\n",
    "print(len(mobi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amb el mètode words hem pogut accedir a les paraules del llibre (que em emmagatzemat a la variable mobi). A la darrera línia hem comptat les posicions de la llista mobi, és a dir, el nombre de paraules del llibre. En corpus de text com aquest podem accedir a:\n",
    "\n",
    "- al text en brut, amb el mètode raw\n",
    "- els paràgrafs, amb el mètode paras\n",
    "- a les oracions, amb el mètode sents\n",
    "- a les paraules, amb el mètode words\n",
    "\n",
    "Al programa següent (programa-4-1.py), podem observar el funcionament d'aquests mètodes (quan l'executis, per passar d'un nivell al següent, pitja la tecla enter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"TEXT EN BRUT\")\n",
    "a=input()\n",
    "\n",
    "text_en_brut=nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "print(text_en_brut[0:100000])\n",
    "\n",
    "print(\"PARAGRAFS\")\n",
    "a=input()\n",
    "\n",
    "paragrafs=nltk.corpus.gutenberg.paras(\"melville-moby_dick.txt\")\n",
    "for para in paragrafs:\n",
    "    print(para)\n",
    "\n",
    "print(\"ORACIONS\")\n",
    "a=input()\n",
    "\n",
    "oracions=nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")\n",
    "for oracio in oracions:\n",
    "    print(oracio)\n",
    "\n",
    "print(\"PARAULES\")\n",
    "a=input()\n",
    "\n",
    "paraules=nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "for paraula in paraules:\n",
    "    print(paraula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amb a=input() el sistema espera que l'usuari introdueixi algun valor i l'emmagatzema en a (que no fem servir per res). El que aconseguim és que el sistema s'esperi fins que l'usuari premi la tecla Enter.\n",
    "\n",
    "Per ara hem estat treballant amb un corpus textual pla, sense cap tipus d'anàlisi ni anotació. Més endavant veurem que NLTK també proporciona corpus amb diversos nivells d'anàlisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Accés a corpus propis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK proporciona una bona col·lecció de corpus, però amb tota seguretat necessitarem també treballar amb corpus propis. Per fer això podem fer servir les instruccions que ja sabem per obrir i llegir arxius de text (mireu el apartat 2.3), o bé podrem fer servir els diferents lectors de corpus que proporciona NLTK. \n",
    "\n",
    "El lector de corpus més elemental que proporciona l'NLTK és el PlaintextCorpusReader, que serveis, com indica el seu nom, per llegir corpus que estiguin en format de text pla (és a dir, només el text, sense cap tipus d'anotació). En el programa-4-2.py es pot observar el funcionament bàsic. En aquest cas també farem servir la novel·la Moby Dick, descarregada del Projecte Gutenberg i que podem trobar en els fitxers d'aquest capítol (i caldrà que el tingueu en la mateix directori que aquest Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTEU AQUESTA CEL·LA NOMÉS SI ESTEU TREBALLANT AMB GOOGLE COLAB. \n",
    "#Us demanarà que trieu l'arxiu mobi_dick.txt del vostre ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "corpus = PlaintextCorpusReader(\".\", 'mobi_dick.txt')\n",
    "for oracio in corpus.sents():\n",
    "    print(oracio)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Com podem observar, a la sortida tindrem les oracions, que de fet, si us fixeu, són llistes de paraules:\n",
    "\n",
    "...\n",
    "['In', 'a', 'word', ',', 'the', 'whale', 'was', 'seized', 'and', 'sold', ',', 'and', 'his', 'Grace', 'the', 'Duke', 'of', 'Wellington', 'received', 'the', 'money', '.']\n",
    "...\n",
    "\n",
    "El PlaintextCorpusReader carrega el fitxer de text i du a terme un procés de segmentació en paràgrafs (a partir de salts de paràgrafs), en oracions (fent servir un segmentador determinat) i en paraules (o tokens, fent servir un tokenitzador determinat). Si no indique res en el moment de cridar al PlaintextCorpusReader fa servir:\n",
    "\n",
    "Tokenitzador: word_tokenizer=WordPunctTokenizer(),\n",
    "Segmentador: sent_tokenizer=nltk.data.LazyLoader('tokenizers/punkt/english.pickle') És a dir, assumeix per defecte que la llengua del corpus és l'anglès.\n",
    "\n",
    "Com que el tokenitzador que fa servir és molt simple i es basa en separar el text en seqüències de caràcters alfabetics i no alfabètics fent servir l'expressió regular \\w+|[^\\w\\s]+ es poden produir errors, com el següent:\n",
    "\n",
    "['\"', 'Won', \"'\", 't', 'the', 'Duke', 'be', 'content', 'with', 'a', 'quarter', 'or', 'a', 'half', '?\"']\n",
    "\n",
    "on won't s'ha separat com \n",
    "\n",
    "won\n",
    "'\n",
    "t\n",
    "\n",
    "Fixem-nos ara que si apliquem aquest mateix programa a un corpus en català la tokenització no serà totalment correcta. Amb aquest capítol es distribueix un fragment del Corpus del Diari Oficial de la Generalitat de Cataluna (corpus DOGC), concretament el corresponent a la versió catalana de l'any 2015. Si carreguem el fitxer DOGC-2015-cat.txt, en comptes de mobi_dick.txt obtenim un resultat com el següent (mostrem únicament un fragment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTEU AQUESTA CEL·LA NOMÉS SI ESTEU TREBALLANT AMB GOOGLE COLAB. \n",
    "#Us demanarà que trieu l'arxiu DOGC-2015-cat.txt del vostre ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt')\n",
    "for oracio in corpus.sents():\n",
    "    print(oracio)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si observem el resultat veurem errors com els següents:\n",
    "\n",
    "...\n",
    "['El', 'ple', 'de', 'l', \"'\", 'Ajuntament', 'd', \"'\", 'Òrrius', 'a', 'la', 'sessió', 'extraordinària', 'celebrada', 'el', 'dia', '20', 'de', 'gener', 'de', '2015', 'va', 'aprovar', 'inicialment', 'el', 'projecte', 'd', \"'\", 'obra', \n",
    "...\n",
    "\n",
    "Fixem-nos que, per exemple, els apòstrofs (') queden com a tokens aïllats, separats de l'article o preposició corresponent.\n",
    "\n",
    "En aquest mateix capítol, en els apartats 4.4 i 4.5, tractem més a fons la segmentació en unitats lèxiques (tokenització) i la segmentació en oracions. El que sí que avancem ara és que al PlaintextCorpusReader se li pot indicar quin tokenitzador i segmentador ha de fer servir. Això es pot fer de la següent manera (programa-4-3.py)(necessitarei descarregar l'arxiu catalan.pickle que teniu en la secció d'arxius d'aquest capítol i posar-lo en el mateix directori que aquest notebook):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTEU AQUESTA CEL·LA NOMÉS SI ESTEU TREBALLANT AMB GOOGLE COLAB. \n",
    "#Us demanarà que trieu l'arxiu catalan.pickle del vostre ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "for oracio in corpus.sents():\n",
    "    print(oracio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Fixeu-vos que hem definit un segmentador (que carreguem de catalan.pickle) i un tokenitzador basat en expressions regulars, i que fem servir aquests nous elements quan creem el PlaintextCorpusReader. Ara hem arreglat l'aspecte dels apòstrofs en la tokenització i la sortida (mostrem només un fragment) és:\n",
    "\n",
    "['El', 'ple', 'de', \"l'\", 'Ajuntament', \"d'\", 'Òrrius', 'a', 'la', 'sessió', 'extraordinària', 'celebrada', 'el', 'dia', '20', 'de', 'gener', 'de', '2015', 'va', 'aprovar', 'inicialment', 'el', 'projecte', \"d'\", 'obra', '.']\n",
    "\n",
    "En el apartat 4.4 veurem amb més detall el tema de les expressions regulars. Comparem ara les expressions regulars del tokenitzador per defecte:\n",
    "\n",
    "\\w+|[^\\w\\s]+\n",
    "\n",
    "i el  que hem creat nosaltres:\n",
    "\n",
    "[ldsmLDSM]\\'|\\w+|[^\\w\\s]+\n",
    "\n",
    "Hem afegit la part [ldsmLDSM]\\' que defineix els tokens formats per: l', d', s', m', L', D', S', M'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Ocurrències (tokens) i tipus (types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificant molt lleugerament el programa-4-3.py, per tal que ens doni totes les paraules (o tokens) en comptes de totes les oracions i perquè ens proporcioni el nombre total de paraules (programa-4-4.py):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "#for paraula in corpus.words():\n",
    "    #print(paraula)\n",
    "print(\"TOTAL PARAULES:\",len(corpus.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El programa s'està una bona estona mostrant paraules per pantalla (en aquesta versió per al notebook de jupyter hem comentat les línies on escriu les paraules i simplement mostarà el nombre total de paraules) i després ens dóna el nombre de paraules totals:\n",
    "\n",
    "...\n",
    "5524\n",
    ",\n",
    "d'\n",
    "11\n",
    ".\n",
    "12\n",
    ".\n",
    "2009\n",
    "),\n",
    "TOTAL PARAULES: 2448870\n",
    "\n",
    "Si miréssim amb deteniment la llista veuríem que moltes de les paraules ocorren més d'un cop (en l'apartat 4.6 veurem com calcular la freqüència i la distribució de freqüències de les paraules). Si ara el que volem és obtenir una llista de paraules diferents, podem fer servir la instrucció set, com podem observar en el programa-4-5.py:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "ocurrencies=corpus.words()\n",
    "tipus=set(ocurrencies)\n",
    "\n",
    "print(\"OCURRENCIES:\",len(ocurrencies))\n",
    "print(\"TIPUS:\",len(tipus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ens dóna a la sortida:\n",
    "\n",
    "OCURRENCIES: 2448870\n",
    "TIPUS: 34872\n",
    "\n",
    "Per resumir podem dir que les ocurrències són el nombre total de paraules que apareixen en el corpus i els tipus el nombre de paraules diferents que apareixen en el corpus.\n",
    "\n",
    "Cal tenir en compte, però,que no podem parlar estrictament de paraules, ja que també s’inclouen els signes de puntuació, les xifres, etc.\n",
    "\n",
    "Podem calcular un índex de riquesa lèxica dividint el nombre d’ocurrències entre el nombre de tipus. Ho veiem al programa-4-6.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "ocurrencies=corpus.words()\n",
    "tipus=set(ocurrencies)\n",
    "riquesalexica=len(ocurrencies)/len(tipus)\n",
    "\n",
    "print(\"OCURRENCIES:\",len(ocurrencies))\n",
    "print(\"TIPUS:\",len(tipus))\n",
    "print(\"RIQUESA LÊXICA:\",round(riquesalexica,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ens dóna a la sortida:\n",
    "\n",
    "OCURRENCIES: 2448870\n",
    "TIPUS: 34872\n",
    "RIQUESA LÊXICA: 70.22\n",
    "\n",
    "Cosa que indica que cada paraules de mitjana es fa servir 70 vegades. Experimenta una mica amb diferents tipus de text per veure com varia aquest índex de riquesa lèxica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Tokenització"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segmentació en unitats lèxiques o tokenització, consisteix a dividir el text en unitats més petites (que sovint coincideixen amb paraules). Tot i que es tracta d'una tasca molt bàsica i necessària per a poder portar a terme tasques d’anàlisi més avançades, aquesta tasca presenta nombrosos problemes que no són fàcils de solucionar. Hi ha nombrosos treballs sobre aquesta àrea entre els quals es poden destacar els treballs de Grefenstette i Tapanainen (1994) i Mikheev (2002). Sigui com sigui, actualment es pot considerar que aquesta tasca es resol de manera satisfactòria i no hi ha una recerca activa per millorar-la. En aquest apartat aprendrem algunes tècniques per a segmentar el text en unitats lèxiques i anirem observant els diferents problemes que hi apareixen i com es poden solucionar. Les proves dels diferents sistemes els farem al l'oracio:\n",
    "\n",
    "El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\n",
    "\n",
    "Tot i que el resultat desitjat de la tokenització pot dependre de les tasques concretes, la sortida desitjada del nostre sistema hauria de ser una cosa del següent estil:\n",
    "\n",
    "['El','Sr.','Martínez','arribarà','demà','d'','Alacant','amb','la','R.E.N.F.E','a','les','22.30','h.',i','s'','haurà','d'','allotjar','a','l'','hotel','de',' l'','estació',.']\n",
    "\n",
    "El primer tokenitzador que provarem fa servir la instrucció split(), que divideix una cadena segons el separador que s'indiqui, i si no s'indica res, per espais. El podem veure al programa-4-7.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokens=oracio.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que ens dóna la següent sortida, que no és exactament la que volíem:\n",
    "\n",
    "['El', 'Sr.', 'Martínez', 'arribarà', 'demà', \"d'Alacant\", 'amb', 'la', 'R.E.N.F.E.', 'a', 'les', '22.30', 'h.', 'i', \"s'haurà\", \"d'allotjar\", 'a', \"l'hotel\", 'de', \"l'estació.\"]\n",
    "\n",
    "NLTK proporciona una sèrie de tokenitzadors que presentem a continuació:\n",
    "\n",
    "#### WhitespaceTokenizer\n",
    "\n",
    "Separa el text per espais en blanc, com ho hem fet en l'exemple anterior. En aquest cas els espais en blanc poden ser els caràcters: espai en blanc, tabulador i nova línia. Veiem-ho (programa-4-8.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokens=nltk.tokenize.WhitespaceTokenizer().tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que dóna com a sortida:\n",
    "\n",
    "['El', 'Sr.', 'Martínez', 'arribarà', 'demà', \"d'Alacant\", 'amb', 'la', 'R.E.N.F.E.', 'a', 'les', '22.30', 'h.', 'i', \"s'haurà\", \"d'allotjar\", 'a', \"l'hotel\", 'de', \"l'estació.\"]\n",
    "\n",
    "\n",
    "Aprofito aquest exemple per explicar diverses maneres d'importar un tokenitzador, o qualsevol mètode t'una classe determinada. en el cas anterior hem importat tot l'nltk i hem cridat al mètode fent:\n",
    "\n",
    "tokens=nltk.tokenize.WhitespaceTokenizer().tokenize(oracio)\n",
    "\n",
    "Això també es pot fer de la següent manera alternativa (programa-4-8b.py)(fixeu-vos com accedim ara al mètode tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokens=WhitespaceTokenizer().tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i també de la següent manera, donant un nom a la classe que pot ser qualsevol)(programa-4-8c.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer as tokenitzador\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokens=tokenitzador().tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaceTokenizer\n",
    "\n",
    "És igual que l'anterior, però en aquest cas l'únic caràcter que es té en compte és el d'espai en blanc (\" \"). Equival a split(\" \"). En el nostre exemple la sortida seria exactament la mateixa i no cal proporcionar ni el codi ni el programa.\n",
    "\n",
    "#### TreebankWordTokenizzer\n",
    "\n",
    "Aquest tokenitzador fa servir les convencions del Penn Treebank corpus, qué és un corpus anotat de l'anglès creat als any 1980 a partir d'articles del Wall Street Journal. Com que és per a l'anglès, no funcionarà del tot bé per al català i per això en l'exemple poso una oració de l'anglès:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as tokenitzador\n",
    "\n",
    "oracio=\"We need to conduct an assessment to learn whether a student's dificulties are because he or she can't or won't complete assignments.\"\n",
    "tokens=tokenitzador().tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i la sortida serà:\n",
    "\n",
    "['We', 'need', 'to', 'conduct', 'an', 'assessment', 'to', 'learn', 'whether', 'a', 'student', \"'s\", 'dificulties', 'are', 'because', 'he', 'or', 'she', 'ca', \"n't\", 'or', 'wo', \"n't\", 'complete', 'assignments', '.']\n",
    "\n",
    "Aquest tokenitzador es fa servir força per a l'anglès, i per aquest moti s'ha creat una funció específica que fa de wrapper (wrapper function) per simplificar el seu ús (programa-4-9b.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "oracio=\"We need to conduct an assessment to learn whether a student's dificulties are because he or she can't or won't complete assignments.\"\n",
    "tokens=word_tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que proporciona exactament la mateixa sortida.\n",
    "\n",
    "#### RegexpTokenizer\n",
    "\n",
    "A l'apartat anterior (4.3. Ocurrències (tokens) i tipus (types)) ja vam veure un exemple del tokenitzador per expressions regulars. Aquest és un tipus de tokenitzador que ens permet un control total sobre el procés de tokenització. Clar que per treure'n el màxim profit cal dominar les expressions regulars de Python. Podeu trobar una explicació detallada a Regular Expression HOWTO i també un bon resum aquí.\n",
    "\n",
    "Veiem ara alguns exemples, començant pel programa-4-10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ens ofereix la següent sortida:\n",
    "\n",
    "['El', 'Sr', '.', 'Martínez', 'arribarà', 'demà', \"d'\", 'Alacant', 'amb', 'la', 'R', '.', 'E', '.', 'N', '.', 'F', '.', 'E', '.', 'a', 'les', '22', '.', '30', 'h', '.', 'i', \"s'\", 'haurà', \"d'\", 'allotjar', 'a', \"l'\", 'hotel', 'de', \"l'\", 'estació', '.']\n",
    "\n",
    "Com podem observar, ens separa Sr del punt (.) i realment voldríem tenir Sr. com a token, i el mateix passa amb h.. També tokenitza incorrectamente R.E.N.F.E. Podríem solucionar això modificant l'expressió regular (programa-4-10.py) de manera que afegim aquestes dues unitats com a tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokenitzador=RegexpTokenizer('Sr\\.|h\\.|R\\.E\\.N\\.F\\.E\\.|[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que ens retornaria la tokenització correcta d'aquestes dues unitats:\n",
    "\n",
    "['El', 'Sr', '.', 'Martínez', 'arribarà', 'demà', \"d'\", 'Alacant', 'amb', 'la', 'R.E.N.F.E.', 'a', 'les', '22', '.', '30', 'h', '.', 'i', \"s'\", 'haurà', \"d'\", 'allotjar', 'a', \"l'\", 'hotel', 'de', \"l'\", 'estació', '.']\n",
    "\n",
    "El fet d'afegir una llista d'abreviatures corrents és força habitual, però no podem esperar tenir una llista prou completa d'acrònims. Per aquest motiu, hem d'intentar expressar els acrònims d'una manera més general, com per exemple la que presentem al programa-4-10c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokenitzador=RegexpTokenizer('[A-Z\\.]{2,}|Sr\\.|h\\.|[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que ens ofereix la mateixa sortida però que ara contempla qualsevol acrònim tipus R.E.N.F.E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per defecte, el tokenitzador per expressions regulars considera que el que expressem són els tokens, però podem definir el que són els separadors de tokens amb el modificador gaps=True. En el programa-4-10.py podem veure un exemple simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracio=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\"\n",
    "tokenitzador=RegexpTokenizer('\\s+',gaps=True)\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que es dóna la següent sortida (que coincideix amb la del tokenitzador per espais en blanc):\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "['El', 'Sr.', 'Martínez', 'arribarà', 'demà', \"d'Alacant\", 'amb', 'la', 'R.E.N.F.E.', 'a', 'les', '22.30', 'h.', 'i', \"s'haurà\", \"d'allotjar\", 'a', \"l'hotel\", 'de', \"l'estació.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Segmentació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la secció anterior hem vist com separar el text en unitat lèxiques, procés que també rep el nom de tokenització. En aquesta secció veurem com podem separar un paràgraf en segments, que són unitats semblants a oracions. Aquest procés rep el nom de segmentació.\n",
    "\n",
    "En aquesta secció farem servir dos segments de prova, un que no presentarà grans problemes:\n",
    "\n",
    "Avui fa un dia molt bonic. Demà l'Albert anirà a dinar a casa.\n",
    "\n",
    "i un altre que sí que en presenta.\n",
    "\n",
    "El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació. L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo. La tornada la farà en avió en el vol AF.352.\n",
    "\n",
    "#### PunktSentenceTokenizer\n",
    "\n",
    "Aquest és un segmentador senzill que bàsicament segmenta per punts. Per veure com funciona provem el programa-4-11.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "paragraf1=\"Avui fa un dia molt bonic. Demà l'Albert anirà a dinar a casa.\"\n",
    "paragraf2=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació. L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo. La tornada la farà en avió en el vol AF.352.\"\n",
    "segmentador=PunktSentenceTokenizer()\n",
    "segments1=segmentador.tokenize(paragraf1)\n",
    "print(segments1)\n",
    "segments2=segmentador.tokenize(paragraf2)\n",
    "print(segments2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que proporciona la següent sortida:\n",
    "\n",
    "['Avui fa un dia molt bonic.', \"Demà l'Albert anirà a dinar a casa.\"]\n",
    "['El Sr.', \"Martínez arribarà demà d'Alacant amb la R.E.N.F.E.\", \"a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\", \"L'endemà visitarà al Dr.\", \"Rovira a l'Av.\", 'Tibidabo.', 'La tornada la farà en avió en el vol AF.352.']\n",
    "\n",
    "#### sent_tokenize()\n",
    "\n",
    "El procés de segmentació també es pot fer amb sent_tokenizer(), que crida a una instància especial del PunktSenteceTokenizer que ha estat entrenada i que funciona força bé per a diverses llengües europees. Es tracta d'una implementació de l'algorisme de Kiss and Strunk (2006). Veiem-ho al programa-4-11b.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraf1=\"Avui fa un dia molt bonic. Demà l'Albert anirà a dinar a casa.\"\n",
    "\n",
    "paragraf2=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació. L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo. La tornada la farà en avió en el vol AF.352.\"\n",
    "\n",
    "segments1=sent_tokenize(paragraf1)\n",
    "print(segments1)\n",
    "segments2=sent_tokenize(paragraf2)\n",
    "print(segments2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ens ofereix una sortida molt millor, tot i que no perfecta:\n",
    "\n",
    "['Avui fa un dia molt bonic.', \"Demà l'Albert anirà a dinar a casa.\"]\n",
    "[\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E.\", \"a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\", \"L'endemà visitarà al Dr. Rovira a l'Av.\", 'Tibidabo.', 'La tornada la farà en avió en el vol AF.352.']\n",
    "\n",
    "#### Carregar un segmentador concret per a una llengua determinada\n",
    "\n",
    "Amb les dades de l'NLTK es distribueixen una sèrie de segmentadors per a llengües determinades. Concretament es distrueixen els següents: txec, finlandès, noruec, espanyol, danès, francès, polonès, suec, holandès, alemany, portuguès, turc, anglès, grec, estonià, italià, eslovè. No es distribueix un per al català, però en aquesta mateixa secció aprendrem a crear un d'específic per al català. En el programa-4-12.txt carreguem un d'específic per a l'anglès:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "paragraf=\"Today Mr. Smith and Ms. Johanson will meet at St. Patrick church.\"\n",
    "\n",
    "segmentador=nltk.data.load(\"tokenizers/punkt/PY3/english.pickle\")\n",
    "segments=segmentador.tokenize(paragraf)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que proporciona la sortida:\n",
    "\n",
    "['Today Mr. Smith and Ms. Johanson will meet at St. Patrick church.']\n",
    "\n",
    "#### Entrenament d'un tokenitzador\n",
    "\n",
    "L'algorisme de Kiss and Strunk (2006) permet entrenar un segmentador a partir de text sense cap tipus d'anotació. NLTK implementa aquest algorisme. Anem a aprendre un segmentador per al català (de fet crear un catalan.pickle) a partir del corpus de DOGC corresponent a l'any 2015. El programa-4-13.py implementa aquest aprenentatge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize.punkt\n",
    "import pickle\n",
    "import codecs\n",
    "segmentador = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "text = codecs.open(\"DOGC-2015-cat.txt\",\"r\",\"utf8\").read()\n",
    "segmentador.train(text)\n",
    "out = open(\"catalan.pickle\",\"wb\")\n",
    "pickle.dump(segmentador, out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearem el catalan.pickle que podrem fer servir com a segmentador en el programa-4-12b.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "paragraf=\"Today Mr. Smith and Ms. Johanson will meet at St. Patrick church.\"\n",
    "\n",
    "segmentador=nltk.data.load(\"catalan.pickle\")\n",
    "paragraf1=\"Avui fa un dia molt bonic. Demà l'Albert anirà a dinar a casa.\"\n",
    "\n",
    "paragraf2=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació. L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo. La tornada la farà en avió en el vol AF.352.\"\n",
    "\n",
    "segments1=segmentador.tokenize(paragraf1)\n",
    "print(segments1)\n",
    "segments2=segmentador.tokenize(paragraf2)\n",
    "print(segments2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ofereix la següent sortida:\n",
    "\n",
    "['Avui fa un dia molt bonic.', \"Demà l'Albert anirà a dinar a casa.\"]\n",
    "[\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E.\", \"a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\", \"L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo.\", 'La tornada la farà en avió en el vol AF.352.']\n",
    "\n",
    "Funciona del tot correctament el segmentador que hem entrenat?\n",
    "\n",
    "#### Personalitzar el segmentador\n",
    "\n",
    "Podem personalitzar el segmentador entrenat per afegir noves abreviatures o acrònims que no han estat detectats en el procés d'entrenament. Ho podem fer específicament per a un programa determinat, com en el programa-4-12.c, on carreguem el catalan.pickle que hem entrenat i afegim R.E.N.F.E. (fixeu-vos que ho fem en minúscules i sense el punt final):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer,PunktParameters\n",
    "paragraf=\"Today Mr. Smith and Ms. Johanson will meet at St. Patrick church.\"\n",
    "\n",
    "segmentador=nltk.data.load(\"catalan.pickle\")\n",
    "abreviacions_extra = ['r.e.n.f.e']\n",
    "segmentador._params.abbrev_types.update(abreviacions_extra)\n",
    "paragraf1=\"Avui fa un dia molt bonic. Demà l'Albert anirà a dinar a casa.\"\n",
    "\n",
    "paragraf2=\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació. L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo. La tornada la farà en avió en el vol AF.352.\"\n",
    "\n",
    "segments1=segmentador.tokenize(paragraf1)\n",
    "print(segments1)\n",
    "segments2=segmentador.tokenize(paragraf2)\n",
    "print(segments2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i ara la sortida ja segmenta millor:\n",
    "\n",
    "[\"El Sr. Martínez arribarà demà d'Alacant amb la R.E.N.F.E. a les 22.30 h. i s'haurà d'allotjar a l'hotel de l'estació.\", \"L'endemà visitarà al Dr. Rovira a l'Av. Tibidabo.\", 'La tornada la farà en avió en el vol AF.352.']\n",
    "\n",
    "En els fitxers adjunts podeu trobar una llista d'abreviacions. Ara el que farem serà carregar el catalan.pickle que hem entrenat i modificar-lo afegint la llista d'abreviatures i acrònims del fitxer. Gravarem aquest nou segmentador com a catalan-mod.pickle. (programa-4-14.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTEU AQUESTA CEL·LA NOMÉS SI ESTEU TREBALLANT AMB GOOGLE COLAB. \n",
    "#Us demanarà que trieu l'arxiu abreviatures.txt del vostre ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import codecs\n",
    "import pickle\n",
    "segmentador=nltk.data.load(\"catalan.pickle\")\n",
    "fitxer_abreviacions=codecs.open(\"abreviatures.txt\",\"r\",encoding=\"utf-8\")\n",
    "abreviacions_extra =[]\n",
    "for abreviacio in fitxer_abreviacions.readlines():\n",
    "    abreviacio=abreviacio.rstrip()\n",
    "    abreviacions_extra.append(abreviacio)\n",
    "segmentador._params.abbrev_types.update(abreviacions_extra)\n",
    "out = open(\"catalan-mod.pickle\",\"wb\")\n",
    "pickle.dump(segmentador, out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiqueu el programa-4-12b.py per a que carregui aquest nou segmentador i verifiqueu si funciona correctament.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Freqüències i distribució de freqüències"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest apartat aprendrem a fer alguns càlculs senzills sobre corpus: freqüències absolutes i freqüències relatives, distribucions de freqüència i a trobar les col.locacions més freqüents d’un corpus.\n",
    "\n",
    "### 4.6.1. Freqüència absoluta\n",
    ".\n",
    "Entenem per freqüència absoluta el nombre total de vegades que apareix una determinada unitat lèxica en el nostre corpus.\n",
    "\n",
    "El càlcul de la freqüència absoluta d’una paraula és senzill: podem utilitzar un diccionari per a posar com a clau les paraules i anar incrementant el valor del diccionari cada cop que apareix la paraula. En el programa se-\n",
    "güent (programa-4-15.py) podem veure una implementació senzilla d’aquesta idea:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Freqüències i distribució de freqüències\n",
    "\n",
    "En aquest apartat aprendrem a fer alguns càlculs senzills sobre corpus: freqüències absolutes i freqüències relatives, distribucions de freqüència i a trobar les col.locacions més freqüents d’un corpus.\n",
    "\n",
    "### 4.6.1. Freqüència absoluta\n",
    ".\n",
    "Entenem per freqüència absoluta el nombre total de vegades que apareix una determinada unitat lèxica en el nostre corpus.\n",
    "\n",
    "El càlcul de la freqüència absoluta d’una paraula és senzill: podem utilitzar un diccionari per a posar com a clau les paraules i anar incrementant el valor del diccionari cada cop que apareix la paraula. En el programa se-\n",
    "güent (programa-4-15.py) podem veure una implementació senzilla d’aquesta idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "frequencia={}\n",
    "\n",
    "for paraula in corpus.words():\n",
    "    frequencia[paraula]=frequencia.get(paraula,0)+1\n",
    "\n",
    "for clau in frequencia.keys():\n",
    "    print(frequencia[clau],clau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i per pantalla ens mostrarà les paraules i les freqüències, però d'una manera desordenada, ja que els diccionaris de python són estructures de dades sense un ordre determinat.\n",
    "\n",
    "NLTK proporciona una funció FreqDist que facilita molt el càlcul de freqüències.Veiem el seu ús en el programa-4-16.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "frequencia=FreqDist(corpus.words())\n",
    "\n",
    "for mc in frequencia.most_common():\n",
    "    print(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara sí que ens proporciona les paraules i la freqüència de cada paraula endreçada de més freqüent a menys freqüent. Com que n'hi ha moltes, podem modificar fàcilment el programa perquè ens proporcioni les 25 més freqüents, canviant la línia:\n",
    "\n",
    "for mc in frequencia.most_common():\n",
    "\n",
    "per\n",
    "\n",
    "for mc in frequencia.most_common(25):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "frequencia=FreqDist(corpus.words())\n",
    "\n",
    "for mc in frequencia.most_common(25):\n",
    "    print(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2. Freqüència relativa\n",
    "\n",
    "La freqüència absoluta d’una paraula en un determinat corpus no ens dóna informació real sobre si la paraula és molt freqüent o no, perquè això dependrà de la mida del corpus. Que una paraula aparegui, diguem-ne, 22 vegades en\n",
    "en nostre corpus, no ens diu res, ja que si el corpus és molt gran potser aquest valor de freqüència és petit.\n",
    "\n",
    "La freqüència relativa d’una paraula en un corpus és el nombre de vegades que hi apareix dividida pel nombre total de paraules en el corpus.\n",
    "\n",
    "FreqDist ens facilita molt el càlcul de la freqüència relativa ja que la podem consultar amb el mètode freq(). Ho podem veure en el programa-4-17.py, que és una modificació de l'anterior. Ara guardem al fitxer frequencies.txt les paraules endreçades per freqüència i mostrem la freqüència aobsoluta i la relativa de cada paraula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import codecs\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "frequencia=FreqDist(corpus.words())\n",
    "\n",
    "sortida=codecs.open(\"frequencies.txt\",\"w\",encoding=\"utf-8\")\n",
    "\n",
    "for mc in frequencia.most_common():\n",
    "    paraula=mc[0]\n",
    "    frequencia_absoluta=mc[1]\n",
    "    frequencia_relativa=frequencia.freq(paraula)\n",
    "    cadena=str(frequencia_absoluta)+\"\\t\"+str(frequencia_relativa)+\"\\t\"+paraula\n",
    "    print(cadena)\n",
    "    sortida.write(cadena+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I la sortida (mostrem només les 25 primeres):\n",
    "\n",
    "228896 0.09347004945137961 de\n",
    "\n",
    "139175 0.05683233491365405 ,\n",
    "\n",
    "85450 0.0348936448239392 .\n",
    "\n",
    "78342 0.03199108160090164 la\n",
    "\n",
    "63452 0.02591072617166285 d'\n",
    "\n",
    "54381 0.022206568743951292 i\n",
    "\n",
    "51190 0.0209035187657981 l'\n",
    "\n",
    "47788 0.01951430659855362 a\n",
    "\n",
    "44098 0.018007489168473622 del\n",
    "\n",
    "37175 0.015180470992743592 /\n",
    "\n",
    "28550 0.011658438381784252 :\n",
    "\n",
    "27148 0.011085929428675267 que\n",
    "\n",
    "26976 0.011015692952259614 el\n",
    "\n",
    "26544 0.010839285058006345 per\n",
    "\n",
    "25021 0.010217365560442164 les\n",
    "\n",
    "22050 0.00900415293584388 en\n",
    "\n",
    "18132 0.007404231339352436 2015\n",
    "\n",
    "15739 0.006427045943639311 -\n",
    "\n",
    "15648 0.006389885947396146 al\n",
    "\n",
    "14972 0.006113840261018347 )\n",
    "\n",
    "14063 0.005742648650193763 es\n",
    "\n",
    "13936 0.005690787996096159 (\n",
    "\n",
    "12529 0.005116237284951835 amb\n",
    "\n",
    "11781 0.004810790282865158 s'\n",
    "\n",
    "10635 0.004342819341165517 Catalunya\n",
    "\n",
    "### 4.6.3. La llei de Zipf\n",
    "\n",
    "La Llei de Zipf afirma que donat un corpus, la freqüència d’una paraula és inversament proporcional a la seva posició a la taula de freqüències (rank).\n",
    "\n",
    "Amb la seva llei, Zipf (1949) afirma que hi ha una constant k que es pot calcular multiplicant la freqüència de qualsevol paraula per la seva posició a la taula (rank) (k = f · r).\n",
    "\n",
    "En el programa següent (programa-4-18.py) avaluem la llei de Zipf amb les 50 paraules més freqüents del corpus Cess_cat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_cat\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "paraules=cess_cat.words()\n",
    "freqdist=nltk.FreqDist(paraules)\n",
    "sortida=codecs.open(\"sortida.txt\",\"w\", encoding=\"utf-8\")\n",
    "\n",
    "posicio=0\n",
    "p = re.compile('\\w+')\n",
    "for mc in freqdist.most_common(50):\n",
    "    if p.match(mc[0]):\n",
    "        posicio+=1\n",
    "        freq=mc[1]\n",
    "        fxr=posicio*freq\n",
    "        cadena=mc[0]+\"\\t\"+str(freq)+\"\\t\"+str(posicio)+\"\\t\"+str(fxr)\n",
    "        print(cadena)\n",
    "        sortida.write(cadena+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el fitxer sortida.txt podem observar les paraules amb la seva freqüència, posició i resultat del producte de la freqüencia i la posició (que tendeix a ser constant):\n",
    "\n",
    "de 23684 1 23684\n",
    "\n",
    "la 15695 2 31390\n",
    "\n",
    "que 12703 3 38109\n",
    "\n",
    "i 11819 4 47276\n",
    "\n",
    "el 9749 5 48745\n",
    "\n",
    "a 9568 6 57408\n",
    "\n",
    "l' 8892 7 62244\n",
    "\n",
    "d' 6970 8 55760\n",
    "\n",
    "del 5826 9 52434\n",
    "\n",
    "en 5783 10 57830\n",
    "\n",
    "per 5378 11 59158\n",
    "\n",
    "un 5247 12 62964\n",
    "\n",
    "les 5219 13 67847\n",
    "\n",
    "va 5217 14 73038\n",
    "\n",
    "ha 5114 15 76710\n",
    "\n",
    "els 4207 16 67312\n",
    "\n",
    "una 3895 17 66215\n",
    "\n",
    "amb 3755 18 67590\n",
    "\n",
    "es 3265 19 62035\n",
    "\n",
    "al 2843 20 56860\n",
    "\n",
    "no 2646 21 55566\n",
    "\n",
    "El 2551 22 56122\n",
    "\n",
    "dels 2445 23 56235\n",
    "\n",
    "s' 2327 24 55848\n",
    "\n",
    "és 2062 25 51550\n",
    "\n",
    "Tot i que la llei només mostra una tendència i no és exacta, recalca el fet que en un corpus hi ha molt poques paraules molt freqüents i moltes paraules poc freqüents. En el programa següent (programa-4-19.py) grafiquem aquest fenomen amb un subconjunt del corpus Cess_cat de 1.000 paraules. Com podem observar al gràfic (el principi esmentat, és a dir, que molt poques paraules apareixen moltes vegades i que moltes apareixen poques vegades, s’acompleix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_cat\n",
    "import pylab\n",
    "paraules=cess_cat.words()\n",
    "paraules1=paraules[:1000]\n",
    "freqdist=nltk.FreqDist(paraules1)\n",
    "freqdist.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
