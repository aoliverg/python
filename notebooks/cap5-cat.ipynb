{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Etiquetatge morfosintàctic\n",
    "\n",
    "## Introducció\n",
    "\n",
    "En aquest mòdul estudiarem la tasca anomenada etiquetatge morfosintàctic (en anglès part-of-speech tagging o POS-tagging). Aquesta tasca consisteix a assignar a cada paraula d'un text una categoria gramatical i altra informació addicional (com poden ser diverses subcategories, el lema associat, etc.) Aquesta és una tasca fonamental en el processament del llenguatge natural, encara que no està exempta de problemes que no estan encara totalment resolts. El llenguatge natural és ambigu des de molts punts de vista, i també ho és en el morfosintàctic. Una determinada forma (com ara casa) pot tenir diverses interpretacions morfosintàctiques, pot ser un substantiu comú femení singular (amb lema casa) i també una forma de present o d'imperatiu del verb casar. Els etiquetadors morfosintàctics hauran d'intentar donar la interpretació adequada segons el context d'utilització; per tant hauran de desambiguar les diferents possibilitats.\n",
    "\n",
    "Abans d'abordar l'etiquetatge morfosintàctic veurem també l'anàlisi morfològica, és a dir, l'anàlisi que ens permet determinar el lema i la categoria gramatical (i altres subcategorizaciones) d'una determinada forma. Un cop siguem capaços de fer l'anàlisi morfològica, passarem a estudiar diverses tècniques que ens permetran etiquetar textos des del punt de vista morfosintàctic i que intentaran desambiguar (amb més o menys èxit) les diferents possibilitats.\n",
    "\n",
    "# # 5.1. Morfologia computacional\n",
    "\n",
    "### 5.1.1. El formalisme de descomposició morfològica\n",
    "\n",
    "Hi ha tot un seguit de formalismes per descriure la morfologia d'una llengua. En aquest capítol només presentarem un, que és senzill d'implementar i ens servirà per comprendre els mecanismes fonamentals: el formalisme de descomposició morfològica (Alshawi, 1992). La idea bàsica d'aquest formalisme és senzilla i es basa en dos tipus de coneixement:\n",
    "\n",
    "Un diccionari que conté informació morfosintàctica sobre la base o la paraula que es considera forma de referència.\n",
    "Regles que contenen informació sobre la morfologia de la llengua.\n",
    "En el programa-5-1.py podem veure una primera implementació molt senzilla d'aquesta idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrel=\"cant\"\n",
    "t1=\"o\"\n",
    "t2=\"es\"\n",
    "t3=\"a\"\n",
    "t4=\"em\"\n",
    "t5=\"eu\"\n",
    "t6=\"en\"\n",
    "\n",
    "print(arrel+t1)\n",
    "print(arrel+t2)\n",
    "print(arrel+t3)\n",
    "print(arrel+t4)\n",
    "print(arrel+t5)\n",
    "print(arrel+t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest programa definim una sèrie de variables: una que conté el lema d’un verb i altres que contenen les terminacions de present d’indicatiu. Mitjançant l’operador + que concatena cadenes obtenim les formes corresponents al present d'indicatiu.\n",
    "\n",
    "Seguint aquesta idea bàsica idearem un formalisme per expressar les regles (que a més de les formes volem que ens donin una etiqueta de categoria gramatical i subcategoritzacions). També idearem un formalisme per expressar el diccionari de lemes, que a més del propi lema ens indicarà el tipus de flexió que segueix (és a dir, el paradigma flexiu).\n",
    "\n",
    "Com a formalisme per a les regles proposem una sèrie de valors separats per dos punts (:):\n",
    "```\n",
    "terminació_forma:terminació_lema:etiqueta:paradigma\n",
    "```\n",
    "\n",
    "Per exemple:\n",
    "```\n",
    "o:ar:VMIP1S:V1\n",
    "es:ar:VMIP2S:V1\n",
    "a:ar:VMIP3S:V1\n",
    "em:ar:VMIP1P:V1\n",
    "eu:ar:VMIP2P:V1\n",
    "en:ar:VMIP3P:V1\n",
    "```\n",
    "\n",
    "Per les etiquetes fem servir les etiquetes EAGLES per al català. Al fitxer regles.txt podem observar un conjunt més extens de regles morfològiques.\n",
    "\n",
    "Com a formalisme per al diccionari de lemes seguirem una idea similar:\n",
    "\n",
    "```\n",
    "lema:paradigma\n",
    "```\n",
    "\n",
    "Que en l'exemple seria:\n",
    "```\n",
    "cantar:V1\n",
    "```\n",
    "Al fitxer diccionari.txt podem observar un diccionari més complet.\n",
    "\n",
    "Ara necessitem un programa que ens permeti generar totes les formes a partir de les regles i el diccionari (programa-5-2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTA AQUESTA CEL·LA NOMÉS SI ESTÀS TREBALLANT EN GOOGLE COLAB.\n",
    "#Et demanarà que seleccionis l'arxiu regles.txt del teu ordinador i el pujarà l'entorn Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "#Et demanarà que seleccionis l'arxiu diccionari.txt del teu ordinador i el pujarà l'entorn Google Colab.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fregles=open(\"regles.txt\",\"r\")\n",
    "\n",
    "regles=[]\n",
    "\n",
    "while True:\n",
    "\tlinia=fregles.readline().rstrip()\n",
    "\tif not linia:break\n",
    "\tregles.append(linia)\n",
    "fregles.close()\n",
    "\n",
    "fdiccionari=open(\"diccionari.txt\",\"r\")\n",
    "\n",
    "while True:\n",
    "\tlinia=fdiccionari.readline().rstrip()\n",
    "\tif not linia:break\n",
    "\t(lema,tipus)=linia.split(\":\")\n",
    "\tfor regla in regles:\n",
    "\t\t(tf,tl,etiqueta,tipus2)=regla.split(\":\")\n",
    "\t\tif ((tipus2 == tipus)&(lema.endswith(tl))):\n",
    "\t\t\tprint(lema[0:(len(lema)-len(tl))]+tf,lema,etiqueta)\n",
    "\n",
    "fdiccionari.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estem parlant d'anàlisi morfològica, però en realitat el que hem fet és un programa que genera formes amb el seu lema i una etiqueta morfosintàctica, el que anomenem diccionari morfològic. Els programes d'anàlisi morfològica sovint funcionen amb un diccionari morfològic.\n",
    "\n",
    "Al següent programa (programa-5-3.py) carreguem aquest diccionari i duem a terme l'anàlisi de les paraules que indica l'usuari (el programa finalitza quan l'usuari introdueix un espai en blanc). El diccionari que hem creat en el programa anterior el podeu trobar en els arxius del capítol (diccionari-curt-cat.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTA AQUESTA CEL·LA NOMÉS SI ESTÀS TREBALLANT EN GOOGLE COLAB.\n",
    "#Et demanarà que seleccionis l'arxiu diccionari-curt-cat.txt del teu ordinador i el pujarà l'entorn Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "diccionari={}\n",
    "arxiu_diccionari=codecs.open(\"diccionari-curt-cat.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in arxiu_diccionari:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\" \")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    diccionari[forma]=diccionari.get(forma,\"\")+\":\"+lema+\"\\t\"+etiqueta\n",
    "    \n",
    "while 1:\n",
    "    paraula=input()\n",
    "    if paraula==\" \":\n",
    "        break\n",
    "    if paraula in diccionari:\n",
    "        print(\"ANALISI:\",paraula,diccionari[paraula])\n",
    "    else:\n",
    "        print(\"PARAULA DESCONEGUDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Anàlisi morfologica\n",
    "\n",
    "En aquest apartat construirem un analitzador morfològic, és a dir, un programa que és capaç de donar-nos les anàlisis morfològiques de totes les paraules d'un text. Per a les paraules ambigües des del punt de vista morfosintàctic, ens retornarà totes les possibles interpretacions. Amb el que hem fet fins ara tenim tots els components:\n",
    "\n",
    "- Un programa que carregui el diccionari morfològic del català (el programa-5-3.py)\n",
    "- Un programa capaç de llegir un document i segmenar-lo en oracions i tokenitzar-lo (per exemple, el programa-4-3.py)\n",
    "\n",
    "Al programa-5-4.py podem observar una primera versió del programa (que analitzarà l'arxiu noticia.txt, que conté un fragment de notícia publicada al diari Ara). Utilitza també el catalan-mod.pickle que hem creat al capítol anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTA AQUESTA CEL·LA NOMÉS SI ESTÀS TREBALLANT EN GOOGLE COLAB I \n",
    "#NO HAS PUJAT ENCARA l'arxiu diccionari-cat.txt\n",
    "#Et demanarà que seleccionis l'arxiu diccionari-cat.txt del teu ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTA AQUESTA CEL·LA NOMÉS SI ESTÀS TREBALLANT EN GOOGLE COLAB.\n",
    "#Et demanarà que seleccionis l'arxiu noticia.txt del teu ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload() \n",
    "#Després et demanarà que seleccis l'arxiu catalan-mod.pickle i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "diccionari={}\n",
    "arxiu_diccionari=codecs.open(\"diccionari-cat.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in arxiu_diccionari:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\" \")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    if forma in diccionari:\n",
    "        diccionari[forma]=diccionari.get(forma,\"\")+\" \"+lema+\" \"+etiqueta\n",
    "    else:\n",
    "        diccionari[forma]=lema+\" \"+etiqueta\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan-mod.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'noticia.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "for forma in corpus.words():\n",
    "    if forma in diccionari:\n",
    "        info=diccionari[forma]\n",
    "    else:\n",
    "        info=\"DESCONEGUDA\"\n",
    "    print(forma+\" \"+info) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com veiem, hi ha alguns problemes, com per exemple:\n",
    "\n",
    "- Les paraules que estan a la primera posició d'una oració, i per tant estan escrites en majúscules, tot i estar al diccionari, les etiqueta com a desconegudes. Per solucionar això, primer buscarem al diccionari les paraules tal i com apareixen al text, si no les troba, llavors les buscarà passada a minúscula, i si tot i així no la troba, la marcarpa com a desconeguda.\n",
    "- Els signes de puntuació els marca com a desconeguts, ja que són tokens que no apareixen al diccionari morfològic. La solució és senzilla i consisteix a incloure els signes de puntuació al diccionari morfològic. Al diccionari2-cat.txt hem afegit les següents entrades:\n",
    "```\n",
    "\" \" Fe\n",
    "\n",
    "' ' Fe\n",
    "\n",
    ". . Fp\n",
    "\n",
    ", , Fc\n",
    "\n",
    "; ; Fx\n",
    "\n",
    ": : Fd\n",
    "\n",
    "( ( Fpa\n",
    "\n",
    ") ) Fpt\n",
    "\n",
    "[ [ Fca\n",
    "\n",
    "] ] Fct\n",
    "```\n",
    "\n",
    "Veiem la nova implementació al programa-5-5.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "diccionari={}\n",
    "arxiu_diccionari=codecs.open(\"diccionari2-cat.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in arxiu_diccionari:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\" \")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    if forma in diccionari:\n",
    "        diccionari[forma]=diccionari.get(forma,\"\")+\" \"+lema+\" \"+etiqueta\n",
    "    else:\n",
    "        diccionari[forma]=lema+\" \"+etiqueta\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan-mod.pickle\")\n",
    "tokenitzador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'noticia.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "for forma in corpus.words():\n",
    "    if forma in diccionari:\n",
    "        info=diccionari[forma]\n",
    "    elif forma.lower() in diccionari:\n",
    "        info=diccionari[forma.lower()]\n",
    "    else:\n",
    "        info=\"DESCONEGUDA\"\n",
    "    print(forma+\" \"+info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encara queden problemes per resoldre, com els noms propis, algun aspecte malament tokenitzat, etc. També poden aparèixer paraules desconegudes  que siguin correctes, però que no estiguin recollides al diccionari morfològic, etc. En propers apartats d'aquest mateix capítol anirem presentant solucions a aquestes qüestions.\n",
    "\n",
    "## 5.3. Etiquetatge morfosintàctic\n",
    "\n",
    "### 5.3.1. Introducció\n",
    "\n",
    "En aquest mòdul estudiarem la tasca anomenada etiquetatge morfosintàctic (en anglès part-of-speech tagging o POS-tagging). Aquesta tasca consisteix a assignar a cada paraula d’un text una categoria gramatical i altra informació addicional (com poden ser diverses subcategories, el lema associat, etc.) Aquesta és una tasca fonamental en el processament del llenguatge natural, tot i que no està exempta de problemes que no estan encara totalment resolts. \n",
    "\n",
    "El llenguatge natural és ambigu des de molts punts de vista, i també ho és en el morfosintàctic. Una determinada forma (com per exemple casa) pot tenir diverses interpretacions morfosintàctiques, pot ser un substantiu comú femení singular (amb lema casa) i també una forma de present o d’imperatiu del verb casar. Els etiquetadors morfosintàctics hauran d’intentar donar la interpretació adequada segons el context on apareix una determinada paraula; per tant hauran de desambiguar les diferents possibilitats.\n",
    "\n",
    "En el mòdul veurem diverses tècniques que ens permetran etiquetar textos des del punt de vista morfosintàctic i que intentaran desambiguar (amb major o menor èxit) les diferents possibilitats.\n",
    "\n",
    "### 5.3.2. Etiquetatge morfosintàctic vs. anàlisi morfològica\n",
    "\n",
    "A l'apartat anterior van estudiar la tasca anomenada anàlisi morfològica, que consisteix a assignar a cada paraula d'un text totes les possibles anàlisis morfològiques.\n",
    "\n",
    "Un analitzador morfosintàctic ofereix la mateixa sortida, però desambiguada, és a dir, tria una de les possibilitats de cada paraula. Veiem ara l'anàlisi morfosintàctica feta per Freeling de la mateixa oració:\n",
    "\n",
    "L’etiquetatge morfosintàctic és una tasca bàsica per a moltes tasques de processament del llenguatge natural. Si volem fer una anàlisi sintàctica d’una oració, un pas previ és conèixer la categoria gramatical de cada paraula. Disposar de textos etiquetats a escala morfosintàctica és interessant per a molts estudis. Podem saber quins són els substantius més utilitzats en un corpus, veure totes les aparicions d’un verb independentment de la forma concreta, etc. L’etiquetatge morfosintàctic també es fa servir per a extreure els termes més rellevants d’un determinat document o conjunt de documents. L’etiquetatge també es fa servir per a la classificació de documents i recuperació d’informació.\n",
    "\n",
    "### 5.3.4. Etiquetador per a l'anglès\n",
    "\n",
    "NLTK proporciona un etiquetador per a l'anglès que funciona prou bé i que es pot fer servir fàcilment de manera directa, com al programa-5-6.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "oracion=\"They refuse to permit us to obtain the refuse permit\"\n",
    "palabras = nltk.tokenize.word_tokenize(oracion)\n",
    "analisis=nltk.pos_tag(palabras)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixeu-vos que la paraula permit l'ha etiquetat correctament com a verb i com a substantiu. Fixeu-vos que les etiquetes les dona amb un etiquetari (tagset), o conjunt d'etiquetes, determinat. En aquest cas concret es fan servir el tagset WSJ (Wall Street Journal), que és el següent:\n",
    "\n",
    "Tag - Function\n",
    "\n",
    "CC - coordinating conjunction\n",
    "\n",
    "CD - cardinal number\n",
    "\n",
    "DT - determiner\n",
    "\n",
    "EX - existential there\n",
    "\n",
    "FW - foreign word\n",
    "\n",
    "IN - preposition\n",
    "\n",
    "JJ - adjective\n",
    "\n",
    "JJR - adjective, comparative\n",
    "\n",
    "JJS - adjective, superlative\n",
    "\n",
    "MD - modal\n",
    "\n",
    "NN - non-plural common noun\n",
    "\n",
    "NNP - non-plural proper noun\n",
    "\n",
    "NNPS - plural proper noun\n",
    "\n",
    "NNS - plural common noun\n",
    "\n",
    "of - the word of\n",
    "\n",
    "PDT - pre-determiner\n",
    "\n",
    "POS - posessive\n",
    "\n",
    "PRP - pronoun\n",
    "\n",
    "puncf - final punctuation (period, question mark and exclamation mark)\n",
    "\n",
    "punc - other punction\n",
    "\n",
    "RB - adverb\n",
    "\n",
    "RBR - adverb, comparative\n",
    "\n",
    "RBS - adverb, superlative\n",
    "\n",
    "RP - particle\n",
    "\n",
    "TO - the word to\n",
    "\n",
    "UH - interjection\n",
    "\n",
    "VB - verb, base form\n",
    "\n",
    "VBD - verb, past tense\n",
    "\n",
    "VBG - verb, gerund or present participle\n",
    "\n",
    "VBN - verb, past participle\n",
    "\n",
    "VBP - verb, non-3rd person\n",
    "\n",
    "VBZ - verb, 3rd person\n",
    "\n",
    "WDT - wh-determiner\n",
    "\n",
    "WP - wh-pronoun\n",
    "\n",
    "WRB - wh-adverb\n",
    "\n",
    "sym - symbol\n",
    "\n",
    "2 - ambiguously labelled\n",
    "\n",
    "Com veurem quan comencem a construir etiquetadors per a altres llengües, com el català, les etiquetes que farem servir seran diferents. Hi ha una proposta d'etiquetari universal (universal tagset). A continuació podem observar aquest etiquetari.\n",
    "\n",
    "Tag Meaning - English Examples\n",
    "\n",
    "ADJ - adjective - new, good, high, special, big, local\n",
    "\n",
    "ADP - adposition - on, of, at, with, by, into, under\n",
    "\n",
    "ADV - adverb - really, already, still, early, now\n",
    "\n",
    "CONJ - conjunction - and, or, but, if, while, although\n",
    "\n",
    "DET - determiner, article - the, a, some, most, every, no, which\n",
    "\n",
    "NOUN - noun - year, home, costs, time, Africa\n",
    "\n",
    "NUM - numeral - twenty-four, fourth, 1991, 14:24\n",
    "\n",
    "PRT - particle - at, on, out, over per, that, up, with\n",
    "\n",
    "PRON  - pronoun - he, their, her, its, my, I, us\n",
    "\n",
    "VERB - verb - is, say, told, given, playing, would\n",
    "\n",
    ". - punctuation marks-  . , ; !\n",
    "\n",
    "X - other - ersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "NLTK ens permet convertir les etiquetes d'un determinat etiquetari al l'etiquetari universal. Ho podem veure al programa-5-6b.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "oracion=\"They refuse to permit us to obtain the refuse permit\"\n",
    "palabras = nltk.tokenize.word_tokenize(oracion)\n",
    "analisis=nltk.pos_tag(palabras)\n",
    "for ana in analisis:\n",
    "    forma=ana[0]\n",
    "    etiqueta=ana[1]\n",
    "    universal=nltk.tag.mapping.map_tag('en-ptb', 'universal', etiqueta)\n",
    "    print(forma,etiqueta,universal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.4.1. L'etiquetador per defecte\n",
    "\n",
    "En aquest apartat presentem un etiquetador molt simple, que l’únic que fa és etiquetar totes les paraules amb una etiqueta determinada, és a dir, etiqueta totes les paraules amb la mateixa etiqueta. Per determinar quina etiqueta triarem el que farem primer és calcular quina és l'etiqueta més freqüent. Per a aconseguir això farem ús de corpus ja etiquetats: per a l'anglès farem servir el Brown Corpus i per al català el CESS_CAT.\n",
    "\n",
    "Per a l'anglès:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tags=[tag for (word,tag) in brown.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I per definir un etiquetador per defecte fem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=word_tokenize(oracio)\n",
    "default_tagger=nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com podeu observar, etiqueta totes les paraules amb 'NN'.\n",
    "\n",
    "Per al català:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_cat\n",
    "tags=[tag for (word,tag) in cess_cat.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ens pot sorprendre que l'etiqueta més freqüent en català sigui la corresponent a la preposició ja que esperaríem que fos també la corresponent a substantiu. El que passa és que en l'etiquetari català tenin posicions per la categoria i diverses subcategoritzacions (gènere i nombre). Si modifiquem les dues darreres línies per fer que miri només la primera posició de l'etiqueta, obtindrem que la categoria més freqüent és 'n' (substantiu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[tag[0] for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríem fer també un etiquetador per defecte del català indicant que l'etiqueta per defecte fons 'n'. Però realment preferiríem indicar l'etiqueta completa més freqüent per als substantius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[tag for (word,tag) in cess_cat.tagged_words()]\n",
    "nltk.FreqDist(tags).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ens dona que els noms comuns femenins singulars són lleugerament més freqüents que els noms comuns masculins singulars. Ara podem definir un etiquetador per defecte per al català:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "oracio=\"Avui fa sol però demà plourà\"\n",
    "tokens=word_tokenize(oracio)\n",
    "default_tagger=nltk.DefaultTagger('ncfs000')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja ens podem imaginar que aquest etiquetador no funcionarà gaire bé. En el següent apartat aprendrem a avaluar etiquetadors i podrem veure la precisió d'aquest etiquetador. També pot servir pels casos que tinguem una paraula desconeguda ja que li podrem assignar l'etiqueta més freqüent (sense comptar la de preposició, ja que sent una categoria tancada és pràcticament impossible que sigui desconeguda). \n",
    "\n",
    "### 5.4.2. L'etiquetador per unigrames\n",
    "\n",
    "L’etiquetador per defecte estudiat en l'apartat anterior etiqueta totes les paraules amb l’etiqueta més freqüent en tot el corpus. En aquest apartat i els propers estudiarem una sèrie d’etiquetadors anomenats genèricament etiquetadors per n-grames. Un n-grama és una combinació d'n elements. En general aquests etiquetadors aprenen a partir del corpus tenint en compte un context de n paraules. En el cas de l’etiquetador per unigrames l’únic que tenim en compte és la paraula per etiquetar mateixa, sense cap context. L'etiquetador per unigrames etiquetarà cada paraula amb l'etiqueta més frequent per a aquella paraula. \n",
    "\n",
    "Amb l’NLTK crear un etiquetador per unigrames és molt senzill. Primer ho farem per a l’anglès i després per al català.\n",
    "\n",
    "Per a l'anglès"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents=brown.tagged_sents()\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=nltk.word_tokenize(oracio)\n",
    "unigram_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ens fixem en el resultat, veiem que refuse l'etiqueta les dues vegades que apareix com a verb, ja que aquesta és l'etiqueta més freqüent per a aquesta paraula.\n",
    "\n",
    "Per al català\n",
    "\n",
    "Farem el mateix, però aquesta vegada en un programa (programa-5-7.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_cat\n",
    "from nltk.tokenize import word_tokenize\n",
    "tagged_sents=cess_cat.tagged_sents()\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"avui fa sol però demà plourà\"\n",
    "tokens=word_tokenize(oracio)\n",
    "analisi=unigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També podem entrenar un etiquetador amb un corpus propi. Ara farem servir el Wikicorpus, que és un corpus format per textos de la Vikipèdia i etiquetats amb Freeling. Com que aquest corpus ja té uns anys, descarregarem els textos i el tornarem a etiquetar amb una versió més nova de Freeling. Podeu descarregar aquest corpus dels fitxers d'aquest capítol (recordeu descomprimir-lo un cop descarregat). Observem el format del corpus:\n",
    "\n",
    "```\n",
    "S' es P0300000 0.999814\n",
    "hi hi PP3CN000 1\n",
    "poden poder VMIP3P0 0.470339\n",
    "representar representar VMN0000 1\n",
    "nombres nombre NCMP000 0.454513\n",
    "enters enter AQ0MP0 0.366457\n",
    "o o CC 0.999266\n",
    "decimals decimal AQ0CP0 0.566499\n",
    ". . Fp 1\n",
    "```\n",
    "\n",
    "És a dir, forma, lema, etiqueta i probabilitat de l'etiqueta. Haurem de crear un codi capaç de llegir aquest corpus i crear les tagged_sents com a llistes de tagged_words on cada tagged_word és una tupla forma, etiqueta. Ho podem veure al programa-5-8.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTA AQUESTA CEL·LA NOMÉS SI ESTÀS TREBALLANT EN GOOGLE COLAB I\n",
    "#NO HAS PUJAT ENCARA L'ARXIU catalanTagged_0_5000-utf-8.txt\n",
    "#ET demanarà que seleccionis l'arxiu catalanTagged_0_5000-utf-8.txt del teu ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"catalanTagged_0_5000-utf-8.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"avui fa sol però demà plourà\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=unigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3. L'etiquetador per bigrames\n",
    "\n",
    "L'etiquetador per unigrames no té en compte el context d'aparició de les paraules, i les etiqueta sempre amb l'etiqueta més freqüent per a aquesta paraula. En aquest apartat construirem un etiquetador per bigrames que té en compte la pròpia paraula a etiquetar i la paraula anterior.\n",
    "\n",
    "Per a l'anglès:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents=brown.tagged_sents()\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents)\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=word_tokenize(oracio)\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "bigram_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixem-nos que ara pot etiquetar el segon refuse com a nom, ja que té en compte el context immediat.\n",
    "\n",
    "Per al català\n",
    "\n",
    "Modifiquem el programa-5-8.py per obtenir el programa-5-9.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"catalanTagged_0_5000-utf-8.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents)\n",
    "oracio=\"avui fa sol però demà plourà\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ens fixem en la sortida veurem que no ha pogut etiquetar cap paraula. Això és degut a un fenomen conegut com a dispersió de dades. Hi ha molts més unigrames que bigrames en un corpus. Si entrenem un etiquetador amb bigrames, necessitarem un corpus més gran per poder trobar suficients evidències de cada bigrama de l'oracio a analitzar, com que no sempre és possible disposar de grans corpus, es pot recórrer a la tècnica coneguda com a backoff. A aquesta tècnica també se la coneix com a combinació d'etiquetadors. Al programa-5-10.py combinem un etiquetador de bigrames amb un d'unigrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"catalanTagged_0_5000-utf-8.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "oracio=\"avui fa sol però demà plourà\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i com veiem, ha pogut etiquetar totes les paraules, ja que amb l'etiquetador de bigrames no ha pogut, però sí fent servir el d'unigrames.\n",
    "\n",
    "\n",
    "\n",
    "### 5.4.5. El etiquetador por trigramas\n",
    "\n",
    "El etiquetador por trigramas etiqueta una palabra teniendo en cuenta el contexto formado por las dos palabras anteriores. En este caso, el problema de la dispersión de datos será aún más pronunciado. En el programa-5-11.py implementamos un etiquetador por trigramas que se combina con uno por bigramas y a su vez por uno por unigramas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(tagged_sents,backoff=bigram_tagger)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.5. Tractament de paraules desconegudes: diccionari morfològic, etiquetador per afixos i etiquetador per defecte\n",
    "En el programa anterior hem vist que un etiquetador entrenat amb un corpus no serà capaç d'etiquetar paraules que no apareguin en el corpus. Per solucionar això farem servir 3 estratègies:\n",
    "\n",
    "- entrenar un etiquetador per unigrames a partir d'un diccionari morfològic, que en aquest cas serà el del propi Freeling i que podeu descarregar d'aquest apartat.\n",
    "- entrenar un etiquetador per afixos que el que fa és fer servir totes les paraules del diccionari morfològic (i en el seu defecte podria fer-se servir les pròpies paraules del corpus), per aprendre les etiquetes més freqüents segons les terminacions de les paraules.\n",
    "- i si tot això falla, fer servir un etiquetador per defecte, que faci servir l'etiqueta més freqüent en el nostre corpus d'aprenentatge\n",
    "\n",
    "Veiem primer aquests tres entrenaments per separat i després ho combinarem tot en un únic etiquetador.\n",
    "\n",
    "Etiquetador per unigrames a partir d'un diccionari morfològic\n",
    "\n",
    "Farem servir aquesta part de codi (programa-5-12.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓ: EXECUTA AQUESTA CEL·LA NOMÉS SI ESTÀS TREBALLANT EN GOOGLE COLAB I\n",
    "#NO HAS PUJAT ENCARA L'ARXIU diccionari-cat.txt\n",
    "#Et demanarà que seleccionis l'arxiu diccionari-cat.txt del teu ordinador i el pujarà a l'entorn de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"diccionari-cat.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    cont+=1\n",
    "    if cont==10000:\n",
    "        break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\" \")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    tupla=(forma,etiqueta)\n",
    "    tagged_words.append(tupla)\n",
    "tagged_sents.append(tagged_words)\n",
    "\n",
    "unigram_tagger_diccionari=nltk.UnigramTagger(tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixeu-vos que contem les línies i quan arriba a la 10000 parem l'entrenament per a que no trigui massa. Per fer les proves serà suficient i en el moment de fer l'entrenament definitiu eliminarem (comentarem) aquestes línies de codi de manera que faci l'entrenament amb tot el diccionari. Aquest programa no ofereix cap sortida.\n",
    "\n",
    "Entrenament d'un etiquetador per afixos utilitzant el diccionari morfològic\n",
    "\n",
    "Podem trobar la implementació en el programa-5-13.py que és exactament igual que l'anterior però canvia la darrera línia i ara és:\n",
    "\n",
    "```\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents, affix_length=-3, min_stem_length=2)\n",
    "```\n",
    "\n",
    "que permet entrenar l'etiquetador per afixos tenint en compte els afixos amb tres caràcters sempre i quan l'arrel que quedi tingui 2 o més caràcters.\n",
    "\n",
    "Determinació de l'etiqueta més freqüent i entrenament de l'etiquetador per defecte\n",
    "\n",
    "El programa-5-14.py ens retorna les 10 etiquetes més freqüents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"catalanTagged_0_5000-utf-8.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "tags=[]\n",
    "for ts in tagged_sents:\n",
    "    for wt in ts:\n",
    "        tags.append(wt[1])\n",
    "\n",
    "mft=nltk.FreqDist(tags).most_common(10)\n",
    "print(\"Etiqueta més freqüent: \",mft)\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NP00000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinación de la etiqueta más frecuente y entrenamiento del etiquetador por defecto\n",
    "\n",
    "El programa-5-14.py nos devuelve las 10 etiquetas más frecuentes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tornem a tenir que la més freqüent (de les categories obertes) és el substantiu, concretament el nom propi. I llavors creem l'etiquetador per defecte fent servir aquesta etiqueta.\n",
    "\n",
    "Posarem tot això junt en el següent apartat, o a més, aprendrem a emmagatzemar etiquetadors .\n",
    "\n",
    "## 5.5. Emmagatzematge d'etiquetadors\n",
    "\n",
    "En l'apartat anterior hem après a entrenar i combinar etiquetadors. Cada  cop que volíem etiquetar una oració, entrenàvem un etiquetador i després etiquetàvem. Com que l'entrenament és lent, ens interessa entrenar una vegada i poder guardar l'etiquetador ja entrenat de manera que el puguem fer servir tantes vegades com vulguem.\n",
    "\n",
    "En el programa-5-15.py entrenen i emmagatzem un etiquetador. Fixeu-vos en tots els etiquetadors diferents que entrenem, com els combinem, i com finalment emmagatzem el darrer, que de fet està combinat amb tota la resta. Fixeu-vos també com fem servir el mòdul pickle. I també tingueu en compte que hem comentat les línies que limiten el nombre d'entrades del diccionari morfològic que fa servir per entrenar. Si veieu que triga molt a entrenar, torneu a descomentar aquestes línies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "entrada=codecs.open(\"catalanTagged_0_5000-utf-8.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "tagged_sents_per_unigrams=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_sents_per_unigrams.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "if len(tagged_words)>0:\n",
    "    tagged_sents.append(tagged_words)\n",
    "    tagged_sents_per_unigrams.append(tagged_words)\n",
    "    tagged_words=[]\n",
    "        \n",
    "diccionari=codecs.open(\"diccionari-cat.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "cont=0\n",
    "for linia in diccionari:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\" \")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    tupla=(forma,etiqueta)\n",
    "    tagged_words.append(tupla)\n",
    "tagged_sents_per_unigrams.append(tagged_words)\n",
    "\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NP00000\")\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents_per_unigrams, affix_length=-3, min_stem_length=2,backoff=default_tagger)\n",
    "unigram_tagger_diccionari=nltk.UnigramTagger(tagged_sents_per_unigrams,backoff=affix_tagger)\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents,backoff=unigram_tagger_diccionari)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(tagged_sents,backoff=bigram_tagger)\n",
    "\n",
    "sortida=open('etiquetador-cat.pkl', 'wb')\n",
    "pickle.dump(trigram_tagger, sortida, -1)\n",
    "sortida.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El que és important tenir en compte en el programa anterior és que estem proporcionant un corpus i un diccionari. El corpus ens proporciona informació de forma i etiqueta dins del context de l'oració. En canvi, el diccionari només ens dona informació sobre formes i les seves etiquetes d'una manera totalment fora de context, ja que la paraula que apareix al diccionari abans d'una altra només guarda una relació alfabètica. Per aquest motiu, en tots els entrenaments que suposin un context (bigrams i trigrams) només podem fer servir la informació que prové del corpus. En canvi, per als entrenaments que no es tingui en compte el context (afixos i unigrames) podem fer servir tant la informació que apareix en el corpus com la que apareix al diccionari. Fixeu-vos que al programa fem servir dues llistes per emmagatzemar la informació que fem servir per entrenar:\n",
    "\n",
    "- tagged_sents_per_unigrams: on posem la informació del corpus i del diccionari\n",
    "- tagged_sents: on només posem la informació del corpus\n",
    "\n",
    "Les línies:\n",
    "\n",
    "```\n",
    "cont=0\n",
    "for linia in diccionari:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "```\n",
    "\n",
    "serveixen per limitar la informació que es carrega o bé del corpus o bé del diccionari, o de tots dos. Com que el programa triga molt a executar-se, podeu descomentar (treure el símbol \"#\") del davant de les línies.\n",
    "\n",
    "En els arxius d'aquest capítol trobareu l'etiquetador-cat.pkl resultant per a que el pugeu fer servir en el següent programa sense esperar que es completi l'entrenament.\n",
    "\n",
    "Ara a etiquetador-cat.pkl tenim un etiquetador que podem carregar sempre que vulguem de manera molt ràpida. Fixeu-vos en el programa-5-16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "entrada=open('etiquetador-cat.pkl','rb')\n",
    "etiquetador=pickle.load(entrada)\n",
    "entrada.close()\n",
    "\n",
    "oracio=\"l'àcid desoxiribonucleic (ADN o DNA) és un àcid nucleic que conté les instruccions genètiques utilitzades en el desenvolupament i funcionament de tots els éssers vius coneguts, així com en alguns virus, des d'un punt de vista químic, l'ADN es compon de dos llargs polímers d'unitats simples anomenades nucleòtids, amb un tronc compost de sucres i grups fosfats units per enllaços èster\"\n",
    "tokenitzador=nltk.tokenize.RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "analisi=etiquetador.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Avaluació d'etiquetadors\n",
    "\n",
    "En els apartats anteriors he après a crear etiquetadors estadístics. També hem après a fer-los servir per etiquetar textos i intuïtivament hem vist que funcionen força bé, tot i que poden etiquetar malament algunes paraules. Quan desenvolupem un etiquetador, ens interessaria saber quina precisió assolim. NLTK ens proporciona una manera molt senzilla d'avaluar etiquetadors. Comencem avaluant un etiquetador per a l'anglès (programa-5-17.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents=brown.tagged_sents()\n",
    "print(\"TOTAL ORACIONS:\", len(brown_tagged_sents))\n",
    "train_sents=brown_tagged_sents[:10000]\n",
    "test_sents=brown_tagged_sents[56001:]\n",
    "default_tagger=nltk.DefaultTagger(\"NN\")\n",
    "affix_tagger=nltk.AffixTagger(train_sents, affix_length=-3, min_stem_length=2)\n",
    "unigram_tagger=nltk.UnigramTagger(train_sents, backoff=affix_tagger)\n",
    "bigram_tagger=nltk.BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "precisio=trigram_tagger.evaluate(test_sents)\n",
    "print(\"PRECISIO: \",precisio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa primer ens indicarà el nombre total d'oracions del corpus. Fixeu-vos que fa un conjunt d'oracions d'entrenament (train_sents) amb les primeres 10000 oracions del corpus; i un de test amb les oracions finals, de la 56001 fins la final. Després de crear l'etiquetador amb el mètode evaluate avalua la precisió fent servir les oracions de test. El que fa el programa és etiquetar aquestes oracions i comparar-les amb les etiquetes reals. Si executeu el programa, obtindreu la següent sortida:\n",
    "```\n",
    "TOTAL ORACIONS: 57340\n",
    "\n",
    "PRECISIO:  0.8937947951400866\n",
    "```\n",
    "Canvieu ara el nombre d'oracions per entrenar l'etiquetador i poseu el màxim (sense agafar oracions de test), és a dir, 56000. Si executeu ara el programa, la precisió passa a ser de:\n",
    "```\n",
    "PRECISIO:  0.9220420834770611\n",
    "```\n",
    "Veiem que quan més gran sigui el corpus d'entrenament, obtindrem una millor precisió.\n",
    "\n",
    "Anem a avaluar l'etiquetador del català que hem entrenat i emmagatzemat en l'apartat anterior. Ho fem en el programa-5-18.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import codecs\n",
    "\n",
    "#carreguem l'etiquetador\n",
    "entrada=open('etiquetador-cat.pkl','rb')\n",
    "etiquetador=pickle.load(entrada)\n",
    "entrada.close()\n",
    "\n",
    "#carreguem les oracions del corpus de test\n",
    "\n",
    "entrada=codecs.open(\"wikicorpus-tagged-test.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "test_tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            test_tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "precisio=etiquetador.evaluate(test_tagged_sents)\n",
    "print(\"PRECISIO: \",precisio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ens fixem, fem servir un fragment nou del wikicorpus ja etiquetat (wikicorpus-tagged-test.txt). Primer carreguem l'etiquetador emmagatzemat i després carreguem el nou corpus afegint-lo al test_tagged_sents, que són les que farem servir per avaluar al mètode evaluate. Aquest programa ens proporciona la següent sortida:\n",
    "```\n",
    "PRECISIO:  0.9585872792623847\n",
    "```\n",
    "Recordem que el corpus que estem fent servir és un corpus etiquetat automàticament amb Freeling, i per tant, el 95.8% de precisió que obtenim no és real. Hauríem de fer servir corpus etiquetats manualment, o bé etiquetats automàticament i revisats."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
