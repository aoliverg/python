{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Etiquetado morfosintático\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este módulo estudiaremos la tarea denominada etiquetado morfosintáctico (en inglés part-of-speech tagging o POS-tagging). Esta tarea consiste en asignar a cada palabra de un texto una categoría gramatical y otra información adicional (como pueden ser varias subcategorías, el lema asociado, etc.) Esta es una tarea fundamental en el procesamiento del lenguaje natural, aunque no está exenta de problemas que no están todavía totalmente resueltos. El lenguaje natural es ambiguo desde muchos puntos de vista, y también lo es en el morfosintáctico. Una determinada forma (como por ejemplo casa) puede tener varias interpretaciones morfosintácticas, puede ser un sustantivo común femenino singular (con lema casa) y también una forma de presente o de imperativo del verbo casar. Los etiquetadores morfosintácticos deberán intentar dar la interpretación adecuada según el contexto de utilización; por lo tanto deberán desambiguar las diferentes posibilidades.\n",
    "\n",
    "Antes de abordar el etiquetado morfosintáctico veremos también el análisis morfológico, es decir, el análisis que nos permite determinar el lema y la categoría gramatical (y otras subcategorizaciones) de una determinada forma. Una vez seamos capaces de hacer el análisis morfológico, pasaremos a estudiar diversas técnicas que nos permitirán etiquetar textos desde el punto de vista morfosintáctico y que intentarán desambiguar (con mayor o menor éxito) las diferentes posibilidades.\n",
    "\n",
    "## 5.1. Morfología computacional\n",
    "\n",
    "### 5.1.1. El formalismo de descomposición morfológica\n",
    "\n",
    "Existen toda una serie de formalismos para describir la morfología de una lengua. En este capítulo sólo presentaremos uno, que es sencillo de implementar y nos servirá para comprender los mecanismos fundamentales: el formalismo de descomposición morfológica (Alshawi, 1992). La idea básica de este formalismo es sencilla y se basa en dos tipos de conocimiento:\n",
    "\n",
    "Un diccionario que contiene información morfosintáctica base o la palabra que se considera forma de referencia.\n",
    "Reglas que contienen información sobre la morfología de la lengua.\n",
    "En el programa-5-1.py podemos ver una primera implementación muy sencilla de esta idea. Ejecuta el programa y observa la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canto\n",
      "cantas\n",
      "canta\n",
      "cantamos\n",
      "cantáis\n",
      "cantan\n"
     ]
    }
   ],
   "source": [
    "arrel=\"cant\"\n",
    "\n",
    "t1=\"o\"\n",
    "\n",
    "t2=\"as\"\n",
    "\n",
    "t3=\"a\"\n",
    "\n",
    "t4=\"amos\"\n",
    "\n",
    "t5=\"áis\"\n",
    "\n",
    "t6=\"an\"\n",
    "\n",
    "print(arrel+t1)\n",
    "\n",
    "print(arrel+t2)\n",
    "\n",
    "print(arrel+t3)\n",
    "\n",
    "print(arrel+t4)\n",
    "\n",
    "print(arrel+t5)\n",
    "\n",
    "print(arrel+t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este programa definimos una serie de variables: una que contiene el lema de un verbo y otros que contienen las terminaciones de presente de indicativo. Mediante el operador + que concatena cadenas obtenemos las formas correspondientes al presente de indicativo.\n",
    "\n",
    "Siguiendo esta idea básica crearemos un formalismo para expresar las reglas (que además de las formas queremos que nos den una etiqueta de categoría gramatical y subcategorizaciones). También utilizaremos un formalismo para expresar el diccionario de lemas, que además del propio lema nos indicará el tipo de flexión que sigue (es decir, el paradigma flexivo).\n",
    "\n",
    "Como formalismo para las reglas proponemos una serie de valores separados por dos puntos (:):\n",
    "```\n",
    "terminación_forma:terminación_lema:etiqueta:paradigma\n",
    "```\n",
    "\n",
    "Por ejemplo:\n",
    "```\n",
    "\n",
    "o:ar:VMIP1S:V1\n",
    "\n",
    "as:ar:VMIP2S:V1\n",
    "\n",
    "a:ar:VMIP3S:V1\n",
    "\n",
    "amos:ar:VMIP1P:V1\n",
    "\n",
    "áis:ar:VMIP2P:V1\n",
    "\n",
    "an:ar:VMIP3P:V1\n",
    "```\n",
    "\n",
    "Para las etiquetas utilizamos las etiquetas EAGLES para el castellano. En el archivo reglas.txt podemos observar un conjunto más extenso de reglas morfológicas.\n",
    "\n",
    "Para el formalismo para el diccionario de lemas seguiremo.pickles una idea similar:\n",
    "```\n",
    "lema: paradigma\n",
    "\n",
    "```\n",
    "Que en el ejemplo sería:\n",
    "```\n",
    "cantar: V1\n",
    "```\n",
    "En el archivo diccionario.txt podemos observar un diccionario más completo.\n",
    "\n",
    "Ahora necesitamos un programa que nos permita generar todas las formas a partir de las reglas y el diccionario (programa-5-2.py). Ejecuta el programa y observa la salida. Si trabajas con tu ordenador tienes que tener los archivos reglas.txt y diccionario.txt en el mismo directorio que el programa python. Si trabajas en Google Colab, ejecuta la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB.\n",
    "#Te pedirá que selecciones el archivo reglas.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "#Te pedirá que selecciones el archivo diccionario.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canto cantar VMIP1S\n",
      "cantas cantar VMIP2S\n",
      "canta cantar VMIP3S\n",
      "cantamos cantar VMIP1P\n",
      "cantáis cantar VMIP2P\n",
      "cantan cantar VMIP3P\n",
      "como comer VMIP1S\n",
      "comes comer VMIP2S\n",
      "come comer VMIP3S\n",
      "comemos comer VMIP1P\n",
      "coméis comer VMIP2P\n",
      "comen comer VMIP3P\n",
      "vivo vivir VMIP1S\n",
      "vives vivir VMIP2S\n",
      "vive vivir VMIP3S\n",
      "vivimos vivir VMIP1P\n",
      "vivís vivir VMIP2P\n",
      "viven vivir VMIP3P\n"
     ]
    }
   ],
   "source": [
    "freglas=open(\"reglas.txt\",\"r\")\n",
    "\n",
    "reglas=[]\n",
    "\n",
    "while True:\n",
    "\tlinea=freglas.readline().rstrip()\n",
    "\tif not linea:break\n",
    "\treglas.append(linea)\n",
    "freglas.close()\n",
    "\n",
    "fdiccionario=open(\"diccionario.txt\",\"r\")\n",
    "\n",
    "while True:\n",
    "\tlinea=fdiccionario.readline().rstrip()\n",
    "\tif not linea:break\n",
    "\t(lema,tipo)=linea.split(\":\")\n",
    "\tfor regla in reglas:\n",
    "\t\t(tf,tl,etiqueta,tipo2)=regla.split(\":\")\n",
    "\t\tif ((tipo2 == tipo)&(lema.endswith(tl))):\n",
    "\t\t\tprint(lema[0:(len(lema)-len(tl))]+tf,lema,etiqueta)\n",
    "\n",
    "fdiccionario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos hablando de análisis morfológico, pero en realidad lo que hemos hecho es un programa que genera formas con su lema y una etiqueta morfosintáctica, lo que llamamos diccionario morfológico. Los programas de análisis morfológico a menudo funcionan con un diccionario morfológico.\n",
    "\n",
    "En el siguiente programa (programa-5-3.py) cargamos un gran diccionario morfológico del castellano obtenido del analizador Freeling (diccionario-freeling-spa.txt) y llevamos a cabo el análisis de las palabras que indica el usuario (el programa finaliza cuando el usuario introduce un espacio en blanco). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB.\n",
    "#Te pedirá que selecciones el archivo diccionario-freeling-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICCIONARIO CARGADO\n",
      "Introduce la palabra a analizar: perro\n",
      "ANALISIS: perro :perro\tNCMS000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f772816b598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DICCIONARIO CARGADO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpalabra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Introduce la palabra a analizar: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpalabra\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "diccionario={}\n",
    "archivo_diccipnario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in archivo_diccipnario:\n",
    "    entrada=entrada.rstrip()\n",
    "    campos=entrada.split(\":\")\n",
    "    forma=campos[0]\n",
    "    lema=campos[1]\n",
    "    etiqueta=campos[2]\n",
    "    diccionario[forma]=diccionario.get(forma,\"\")+\":\"+lema+\"\\t\"+etiqueta\n",
    "print(\"DICCIONARIO CARGADO\")\n",
    "while 1:\n",
    "    palabra=input(\"Introduce la palabra a analizar: \")\n",
    "    if palabra==\" \":\n",
    "        break\n",
    "    if palabra in diccionario:\n",
    "        print(\"ANALISIS:\",palabra,diccionario[palabra])\n",
    "    else:\n",
    "        print(\"PALABRA DESCONOCIDA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Análisis morfológico\n",
    "\n",
    "En este apartado vamos a construir un analizador morfológico, es decir, un programa que es capaz de darnos los análisis morfológicos de todas las palabras de un texto. Para las palabras ambiguas desde el punto de vista morfosintáctico, nos devolverá todas las posibles interpretaciones. Con lo que hemos hecho hasta ahora tenemos todos los componentes:\n",
    "\n",
    "- Un programa que cargue el diccionario morfológico del español (el programa-5-3.py)\n",
    "- Un programa capaz de leer un documento y segmentarlo en oraciones y tokenizarlo (por ejemplo, el programa-4-3.py que vimos en la unidad anterior)\n",
    "\n",
    "En el programa-5-4.py podemos observar una primera versión del programa (que analizará el archivo noticia.txt, que contiene un fragmento de noticia publicada en el diario Ara). Utiliza también el catalán-mod.pickle que hemos creado en el capítulo anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB Y\n",
    "#NO HAS SUBIDO TODAVÍA EL ARCHIVO diccionario-freeling-spa.txt\n",
    "#Te pedirá que selecciones el archivo diccionario-freeling-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB \n",
    "#Te pedirá que selecciones el archivo noticia.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "#Después te pedirá que selecciones el archivo spanish.pickle de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putin DESCONOCIDA\n",
      "apenas apenas RG apenas CS apenar VMIP2S0\n",
      "condena condena NCFS000 condenar VMIP3S0 condenar VMM02S0\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "decisión decisión NCFS000\n",
      "de de NCFS000 de SP\n",
      "Trump DESCONOCIDA\n",
      "de de NCFS000 de SP\n",
      "' DESCONOCIDA\n",
      "ceder ceder VMN0000\n",
      "' DESCONOCIDA\n",
      "el el DA0MS0\n",
      "Golán DESCONOCIDA\n",
      "a a NCFS000 a SP\n",
      "Israel DESCONOCIDA\n",
      "Ha DESCONOCIDA\n",
      "sido ser VSP00SM\n",
      "una uno DI0FS0 uno PI0FS00 unir VMM03S0 unir VMSP1S0 unir VMSP3S0\n",
      "comedida comedir VMP00SF\n",
      "reacción reacción NCFS000\n",
      "crítica crítico AQ0FS00 crítica NCFS000 crítico NCFS000\n",
      ", DESCONOCIDA\n",
      "muy muy RG\n",
      "diferente diferente AQ0CS00\n",
      "en en SP\n",
      "tono tono NCMS000\n",
      "y y NCFS000 y CC\n",
      "forma forma NCFS000 formar VMIP3S0 formar VMM02S0\n",
      "a a NCFS000 a SP\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "agrias agrio AQ0FP00 agriar VMIP2S0\n",
      "diatribas diatriba NCFP000\n",
      "oídas oída NCFP000 oír VMP00PF\n",
      "en en SP\n",
      "Moscú DESCONOCIDA\n",
      "en en SP\n",
      "otros otro DI0MP0 otro PI0MP00\n",
      "picos pico NCMP000\n",
      "de de NCFS000 de SP\n",
      "tensión tensión NCFS000\n",
      "de de NCFS000 de SP\n",
      "esta este DD0FS0 este PD0FS00\n",
      "nueva nuevo AQ0FS00 nueva NCFS000\n",
      "era era NCFS000 ser VSII1S0 ser VSII3S0 erar VMIP3S0 erar VMM02S0\n",
      "de de NCFS000 de SP\n",
      "confrontación confrontación NCFS000\n",
      "entre entre SP entrar VMM03S0 entrar VMSP1S0 entrar VMSP3S0\n",
      "el el DA0MS0\n",
      "Kremlin DESCONOCIDA\n",
      "y y NCFS000 y CC\n",
      "Occidente DESCONOCIDA\n",
      ". DESCONOCIDA\n",
      "Rusia DESCONOCIDA\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "considerado considerar VMP00SM\n",
      ", DESCONOCIDA\n",
      "a a NCFS000 a SP\n",
      "través través NCMS000\n",
      "de de NCFS000 de SP\n",
      "sus sus I su DP3CPN\n",
      "portavoces portavoz NCCP000\n",
      ", DESCONOCIDA\n",
      "que que CS que PR0CN00\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "pretensión pretensión NCFS000\n",
      "del de+el SP+DA\n",
      "presidente presidente NCMS000\n",
      "estadounidense estadounidense AQ0CS00 estadounidense NCCS000\n",
      "Donald DESCONOCIDA\n",
      "Trump DESCONOCIDA\n",
      "de de NCFS000 de SP\n",
      "reconocer reconocer VMN0000\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "soberanía soberanía NCFS000\n",
      "israelí israelí AQ0CS00 israelí NCCS000\n",
      "sobre sobre NCMS000 sobre SP sobrar VMM03S0 sobrar VMSP1S0 sobrar VMSP3S0\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "Altos DESCONOCIDA\n",
      "del de+el SP+DA\n",
      "Golán DESCONOCIDA\n",
      "es ser VSIP3S0\n",
      "\" DESCONOCIDA\n",
      "irresponsable irresponsable AQ0CS00 irresponsable NCCS000\n",
      "\" DESCONOCIDA\n",
      "y y NCFS000 y CC\n",
      "\" DESCONOCIDA\n",
      "agrava agravar VMIP3S0 agravar VMM02S0\n",
      "\" DESCONOCIDA\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "conflictos conflicto NCMP000\n",
      "en en SP\n",
      "Oriente DESCONOCIDA\n",
      "Próximo DESCONOCIDA\n",
      ". DESCONOCIDA\n",
      "Pese DESCONOCIDA\n",
      "a a NCFS000 a SP\n",
      "que que CS que PR0CN00\n",
      "Siria DESCONOCIDA\n",
      "consitituye DESCONOCIDA\n",
      "el el DA0MS0\n",
      "principal principal AQ0CS00 principal NCMS000\n",
      "aliado aliado NCMS000 aliar VMP00SM\n",
      "ruso ruso AQ0MS00 ruso NCMS000\n",
      "en en SP\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "región región NCFS000\n",
      ", DESCONOCIDA\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "autoridades autoridad NCFP000\n",
      "rusas ruso AQ0FP00 ruso NCFP000\n",
      "han haber VAIP3P0 haber VMIP3P0\n",
      "evitado evitar VMP00SM\n",
      "recurrir recurrir VMN0000\n",
      "a a NCFS000 a SP\n",
      "ningún ninguno DI0MS0\n",
      "tipo tipo NCMS000\n",
      "de de NCFS000 de SP\n",
      "retórica retórico AQ0FS00 retórica NCFS000 retórico NCFS000\n",
      "incendiaria incendiario AQ0FS00 incendiario NCFS000\n",
      "a a NCFS000 a SP\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "hora hora NCFS000\n",
      "de de NCFS000 de SP\n",
      "valorar valorar VMN0000\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "intenciones intención NCFP000\n",
      "del de+el SP+DA\n",
      "magnate magnate NCCS000\n",
      "neoyorkino neoyorkino AQ0MS00\n",
      ", DESCONOCIDA\n",
      "al a+el SP+DA\n",
      "menos menos RG menos SP\n",
      "por por SP\n",
      "el el DA0MS0\n",
      "momento momento NCMS000\n",
      ". DESCONOCIDA\n",
      "\" DESCONOCIDA\n",
      "Esto DESCONOCIDA\n",
      "es ser VSIP3S0\n",
      "nada nada RG nada NCFS000 nada PI0CS00 nadar VMIP3S0 nadar VMM02S0\n",
      "más más RG más NCMS000\n",
      "que que CS que PR0CN00\n",
      "un uno DI0MS0\n",
      "llamamiento llamamiento NCMS000\n",
      "; DESCONOCIDA\n",
      "esperemos esperar VMM01P0 esperar VMSP1P0\n",
      "que que CS que PR0CN00\n",
      "solo solo AQ0MS00 solo RG solo NCMS000\n",
      "se se P00CN00 se P03CN00 se PP3CN00\n",
      "quede quedar VMM03S0 quedar VMSP1S0 quedar VMSP3S0\n",
      "en en SP\n",
      "eso ese PD00S00\n",
      "\", DESCONOCIDA\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "asegurado asegurado NCMS000 asegurar VMP00SM\n",
      "Dmitri DESCONOCIDA\n",
      "Peskov DESCONOCIDA\n",
      ", DESCONOCIDA\n",
      "el el DA0MS0\n",
      "portavoz portavoz NCCS000\n",
      "del de+el SP+DA\n",
      "Kremlin DESCONOCIDA\n",
      ". DESCONOCIDA\n",
      "\" DESCONOCIDA\n",
      "En DESCONOCIDA\n",
      "cualquier cualquiera DI0CS0\n",
      "caso caso NCMS000 casar VMIP1S0\n",
      ", DESCONOCIDA\n",
      "esta este DD0FS0 este PD0FS00\n",
      "idea ideo AQ0FS00 idea NCFS000 idear VMIP3S0 idear VMM02S0\n",
      "por por SP\n",
      "sí sí RG sí NCMS000 sí PP3CNO0\n",
      "misma mismo AQ0FS00 mismo PI0FS00\n",
      "de de NCFS000 de SP\n",
      "ninguna ninguno DI0FS0 ninguno PI0FS00\n",
      "manera manera NCFS000\n",
      "facilita facilitar VMIP3S0 facilitar VMM02S0\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "objetivos objetivo AQ0MP00 objetivo NCMP000\n",
      "de de NCFS000 de SP\n",
      "llegar llegar VMN0000\n",
      "a a NCFS000 a SP\n",
      "un uno DI0MS0\n",
      "acuerdo acuerdo NCMS000 acordar VMIP1S0\n",
      "en en SP\n",
      "Oriente DESCONOCIDA\n",
      "Próximo DESCONOCIDA\n",
      "; DESCONOCIDA\n",
      "más más RG más NCMS000\n",
      "bien bien RG bien NCMS000 bien CC\n",
      "lo lo NCMS000 el DA00S0 lo PP3MSA0\n",
      "contrario contrario AQ0MS00 contrario NCMS000\n",
      "\", DESCONOCIDA\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "subrayado subrayado NCMS000 subrayar VMP00SM\n",
      ", DESCONOCIDA\n",
      "en en SP\n",
      "tono tono NCMS000\n",
      "circunspecto circunspecto AQ0MS00\n",
      ", DESCONOCIDA\n",
      "el el DA0MS0\n",
      "vocero vocero NCMS000\n",
      "presidencial presidencial AQ0CS00\n",
      ". DESCONOCIDA\n",
      "Por DESCONOCIDA\n",
      "su su DP3CSN\n",
      "parte parte NCCS000 partir VMIP3S0 partir VMM02S0\n",
      ", DESCONOCIDA\n",
      "Maria DESCONOCIDA\n",
      "Zajárova DESCONOCIDA\n",
      ", DESCONOCIDA\n",
      "portavoz portavoz NCCS000\n",
      "de de NCFS000 de SP\n",
      "Exteriores DESCONOCIDA\n",
      ", DESCONOCIDA\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "asegurado asegurado NCMS000 asegurar VMP00SM\n",
      "en en SP\n",
      "su su DP3CSN\n",
      "cuenta cuenta I cuenta NCFS000 contar VMIP3S0 contar VMM02S0\n",
      "de de NCFS000 de SP\n",
      "Twitter DESCONOCIDA\n",
      "que que CS que PR0CN00\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "movimientos movimiento NCMP000\n",
      "del de+el SP+DA\n",
      "presidente presidente NCMS000\n",
      "norteamericano norteamericano AQ0MS00 norteamericano NCMS000\n",
      "son son NCMS000 ser VSIP3P0\n",
      "\" DESCONOCIDA\n",
      "irresponsables irresponsable AQ0CP00 irresponsable NCCP000\n",
      "\", DESCONOCIDA\n",
      "y y NCFS000 y CC\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "advertido advertir VMP00SM\n",
      "que que CS que PR0CN00\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "pretensiones pretensión NCFP000\n",
      "de de NCFS000 de SP\n",
      "Trump DESCONOCIDA\n",
      "podrían poder VMIC3P0\n",
      "desencadenar desencadenar VMN0000\n",
      "\" DESCONOCIDA\n",
      "nuevas nuevo AQ0FP00 nueva NCFP000\n",
      "guerras guerra NCFP000\n",
      "\" DESCONOCIDA\n",
      "y y NCFS000 y CC\n",
      "\" DESCONOCIDA\n",
      "agravar agravar VMN0000\n",
      "\" DESCONOCIDA\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "existentes existente AQ0CP00\n",
      ". DESCONOCIDA\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "diccionario={}\n",
    "archivo_diccionario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in archivo_diccionario:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\":\")\n",
    "    if len(camps)>=3:\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        if forma in diccionario:\n",
    "            diccionario[forma]=diccionario.get(forma,\"\")+\" \"+lema+\" \"+etiqueta\n",
    "        else:\n",
    "            diccionario[forma]=lema+\" \"+etiqueta\n",
    "\n",
    "segmentador= nltk.data.load(\"spanish.pickle\")\n",
    "tokenitzador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'noticia.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "for forma in corpus.words():\n",
    "    if forma in diccionario:\n",
    "        info=diccionario[forma]\n",
    "    else:\n",
    "        info=\"DESCONOCIDA\"\n",
    "    print(forma+\" \"+info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, hay algunos problemas, como por ejemplo:\n",
    "\n",
    "- Las palabras que están en la primera posición de una oración, y por lo tanto están escritas en mayúsculas, a pesar de estar en el diccionario, las etiqueta como desconocidas. Para solucionar esto, primero buscaremos el diccionario las palabras tal y como aparecen en el texto, si no las encuentra, entonces las buscará pasada a minúscula, y si aún así no la encuentra, la marcará como desconocida.\n",
    "- Los signos de puntuación los marca como desconocidos, ya que son tokens que no aparecen en el diccionario morfológico. La solución es sencilla y consiste en incluir los signos de puntuación en el diccionario morfológico o bien ponerlas en el propio programa. \n",
    "\n",
    "Veamos la nueva implementación en el programa-5-5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putin DESCONOCIDA\n",
      "apenas apenas RG apenas CS apenar VMIP2S0\n",
      "condena condena NCFS000 condenar VMIP3S0 condenar VMM02S0\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "decisión decisión NCFS000\n",
      "de de NCFS000 de SP\n",
      "Trump DESCONOCIDA\n",
      "de de NCFS000 de SP\n",
      "' ' Fe\n",
      "ceder ceder VMN0000\n",
      "' ' Fe\n",
      "el el DA0MS0\n",
      "Golán DESCONOCIDA\n",
      "a a NCFS000 a SP\n",
      "Israel DESCONOCIDA\n",
      "Ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "sido ser VSP00SM\n",
      "una uno DI0FS0 uno PI0FS00 unir VMM03S0 unir VMSP1S0 unir VMSP3S0\n",
      "comedida comedir VMP00SF\n",
      "reacción reacción NCFS000\n",
      "crítica crítico AQ0FS00 crítica NCFS000 crítico NCFS000\n",
      ", , Fc\n",
      "muy muy RG\n",
      "diferente diferente AQ0CS00\n",
      "en en SP\n",
      "tono tono NCMS000\n",
      "y y NCFS000 y CC\n",
      "forma forma NCFS000 formar VMIP3S0 formar VMM02S0\n",
      "a a NCFS000 a SP\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "agrias agrio AQ0FP00 agriar VMIP2S0\n",
      "diatribas diatriba NCFP000\n",
      "oídas oída NCFP000 oír VMP00PF\n",
      "en en SP\n",
      "Moscú DESCONOCIDA\n",
      "en en SP\n",
      "otros otro DI0MP0 otro PI0MP00\n",
      "picos pico NCMP000\n",
      "de de NCFS000 de SP\n",
      "tensión tensión NCFS000\n",
      "de de NCFS000 de SP\n",
      "esta este DD0FS0 este PD0FS00\n",
      "nueva nuevo AQ0FS00 nueva NCFS000\n",
      "era era NCFS000 ser VSII1S0 ser VSII3S0 erar VMIP3S0 erar VMM02S0\n",
      "de de NCFS000 de SP\n",
      "confrontación confrontación NCFS000\n",
      "entre entre SP entrar VMM03S0 entrar VMSP1S0 entrar VMSP3S0\n",
      "el el DA0MS0\n",
      "Kremlin DESCONOCIDA\n",
      "y y NCFS000 y CC\n",
      "Occidente occidente NCMS000\n",
      ". . Fp\n",
      "Rusia rusia NCFS000\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "considerado considerar VMP00SM\n",
      ", , Fc\n",
      "a a NCFS000 a SP\n",
      "través través NCMS000\n",
      "de de NCFS000 de SP\n",
      "sus sus I su DP3CPN\n",
      "portavoces portavoz NCCP000\n",
      ", , Fc\n",
      "que que CS que PR0CN00\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "pretensión pretensión NCFS000\n",
      "del de+el SP+DA\n",
      "presidente presidente NCMS000\n",
      "estadounidense estadounidense AQ0CS00 estadounidense NCCS000\n",
      "Donald DESCONOCIDA\n",
      "Trump DESCONOCIDA\n",
      "de de NCFS000 de SP\n",
      "reconocer reconocer VMN0000\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "soberanía soberanía NCFS000\n",
      "israelí israelí AQ0CS00 israelí NCCS000\n",
      "sobre sobre NCMS000 sobre SP sobrar VMM03S0 sobrar VMSP1S0 sobrar VMSP3S0\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "Altos alto AQ0MP00 alto NCMP000\n",
      "del de+el SP+DA\n",
      "Golán DESCONOCIDA\n",
      "es ser VSIP3S0\n",
      "\" \" Fe\n",
      "irresponsable irresponsable AQ0CS00 irresponsable NCCS000\n",
      "\" \" Fe\n",
      "y y NCFS000 y CC\n",
      "\" \" Fe\n",
      "agrava agravar VMIP3S0 agravar VMM02S0\n",
      "\" \" Fe\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "conflictos conflicto NCMP000\n",
      "en en SP\n",
      "Oriente oriente NCMS000 orientar VMM03S0 orientar VMSP1S0 orientar VMSP3S0\n",
      "Próximo próximo AQ0MS00\n",
      ". . Fp\n",
      "Pese pesar VMM03S0 pesar VMSP1S0 pesar VMSP3S0\n",
      "a a NCFS000 a SP\n",
      "que que CS que PR0CN00\n",
      "Siria sirio AQ0FS00 sirio NCFS000\n",
      "consitituye DESCONOCIDA\n",
      "el el DA0MS0\n",
      "principal principal AQ0CS00 principal NCMS000\n",
      "aliado aliado NCMS000 aliar VMP00SM\n",
      "ruso ruso AQ0MS00 ruso NCMS000\n",
      "en en SP\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "región región NCFS000\n",
      ", , Fc\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "autoridades autoridad NCFP000\n",
      "rusas ruso AQ0FP00 ruso NCFP000\n",
      "han haber VAIP3P0 haber VMIP3P0\n",
      "evitado evitar VMP00SM\n",
      "recurrir recurrir VMN0000\n",
      "a a NCFS000 a SP\n",
      "ningún ninguno DI0MS0\n",
      "tipo tipo NCMS000\n",
      "de de NCFS000 de SP\n",
      "retórica retórico AQ0FS00 retórica NCFS000 retórico NCFS000\n",
      "incendiaria incendiario AQ0FS00 incendiario NCFS000\n",
      "a a NCFS000 a SP\n",
      "la la NCMS000 el DA0FS0 lo PP3FSA0\n",
      "hora hora NCFS000\n",
      "de de NCFS000 de SP\n",
      "valorar valorar VMN0000\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "intenciones intención NCFP000\n",
      "del de+el SP+DA\n",
      "magnate magnate NCCS000\n",
      "neoyorkino neoyorkino AQ0MS00\n",
      ", , Fc\n",
      "al a+el SP+DA\n",
      "menos menos RG menos SP\n",
      "por por SP\n",
      "el el DA0MS0\n",
      "momento momento NCMS000\n",
      ". . Fp\n",
      "\" \" Fe\n",
      "Esto este PD00S00\n",
      "es ser VSIP3S0\n",
      "nada nada RG nada NCFS000 nada PI0CS00 nadar VMIP3S0 nadar VMM02S0\n",
      "más más RG más NCMS000\n",
      "que que CS que PR0CN00\n",
      "un uno DI0MS0\n",
      "llamamiento llamamiento NCMS000\n",
      "; ; Fx\n",
      "esperemos esperar VMM01P0 esperar VMSP1P0\n",
      "que que CS que PR0CN00\n",
      "solo solo AQ0MS00 solo RG solo NCMS000\n",
      "se se P00CN00 se P03CN00 se PP3CN00\n",
      "quede quedar VMM03S0 quedar VMSP1S0 quedar VMSP3S0\n",
      "en en SP\n",
      "eso ese PD00S00\n",
      "\", DESCONOCIDA\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "asegurado asegurado NCMS000 asegurar VMP00SM\n",
      "Dmitri DESCONOCIDA\n",
      "Peskov DESCONOCIDA\n",
      ", , Fc\n",
      "el el DA0MS0\n",
      "portavoz portavoz NCCS000\n",
      "del de+el SP+DA\n",
      "Kremlin DESCONOCIDA\n",
      ". . Fp\n",
      "\" \" Fe\n",
      "En en SP\n",
      "cualquier cualquiera DI0CS0\n",
      "caso caso NCMS000 casar VMIP1S0\n",
      ", , Fc\n",
      "esta este DD0FS0 este PD0FS00\n",
      "idea ideo AQ0FS00 idea NCFS000 idear VMIP3S0 idear VMM02S0\n",
      "por por SP\n",
      "sí sí RG sí NCMS000 sí PP3CNO0\n",
      "misma mismo AQ0FS00 mismo PI0FS00\n",
      "de de NCFS000 de SP\n",
      "ninguna ninguno DI0FS0 ninguno PI0FS00\n",
      "manera manera NCFS000\n",
      "facilita facilitar VMIP3S0 facilitar VMM02S0\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "objetivos objetivo AQ0MP00 objetivo NCMP000\n",
      "de de NCFS000 de SP\n",
      "llegar llegar VMN0000\n",
      "a a NCFS000 a SP\n",
      "un uno DI0MS0\n",
      "acuerdo acuerdo NCMS000 acordar VMIP1S0\n",
      "en en SP\n",
      "Oriente oriente NCMS000 orientar VMM03S0 orientar VMSP1S0 orientar VMSP3S0\n",
      "Próximo próximo AQ0MS00\n",
      "; ; Fx\n",
      "más más RG más NCMS000\n",
      "bien bien RG bien NCMS000 bien CC\n",
      "lo lo NCMS000 el DA00S0 lo PP3MSA0\n",
      "contrario contrario AQ0MS00 contrario NCMS000\n",
      "\", DESCONOCIDA\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "subrayado subrayado NCMS000 subrayar VMP00SM\n",
      ", , Fc\n",
      "en en SP\n",
      "tono tono NCMS000\n",
      "circunspecto circunspecto AQ0MS00\n",
      ", , Fc\n",
      "el el DA0MS0\n",
      "vocero vocero NCMS000\n",
      "presidencial presidencial AQ0CS00\n",
      ". . Fp\n",
      "Por por SP\n",
      "su su DP3CSN\n",
      "parte parte NCCS000 partir VMIP3S0 partir VMM02S0\n",
      ", , Fc\n",
      "Maria DESCONOCIDA\n",
      "Zajárova DESCONOCIDA\n",
      ", , Fc\n",
      "portavoz portavoz NCCS000\n",
      "de de NCFS000 de SP\n",
      "Exteriores exterior AQ0CP00 exterior NCMP000\n",
      ", , Fc\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "asegurado asegurado NCMS000 asegurar VMP00SM\n",
      "en en SP\n",
      "su su DP3CSN\n",
      "cuenta cuenta I cuenta NCFS000 contar VMIP3S0 contar VMM02S0\n",
      "de de NCFS000 de SP\n",
      "Twitter DESCONOCIDA\n",
      "que que CS que PR0CN00\n",
      "los lo NCMP000 el DA0MP0 lo PP3MPA0\n",
      "movimientos movimiento NCMP000\n",
      "del de+el SP+DA\n",
      "presidente presidente NCMS000\n",
      "norteamericano norteamericano AQ0MS00 norteamericano NCMS000\n",
      "son son NCMS000 ser VSIP3P0\n",
      "\" \" Fe\n",
      "irresponsables irresponsable AQ0CP00 irresponsable NCCP000\n",
      "\", DESCONOCIDA\n",
      "y y NCFS000 y CC\n",
      "ha ha I haber VAIP3S0 haber VMIP3S0\n",
      "advertido advertir VMP00SM\n",
      "que que CS que PR0CN00\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "pretensiones pretensión NCFP000\n",
      "de de NCFS000 de SP\n",
      "Trump DESCONOCIDA\n",
      "podrían poder VMIC3P0\n",
      "desencadenar desencadenar VMN0000\n",
      "\" \" Fe\n",
      "nuevas nuevo AQ0FP00 nueva NCFP000\n",
      "guerras guerra NCFP000\n",
      "\" \" Fe\n",
      "y y NCFS000 y CC\n",
      "\" \" Fe\n",
      "agravar agravar VMN0000\n",
      "\" \" Fe\n",
      "las la NCMP000 el DA0FP0 lo PP3FPA0\n",
      "existentes existente AQ0CP00\n",
      ". . Fp\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "diccionario={}\n",
    "archivo_diccionario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in archivo_diccionario:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\":\")\n",
    "    if len(camps)>=3:\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        if forma in diccionario:\n",
    "            diccionario[forma]=diccionario.get(forma,\"\")+\" \"+lema+\" \"+etiqueta\n",
    "        else:\n",
    "            diccionario[forma]=lema+\" \"+etiqueta\n",
    "\n",
    "#Añadimos los signos de puntuación\n",
    "diccionario['\"']='\" Fe'\n",
    "diccionario[\"'\"]=\"' Fe\"\n",
    "diccionario['.']='. Fp'\n",
    "diccionario[',']=', Fc'\n",
    "diccionario[';']='; Fx'\n",
    "diccionario[':']=': Fd'\n",
    "diccionario['(']='( Fpa'\n",
    "diccionario[')']=') Fpt'\n",
    "diccionario['[']='[ Fca'\n",
    "diccionario[']']='] Fct'\n",
    "\n",
    "\n",
    "segmentador= nltk.data.load(\"spanish.pickle\")\n",
    "tokenitzador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'noticia.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "for forma in corpus.words():\n",
    "    if forma in diccionario:\n",
    "        info=diccionario[forma]\n",
    "    elif forma.lower() in diccionario:\n",
    "        info=diccionario[forma.lower()]\n",
    "    else:\n",
    "        info=\"DESCONOCIDA\"\n",
    "    print(forma+\" \"+info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún quedan problemas por resolver, como los nombres propios, algún aspecto mal tokenizado, etc. También pueden aparecer palabras desconocidas que son correctas, pero que no estén recogidas en el diccionario morfológico, etc. En próximos apartados de este mismo capítulo iremos presentando soluciones a estas cuestiones.\n",
    "\n",
    "## 5.3. Etiquetado morfosintáctico\n",
    "\n",
    "### 5.3.1. Introducción\n",
    "\n",
    "En este módulo estudiaremos la tarea llamada etiquetado morfosintáctico (en inglés part-of-speech tagging o POS-tagging). Esta tarea consiste en asignar a cada palabra de un texto una categoría gramatical y otra información adicional (como pueden ser varias subcategorías, el lema asociado, etc.) Esta es una tarea fundamental en el procesamiento del lenguaje natural, aunque no está exenta de problemas que no están todavía totalmente resueltos.\n",
    "\n",
    "El lenguaje natural es ambiguo desde muchos puntos de vista, y también lo es en el morfosintáctico. Una determinada forma (como por ejemplo casa) puede tener varias interpretaciones morfosintácticas, puede ser un sustantivo común femenino singular (con lema casa) y también una forma de presente o de imperativo del verbo casar. Los etiquetadores morfosintácticos deberán intentar dar la interpretación adecuada según el contexto donde aparece una palabra; por lo tanto deberán desambiguar las diferentes posibilidades.\n",
    "\n",
    "En el módulo veremos varias técnicas que nos permitirán etiquetar textos desde el punto de vista morfosintáctico y que intentarán desambiguar (con mayor o menor éxito) las diferentes posibilidades.\n",
    "\n",
    "### 5.3.2. Etiquetado morfosintáctico vs. análisis morfológico\n",
    "\n",
    "En el apartado anterior hemos estudiado la función denominada análisis morfológico, que consiste en asignar a cada palabra de un texto todas los posibles análisis morfológicos. \n",
    "\n",
    "Un analizador morfosintáctico ofrece la misma salida, pero desambiguada, es decir, elige una de las posibilidades de cada palabra.\n",
    "\n",
    "El etiquetado morfosintáctico es una tarea básica para muchas tareas de procesamiento del lenguaje natural. Si queremos hacer un análisis sintáctico de una oración, un paso previo es conocer la categoría gramatical de cada palabra. Disponer de textos etiquetados a escala morfosintáctica es interesante para muchos estudios. Podemos saber cuáles son los sustantivos más utilizados en un corpus, ver todas las apariciones de un verbo independientemente de la forma concreta, etc. El etiquetado morfosintáctico también se utiliza para extraer los términos más relevantes de un determinado documento o conjunto de documentos. Estas técnicas también se utiliza para la clasificación de documentos y recuperación de información.\n",
    "\n",
    "### 5.3.4. Etiquetador para el inglés\n",
    "\n",
    "NLTK proporciona un etiquetador para el inglés que funciona bastante bien y que se puede usar fácilmente de manera directa, como en el programa-5-6.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "oracion=\"They refuse to permit us to obtain the refuse permit\"\n",
    "palabras = nltk.tokenize.word_tokenize(oracion)\n",
    "analisis=nltk.pos_tag(palabras)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que la palabra permit la ha etiquetado correctamente como verbo y como sustantivo. Las etiquetas utilizadas son las propias de un etiquetario (tagset), o conjunto de etiquetas, determinado. En este caso concreto se utilizan el tagset WSJ (Wall Street Journal), que es el siguiente:\n",
    "\n",
    "Tag - Function\n",
    "\n",
    "CC - coordinating conjunction\n",
    "\n",
    "CD - cardinal number\n",
    "\n",
    "DT - determiner\n",
    "\n",
    "EX - existential there\n",
    "\n",
    "FW - foreign word\n",
    "\n",
    "IN - preposition\n",
    "\n",
    "JJ - adjective\n",
    "\n",
    "JJR - adjective, comparative\n",
    "\n",
    "JJS - adjective, superlative\n",
    "\n",
    "MD - modal\n",
    "\n",
    "NN - non-plural common noun\n",
    "\n",
    "NNP - non-plural proper noun\n",
    "\n",
    "NNPS - plural proper noun\n",
    "\n",
    "NNS - plural common noun\n",
    "\n",
    "of - the word of\n",
    "\n",
    "PDT - pre-determiner\n",
    "\n",
    "POS - posessive\n",
    "\n",
    "PRP - pronoun\n",
    "\n",
    "puncf - final punctuation (period, question mark and exclamation mark)\n",
    "\n",
    "punc - other punction\n",
    "\n",
    "RB - adverb\n",
    "\n",
    "RBR - adverb, comparative\n",
    "\n",
    "RBS - adverb, superlative\n",
    "\n",
    "RP - particle\n",
    "\n",
    "TO - the word to\n",
    "\n",
    "UH - interjection\n",
    "\n",
    "VB - verb, base form\n",
    "\n",
    "VBD - verb, past tense\n",
    "\n",
    "VBG - verb, gerund or present participle\n",
    "\n",
    "VBN - verb, past participle\n",
    "\n",
    "VBP - verb, non-3rd person\n",
    "\n",
    "VBZ - verb, 3rd person\n",
    "\n",
    "WDT - wh-determiner\n",
    "\n",
    "WP - wh-pronoun\n",
    "\n",
    "WRB - wh-adverb\n",
    "\n",
    "sym - symbol\n",
    "\n",
    "2 - ambiguously labelled\n",
    "\n",
    "Como veremos cuando empezamos a construir etiquetadores para otras lenguas, como el español, las etiquetas que utilizaremos serán diferentes. Hay una propuesta de etiquetario universal (universal tagset). A continuación podemos observar este etiquetario.\n",
    "\n",
    "Tag Meaning - English Examples\n",
    "\n",
    "ADJ - adjective - new, good, high, special, big, local\n",
    "\n",
    "ADP - adposition - on, of, at, with, by, into, under\n",
    "\n",
    "ADV - adverb - really, already, still, early, now\n",
    "\n",
    "CONJ - conjunction - and, or, but, if, while, although\n",
    "\n",
    "DET - determiner, article - the, a, some, most, every, no, which\n",
    "\n",
    "NOUN - noun - year, home, costs, time, Africa\n",
    "\n",
    "NUM - numeral - twenty-four, fourth, 1991, 14:24\n",
    "\n",
    "PRT - particle - at, on, out, over per, that, up, with\n",
    "\n",
    "PRON  - pronoun - he, their, her, its, my, I, us\n",
    "\n",
    "VERB - verb - is, say, told, given, playing, would\n",
    "\n",
    ". - punctuation marks-  . , ; !\n",
    "\n",
    "X - other - ersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "NLTK nos permite convertir las etiquetas de un determinado etiquetario a las del etiquetario universal. Lo podemos ver en el programa-5-6b.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They PRP PRON\n",
      "refuse VBP VERB\n",
      "to TO PRT\n",
      "permit VB VERB\n",
      "us PRP PRON\n",
      "to TO PRT\n",
      "obtain VB VERB\n",
      "the DT DET\n",
      "refuse NN NOUN\n",
      "permit NN NOUN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "oracion=\"They refuse to permit us to obtain the refuse permit\"\n",
    "palabras = nltk.tokenize.word_tokenize(oracion)\n",
    "analisis=nltk.pos_tag(palabras)\n",
    "for ana in analisis:\n",
    "    forma=ana[0]\n",
    "    etiqueta=ana[1]\n",
    "    universal=nltk.tag.mapping.map_tag('en-ptb', 'universal', etiqueta)\n",
    "    print(forma,etiqueta,universal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.4. Entrenamiento de etiquetadores\n",
    "\n",
    "### 5.4.1. El etiquetador por defecto\n",
    "\n",
    "En este apartado se presenta un etiquetador muy simple, que lo único que hace es etiquetar todas las palabras con una etiqueta determinada, es decir, etiqueta todas las palabras con la misma etiqueta. Para determinar qué etiqueta elegiremos lo que haremos primero es calcular cuál es la etiqueta más frecuente. Para lograr esto haremos uso de corpus ya etiquetados: para el inglés utilizaremos el Brown Corpus y para el español el CESS_ESP.\n",
    "\n",
    "Para el inglés:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tags=[tag for (word,tag) in brown.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para definir un etiquetador por defecto hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'NN'),\n",
       " ('refuse', 'NN'),\n",
       " ('to', 'NN'),\n",
       " ('permit', 'NN'),\n",
       " ('us', 'NN'),\n",
       " ('to', 'NN'),\n",
       " ('obtain', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=word_tokenize(oracio)\n",
    "default_tagger=nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, etiqueta todas las palabras con “NN”.\n",
    "\n",
    "Para el español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sps00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "tags=[tag for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos puede sorprender que la etiqueta más frecuente en español sea la correspondiente a la preposición ya que esperaríamos que fuera también la correspondiente a sustantivo. Lo que pasa es que en el etiquetario español los substantivos tienen diversas posiciones para la categoría y varias subcategoritzacions (género y número). Si modificamos las dos últimas líneas para hacer que el program mire sólo la primera posición de la etiqueta, obtendremos que la categoría más frecuente es 'n' (sustantivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags=[tag[0] for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos hacer también un etiquetador por defecto del español indicando que la etiqueta por defecto fuese 'n'. Pero realmente preferiríamos indicar la etiqueta completa más frecuente para los sustantivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sps00', 25272),\n",
       " ('ncms000', 11428),\n",
       " ('Fc', 11420),\n",
       " ('ncfs000', 11008),\n",
       " ('da0fs0', 6838),\n",
       " ('da0ms0', 6012),\n",
       " ('rg', 5937),\n",
       " ('Fp', 5866),\n",
       " ('cc', 5854),\n",
       " ('ncmp000', 5711)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags=[tag for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nos da que los nombres comunes masculinos singulares son ligeramente más frecuentes que los nombres comunes femeninos singulares. Ahora podemos definir un etiquetador por defecto para el español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mañana', 'ncms000'),\n",
       " ('por', 'ncms000'),\n",
       " ('la', 'ncms000'),\n",
       " ('mañana', 'ncms000'),\n",
       " ('lloverá', 'ncms000'),\n",
       " ('.', 'ncms000')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "oracion=\"Mañana por la mañana lloverá.\"\n",
    "tokens=word_tokenize(oracion)\n",
    "default_tagger=nltk.DefaultTagger('ncms000')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya nos podemos imaginar que este etiquetador no funcionará demasiado bien. En el siguiente apartado aprenderemos a evaluar etiquetadores y podremos ver la precisión de este etiquetador. También puede servir para los casos que tengamos una palabra desconocida, ya que le podremos asignar la etiqueta más frecuente (sin contar la de preposición, ya que siendo una categoría cerrada es prácticamente imposible que sea desconocida).\n",
    "\n",
    "### 5.4.2. El etiquetador por unigrames\n",
    "\n",
    "El etiquetador por defecto estudiado en el apartado anterior etiqueta todas las palabras con la etiqueta más frecuente en todo el corpus. En este apartado y los próximos estudiaremos una serie de etiquetadores llamados genéricamente etiquetadores por n-gramas. Un n-grama es una combinación de n elementos. En general estos etiquetadores aprenden a partir del corpus teniendo en cuenta un contexto de n palabras. En el caso del etiquetador para unigrames lo único que tenemos en cuenta es la propia palabra a etiquetar, sin ningún contexto. El etiquetador por unigrames etiquetará cada palabra con la etiqueta más frecuente para esa palabra.\n",
    "\n",
    "Con NLTK es muy sencillo crear un etiquetador por unigrames. Primero lo haremos para el inglés y luego para el español.\n",
    "\n",
    "Para el inglés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'PPSS'),\n",
       " ('refuse', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PPO'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'AT'),\n",
       " ('refuse', 'VB'),\n",
       " ('permit', 'VB')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents=brown.tagged_sents()\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=nltk.word_tokenize(oracio)\n",
    "unigram_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos en el resultado, vemos que refuser la etiqueta las dos veces que aparece como verbo, ya que esta es la estiqueta más frecuente para esta palabra.\n",
    "\n",
    "\n",
    "Para el español\n",
    "\n",
    "Haremos lo mismo, pero esta vez en un programa (programa-5-7.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mañana', 'rg'), ('por', 'sps00'), ('la', 'da0fs0'), ('mañana', 'rg'), ('lloverá', None), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.tokenize import word_tokenize\n",
    "tagged_sents=cess_esp.tagged_sents()\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"Mañana por la mañana lloverá.\"\n",
    "tokens=word_tokenize(oracio)\n",
    "analisis=unigram_tagger.tag(tokens)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar etiquetadores también podemos utilizar nuestros propios corpus etiquetados. En el siguiente ejemplo utilizaremos un fragmento del Wikicorpus del español. Si nos fijamos en el formato de este corpus etiquetado veremos que es forma, lema, etiqueta y probabilidad de la etiqueta separados por tabulador. Tendremos que crear un código capaz de leer este corpus y crear las tagged_sents como listas de tagged_words donde cada tagged_word es una tupla forma, etiqueta. Lo podemos hacer con el programa-5-8.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB Y\n",
    "#NO HAYAS SUBIDO TODAVÍA EL ARCHIVO fragmento-wikicorpus-tagged-spa.txt\n",
    "#Te pedirá que selecciones el archivo fragmento-wikicorpus-tagged-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=unigram_tagger.tag(tokens)\n",
    "print(analisi)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3. El etiquetador por bigramas\n",
    "\n",
    "El etiquetador por unigramas no tiene en cuenta el contexto de aparición de las palabras, y las etiqueta siempre con la etiqueta más frecuente para esa palabra. En este apartado vamos a construir un etiquetador por bigramas que tiene en cuenta la propia palabra a etiquetar y la palabra anterior:\n",
    "\n",
    "Para el inglés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 'PPSS'),\n",
       " ('refuse', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PPO'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'AT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents=brown.tagged_sents()\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents)\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=word_tokenize(oracio)\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "bigram_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijémonos que ahora puede etiquetar el segundo refuse como nombre, ya que tiene en cuenta el contexto inmediato.\n",
    "\n",
    "Para el español\n",
    "\n",
    "Modificamos el programa-5-8.py para obtener el programa-5-9.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mañana', None), ('por', None), ('la', None), ('mañana', None), ('lloverá', None)]\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos en la salida veremos que no ha podido etiquetar ninguna palabra. Esto se debe a un fenómeno conocido como dispersión de datos. Hay muchos más unigramas que bigramas en un corpus. Si entrenamos un etiquetador por bigramas, necesitaremos un corpus más grande para poder encontrar suficientes evidencias de cada bigrama de la oración a analizar, como no siempre es posible disponer de grandes corpus, se puede recurrir a la técnica conocida como backoff. A esta técnica también se la conoce como combinación de etiquetadores. En el programa-5-10.py combinamos un etiquetador por bigramas con uno por unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mañana', 'NCCS000'), ('por', 'SP'), ('la', 'DA0FS0'), ('mañana', 'NCCS000'), ('lloverá', None)]\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y como vemos, ha podido etiquetar la mayoría las palabras, ya que con el etiquetador por bigramas no ha podido, pero sí utilizando el de unigramas.\n",
    "\n",
    "\n",
    "\n",
    "### 5.4.5. El etiquetador por trigramas\n",
    "\n",
    "El etiquetador por trigramas etiqueta una palabra teniendo en cuenta el contexto formado por las dos palabras anteriores. En este caso, el problema de la dispersión de datos será aún más pronunciado. En el programa-5-11.py implementamos un etiquetador por trigramas que se combina con uno por bigramas y a su vez por uno por unigramas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mañana', 'NCCS000'), ('por', 'SP'), ('la', 'DA0FS0'), ('mañana', 'NCCS000'), ('lloverá', None)]\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(tagged_sents,backoff=bigram_tagger)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.5. Tratamiento de palabras desconocidas: diccionario morfológico, etiquetador por afijos y etiquetador por defecto\n",
    "\n",
    "En los programas anteriores hemos visto que un etiquetador entrenado con un corpus no será capaz de etiquetar palabras que no aparezcan en el corpus. Para solucionar esto utilizaremos 3 estrategias:\n",
    "\n",
    "- entrenar un etiquetador por unigramas a partir de un diccionario morfológico, que en este caso será el del propio Freeling y que se puede descargar junto con los programas de este capitulo.\n",
    "- entrenar un etiquetador por afijos que lo que hace es utilizar todas las palabras del diccionario morfológico (y en su defecto podrían utilizarse las propias palabras del corpus), para aprender las etiquetas más frecuentes según las terminaciones de las palabras.\n",
    "- y si todo esto falla, usar un etiquetador por defecto, que use la etiqueta más frecuente en nuestro corpus de aprendizaje\n",
    "\n",
    "Vemos primero estos tres entrenamientos por separado y luego lo combinaremos todo en un único etiquetador.\n",
    "\n",
    "\n",
    "Etiquetador por unigrames a partir de un diccionario morfológico\n",
    "\n",
    "Utilizaremos esta parte de código (programa-5-12.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB Y\n",
    "#NO HAYAS SUBIDO TODAVÍA EL ARCHIVO diccionario-freeling-spa.txt\n",
    "#Te pedirá que selecciones el archivo diccionario-freeling-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    cont+=1\n",
    "    if cont==10000:\n",
    "        break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\":\")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    tupla=(forma,etiqueta)\n",
    "    tagged_words.append(tupla)\n",
    "tagged_sents.append(tagged_words)\n",
    "\n",
    "unigram_tagger_diccionari=nltk.UnigramTagger(tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que contamos las líneas y cuando llega a la 10000 paramos el entrenamiento para que no tarde demasiado. Para hacer las pruebas será suficiente y en el momento de hacer el entrenamiento definitivo eliminaremos (comentaremos) estas líneas de código de manera que haga el entrenamiento con todo el diccionario. Este programa no ofrece ninguna salida.\n",
    "\n",
    "\n",
    "\n",
    "Entrenamiento de un etiquetador por afijos utilizando el diccionario morfológico\n",
    "\n",
    "Podemos encontrar la implementación en el programa-5-13.py que es exactamente igual que el anterior pero cambia la última línea y ahora es:\n",
    "\n",
    "\n",
    "```\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents, affix_length=-3, min_stem_length=2)1\n",
    "```\n",
    "\n",
    "\n",
    "que permite entrenar el etiquetador por afijos teniendo en cuenta los afijos con tres caracteres siempre y cuando la raíz que quede tenga 2 o más caracteres. Este programa tampoco devuelve ninguna salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    cont+=1\n",
    "    if cont==10000:\n",
    "        break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\":\")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    tupla=(forma,etiqueta)\n",
    "    tagged_words.append(tupla)\n",
    "tagged_sents.append(tagged_words)\n",
    "\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents, affix_length=-3, min_stem_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinación de la etiqueta más frecuente y entrenamiento del etiquetador por defecto\n",
    "\n",
    "El programa-5-14.py nos devuelve las 10 etiquetas más frecuentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta más frecuente:  [('SP', 574004), ('NP00000', 330331), ('NCMS000', 251790), ('NCFS000', 247815), ('Fc', 212993), ('Fp', 170279), ('DA0MS0', 170139), ('DA0FS0', 139776), ('CC', 122292), ('NCMP000', 107563)]\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "tags=[]\n",
    "for ts in tagged_sents:\n",
    "    for wt in ts:\n",
    "        tags.append(wt[1])\n",
    "\n",
    "mft=nltk.FreqDist(tags).most_common(10)\n",
    "print(\"Etiqueta más frecuente: \",mft)\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NP00000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y volvemos a obtener que la más frecuente (de las categorías abiertas) es el sustantivo, concretamente el nombre propio. Ahora podemos crear el etiquetador por defecto utilizando esta etiqueta\n",
    "\n",
    "Pondremos todo esto junto en el siguiente apartado, y además, aprenderemos a almacenar etiquetadores.\n",
    "\n",
    "## 5.5. Almacenamiento de etiquetadores\n",
    "\n",
    "En el apartado anterior hemos aprendido a entrenar y combinar etiquetadores. Cada vez que queríamos etiquetar una oración, entrenábamos un etiquetador y luego etiquetábamos. Como el entrenamiento es lento, nos interesa entrenar una vez y poder guardar el etiquetador ya entrenado de manera que lo podamos utilizar tantas veces como queramos.\n",
    "\n",
    "En el programa-5-15.py entrenamos y almacenamos un etiquetador. Fíjate en todos los etiquetadores diferentes que entrenamos, como los combinamos, y como finalmente almacenamos el último, que de hecho está combinado con todos los demás. Fíjate también como usamos el módulo pickle. Y también ten en cuenta que hemos comentado las líneas que limitan el número de entradas del diccionario morfológico que utiliza para entrenar. Si ves que tarda mucho en entrenar, vuelve a descomentar estas líneas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "tagged_sents_per_unigrams=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_sents_per_unigrams.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "if len(tagged_words)>0:\n",
    "    tagged_sents.append(tagged_words)\n",
    "    tagged_sents_per_unigrams.append(tagged_words)\n",
    "    tagged_words=[]\n",
    "        \n",
    "diccionario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "cont=0\n",
    "for linia in diccionario:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\":\")\n",
    "    if len(camps)>=3:\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "tagged_sents_per_unigrams.append(tagged_words)\n",
    "\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NP00000\")\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents_per_unigrams, affix_length=-3, min_stem_length=2,backoff=default_tagger)\n",
    "unigram_tagger_diccionari=nltk.UnigramTagger(tagged_sents_per_unigrams,backoff=affix_tagger)\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents,backoff=unigram_tagger_diccionari)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(tagged_sents,backoff=bigram_tagger)\n",
    "\n",
    "sortida=open('etiquetador-spa.pkl', 'wb')\n",
    "pickle.dump(trigram_tagger, sortida, -1)\n",
    "sortida.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que es importante tener en cuenta en el programa anterior es que estamos proporcionando un corpus y un diccionario. El corpus nos proporciona información de forma y etiqueta dentro del contexto de la oración. En cambio, el diccionario sólo nos da información sobre formas y sus etiquetas de una manera totalmente fuera de contexto, ya que la palabra que aparece en el diccionario antes de otra sólo guarda una relación alfabética. Por este motivo, en todos los entrenamientos que supongan un contexto (bigrama y trigrama) sólo podemos utilizar la información que proviene del corpus. En cambio, para los entrenamientos que no se tenga en cuenta el contexto (afijos y unigramas) podemos utilizar tanto la información que aparece en el corpus como la que aparece en el diccionario. Ten en cuenta que en el programa utilizamos dos listas para almacenar la información que utilizamos para entrenar:\n",
    "\n",
    "- tagged_sents_per_unigrams: donde ponemos la información del corpus y del diccionario.\n",
    "- tagged_sents: donde solo ponemos la información del corpus.\n",
    "\n",
    "Las líneas:\n",
    "\n",
    "for linia in entrada:\n",
    "```\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "```\n",
    "y\n",
    "```\n",
    "cont=0\n",
    "for linia in diccionario:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "```\n",
    "sirven para limitar la información que se carga o bien del corpus o bien del diccionario, o de ambos. Como el programa tarda mucho en ejecutarse, puedes descomentar (quitar el símbolo \"#\") de delante de las líneas.\n",
    "\n",
    "En los archivos de esta unidad encontrarás el etiquetador entrenado (etiquetador-spa.pkl) resultante para que lo podáis utilizar en el siguiente programa sin esperar a que se complete el entrenamiento.\n",
    "\n",
    "Ahora en el archivo etiquetador-spa.pkl tenemos un etiquetador que podemos cargar siempre que queramos de manera muy rápida. Fíjate en el programa-5-16.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "entrada=open('etiquetador-spa.pkl','rb')\n",
    "etiquetador=pickle.load(entrada)\n",
    "entrada.close()\n",
    "\n",
    "oracio=\"Mañana por la mañana lloverá.\"\n",
    "tokenitzador=nltk.tokenize.RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "analisis=etiquetador.tag(tokens)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Evaluación de etiquetadores\n",
    "\n",
    "En los apartados anteriores hemos aprendido a crear etiquetadores estadísticos. También hemos aprendido a usarlos para etiquetar textos e intuitivamente hemos visto que funcionan bastante bien, aunque pueden etiquetar mal algunas palabras. Cuando desarrollamos un etiquetador, nos interesaría saber qué precisión logramos. NLTK nos proporciona una manera muy sencilla de evaluar etiquetadores. Empezamos evaluando un etiquetador para el inglés (programa-5-17.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents=brown.tagged_sents()\n",
    "print(\"TOTAL ORACIONES:\", len(brown_tagged_sents))\n",
    "train_sents=brown_tagged_sents[:10000]\n",
    "test_sents=brown_tagged_sents[56001:]\n",
    "default_tagger=nltk.DefaultTagger(\"NN\")\n",
    "affix_tagger=nltk.AffixTagger(train_sents, affix_length=-3, min_stem_length=2)\n",
    "unigram_tagger=nltk.UnigramTagger(train_sents, backoff=affix_tagger)\n",
    "bigram_tagger=nltk.BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "precisio=trigram_tagger.evaluate(test_sents)\n",
    "print(\"PRECISION: \",precisio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa primero nos indicará el número total de oraciones del corpus. Fíjate que crea un conjunto de oraciones de entrenamiento (train_sents) con las primeras 10000 oraciones del corpus; y uno de test con las oraciones finales, de la 56001 hasta la final. Después de crear el etiquetador con el método evaluate evalúa la precisión utilizando las oraciones de test. Lo que hace el programa es etiquetar estas oraciones y compararlas con las etiquetas reales. Si ejecuta el programa, obtendrá la siguiente salida:\n",
    "```\n",
    "TOTAL ORACIONES: 57340\n",
    "\n",
    "PRECISION:  0.8931815568586869\n",
    "```\n",
    "Cambia ahora el número de oraciones para entrenar el etiquetador y pon el máximo (sin coger oraciones de test), es decir, 56000. Si\n",
    "\n",
    "ejecutas ahora el programa, la precisión pasa a ser de:\n",
    "```\n",
    "PRECISION: 0.9220420834770611\n",
    "```\n",
    "Vemos que cuanto mayor sea el corpus de entrenamiento, obtendremos una mejor precisión.\n",
    "\n",
    "Vamos a evaluar el etiquetador del español que hemos entrenado y almacenado en el apartado anterior. Lo hacemos en el programa-5-18.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import codecs\n",
    "\n",
    "#carreguem l'etiquetador\n",
    "entrada=open('etiquetador-spa.pkl','rb')\n",
    "etiquetador=pickle.load(entrada)\n",
    "entrada.close()\n",
    "\n",
    "#carreguem les oracions del corpus de test\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "test_tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            test_tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "precisio=etiquetador.evaluate(test_tagged_sents)\n",
    "print(\"PRECISION: \",precisio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos, utilizamos un fragmento nuevo de wikicorpus ya etiquetado (fragmento-wikicorpus-test-tagged-spa.txt). Primero cargamos el etiquetador almacenado y luego cargamos el nuevo corpus añadiéndolo al test_tagged_sents, que son las que utilizaremos para evaluar con el método evaluate. Este programa nos proporciona la siguiente salida:\n",
    "```\n",
    "PRECISION:  0.9938872748117409\n",
    "```\n",
    "Recordemos que el corpus que estamos usando es un corpus etiquetado automáticamente con Freeling, y por tanto, el 99.3% de precisión que obtenemos no es real. Deberíamos utilizar corpus etiquetados manualmente, o bien etiquetados automáticamente y revisados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
