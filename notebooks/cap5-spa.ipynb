{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Etiquetado morfosintático\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este módulo estudiaremos la tarea denominada etiquetado morfosintáctico (en inglés part-of-speech tagging o POS-tagging). Esta tarea consiste en asignar a cada palabra de un texto una categoría gramatical y otra información adicional (como pueden ser varias subcategorías, el lema asociado, etc.) Esta es una tarea fundamental en el procesamiento del lenguaje natural, aunque no está exenta de problemas que no están todavía totalmente resueltos. El lenguaje natural es ambiguo desde muchos puntos de vista, y también lo es en el morfosintáctico. Una determinada forma (como por ejemplo casa) puede tener varias interpretaciones morfosintácticas, puede ser un sustantivo común femenino singular (con lema casa) y también una forma de presente o de imperativo del verbo casar. Los etiquetadores morfosintácticos deberán proporcioanr la interpretación adecuada según el contexto de utilización; por lo tanto deberán desambiguar las diferentes posibilidades.\n",
    "\n",
    "Antes de abordar el etiquetado morfosintáctico veremos también el análisis morfológico, es decir, el análisis que nos permite determinar el lema y la categoría gramatical (y otras subcategorizaciones) de una determinada forma. Una vez seamos capaces de hacer el análisis morfológico, pasaremos a estudiar diversas técnicas que nos permitirán etiquetar textos desde el punto de vista morfosintáctico y que intentarán desambiguar (con mayor o menor éxito) las diferentes posibilidades.\n",
    "\n",
    "## 5.1. Morfología computacional\n",
    "\n",
    "### 5.1.1. El formalismo de descomposición morfológica\n",
    "\n",
    "Existen toda una serie de formalismos para describir la morfología de una lengua. En este capítulo sólo presentaremos uno, que es fácil de implementar y nos servirá para comprender los mecanismos fundamentales: el formalismo de descomposición morfológica (Alshawi, 1992).\n",
    "\n",
    "La idea básica de este formalismo es sencilla y se basa en dos tipos de conocimiento:\n",
    "\n",
    "Un diccionario que contiene información morfosintáctica base o la palabra que se considera forma de referencia.\n",
    "Reglas que contienen información sobre la morfología de la lengua.\n",
    "En el programa-5-1.py podemos ver una primera implementación muy sencilla de esta idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raiz=\"cant\"\n",
    "\n",
    "t1=\"o\"\n",
    "\n",
    "t2=\"as\"\n",
    "\n",
    "t3=\"a\"\n",
    "\n",
    "t4=\"amos\"\n",
    "\n",
    "t5=\"áis\"\n",
    "\n",
    "t6=\"an\"\n",
    "\n",
    "print(raiz+t1)\n",
    "\n",
    "print(raiz+t2)\n",
    "\n",
    "print(raiz+t3)\n",
    "\n",
    "print(raiz+t4)\n",
    "\n",
    "print(raiz+t5)\n",
    "\n",
    "print(raiz+t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este programa definimos una serie de variables: una que contiene el lema de un verbo y otros que contienen las terminaciones de presente de indicativo. Mediante el operador + que concatena cadenas obtenemos las formas correspondientes al presente de indicativo.\n",
    "\n",
    "Siguiendo esta idea básica crearemos un formalismo para expresar las reglas (que además de las formas queremos que nos den una etiqueta de categoría gramatical y subcategorizaciones). También utilizaremos un formalismo para expresar el diccionario de lemas, que además del propio lema nos indicará el tipo de flexión que sigue (es decir, el paradigma flexivo).\n",
    "\n",
    "Como formalismo para las reglas proponemos una serie de valores separados por dos puntos (:):\n",
    "```\n",
    "terminación_forma:terminación_lema:etiqueta:paradigma\n",
    "```\n",
    "\n",
    "Por ejemplo:\n",
    "```\n",
    "\n",
    "o:ar:VMIP1S:V1\n",
    "\n",
    "as:ar:VMIP2S:V1\n",
    "\n",
    "a:ar:VMIP3S:V1\n",
    "\n",
    "amos:ar:VMIP1P:V1\n",
    "\n",
    "áis:ar:VMIP2P:V1\n",
    "\n",
    "an:ar:VMIP3P:V1\n",
    "```\n",
    "\n",
    "Para las etiquetas utilizamos las etiquetas EAGLES para el castellano. En el archivo reglas.txt podemos observar un conjunto más extenso de reglas morfológicas.\n",
    "\n",
    "Para el formalismo para el diccionario de lemas seguiremos una idea similar:\n",
    "```\n",
    "lema: paradigma\n",
    "\n",
    "```\n",
    "Que en el ejemplo sería:\n",
    "```\n",
    "cantar: V1\n",
    "```\n",
    "En el archivo diccionario.txt podemos observar un diccionario más completo.\n",
    "\n",
    "Ahora necesitamos un programa que nos permita generar todas las formas a partir de las reglas y el diccionario (programa-5-2.py). Ejecuta el programa y observa la salida. Si trabajas con tu ordenador tienes que tener los archivos reglas.txt y diccionario.txt en el mismo directorio que este notebook. Si trabajas en Google Colab, ejecuta la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB.\n",
    "#Te pedirá que selecciones el archivo reglas.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "#Te pedirá que selecciones el archivo diccionario.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freglas=open(\"reglas.txt\",\"r\")\n",
    "\n",
    "reglas=[]\n",
    "\n",
    "while True:\n",
    "\tlinea=freglas.readline().rstrip()\n",
    "\tif not linea:break\n",
    "\treglas.append(linea)\n",
    "freglas.close()\n",
    "\n",
    "fdiccionario=open(\"diccionario.txt\",\"r\")\n",
    "\n",
    "while True:\n",
    "\tlinea=fdiccionario.readline().rstrip()\n",
    "\tif not linea:break\n",
    "\t(lema,tipo)=linea.split(\":\")\n",
    "\tfor regla in reglas:\n",
    "\t\t(tf,tl,etiqueta,tipo2)=regla.split(\":\")\n",
    "\t\tif ((tipo2 == tipo)&(lema.endswith(tl))):\n",
    "\t\t\tprint(lema[0:(len(lema)-len(tl))]+tf,lema,etiqueta)\n",
    "\n",
    "fdiccionario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos hablando de análisis morfológico, pero en realidad lo que hemos hecho es un programa que genera formas con su lema y una etiqueta morfosintáctica, lo que llamamos diccionario morfológico. Los programas de análisis morfológico a menudo funcionan con un diccionario morfológico.\n",
    "\n",
    "En el siguiente programa (programa-5-3.py) cargamos un gran diccionario morfológico del castellano obtenido del analizador Freeling (diccionario-freeling-spa.txt) y llevamos a cabo el análisis de las palabras que indica el usuario (el programa finaliza cuando el usuario introduce un espacio en blanco). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB.\n",
    "#Te pedirá que selecciones el archivo diccionario-freeling-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "diccionario={}\n",
    "archivo_diccipnario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in archivo_diccipnario:\n",
    "    entrada=entrada.rstrip()\n",
    "    campos=entrada.split(\":\")\n",
    "    forma=campos[0]\n",
    "    lema=campos[1]\n",
    "    etiqueta=campos[2]\n",
    "    diccionario[forma]=diccionario.get(forma,\"\")+\":\"+lema+\"\\t\"+etiqueta\n",
    "print(\"DICCIONARIO CARGADO\")\n",
    "while 1:\n",
    "    palabra=input(\"Introduce la palabra a analizar: \")\n",
    "    if palabra==\" \":\n",
    "        break\n",
    "    if palabra in diccionario:\n",
    "        print(\"ANALISIS:\",palabra,diccionario[palabra])\n",
    "    else:\n",
    "        print(\"PALABRA DESCONOCIDA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Análisis morfológico\n",
    "\n",
    "En este apartado vamos a construir un analizador morfológico, es decir, un programa que es capaz de darnos los análisis morfológicos de todas las palabras de un texto. Para las palabras ambiguas desde el punto de vista morfosintáctico, nos devolverá todas las posibles interpretaciones. Con lo que hemos hecho hasta ahora tenemos todos los componentes:\n",
    "\n",
    "- Un programa que cargue el diccionario morfológico del español (el programa-5-3.py)\n",
    "- Un programa capaz de leer un documento y segmentarlo en oraciones y tokenizarlo (por ejemplo, el programa-4-3.py que vimos en la unidad anterior)\n",
    "\n",
    "En el programa-5-4.py podemos observar una primera versión del programa (que analizará el archivo noticia.txt, que contiene un fragmento de noticia). Utiliza también el spanish.pickle que hemos creado en el capítulo anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB Y\n",
    "#NO HAS SUBIDO TODAVÍA EL ARCHIVO diccionario-freeling-spa.txt\n",
    "#Te pedirá que selecciones el archivo diccionario-freeling-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB \n",
    "#Te pedirá que selecciones el archivo noticia.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "#Después te pedirá que selecciones el archivo spanish.pickle de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "diccionario={}\n",
    "archivo_diccionario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in archivo_diccionario:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\":\")\n",
    "    if len(camps)>=3:\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        if forma in diccionario:\n",
    "            diccionario[forma]=diccionario.get(forma,\"\")+\" \"+lema+\" \"+etiqueta\n",
    "        else:\n",
    "            diccionario[forma]=lema+\" \"+etiqueta\n",
    "\n",
    "segmentador= nltk.data.load(\"spanish.pickle\")\n",
    "tokenitzador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'noticia.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "for forma in corpus.words():\n",
    "    if forma in diccionario:\n",
    "        info=diccionario[forma]\n",
    "    else:\n",
    "        info=\"DESCONOCIDA\"\n",
    "    print(forma+\" \"+info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, hay algunos problemas, como por ejemplo:\n",
    "\n",
    "- Las palabras que están en la primera posición de una oración, y por lo tanto están escritas en mayúsculas, a pesar de estar en el diccionario, las etiqueta como desconocidas. Para solucionar esto, primero buscaremos el diccionario las palabras tal y como aparecen en el texto, si no las encuentra, entonces las buscará pasada a minúscula, y si aún así no la encuentra, la marcará como desconocida.\n",
    "- Los signos de puntuación los marca como desconocidos, ya que son tokens que no aparecen en el diccionario morfológico. La solución es sencilla y consiste en incluir los signos de puntuación en el diccionario morfológico o bien ponerlas en el propio programa. \n",
    "\n",
    "Veamos la nueva implementación en el programa-5-5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "diccionario={}\n",
    "archivo_diccionario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "for entrada in archivo_diccionario:\n",
    "    entrada=entrada.rstrip()\n",
    "    camps=entrada.split(\":\")\n",
    "    if len(camps)>=3:\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        if forma in diccionario:\n",
    "            diccionario[forma]=diccionario.get(forma,\"\")+\" \"+lema+\" \"+etiqueta\n",
    "        else:\n",
    "            diccionario[forma]=lema+\" \"+etiqueta\n",
    "\n",
    "#Añadimos los signos de puntuación\n",
    "diccionario['\"']='\" Fe'\n",
    "diccionario[\"'\"]=\"' Fe\"\n",
    "diccionario['.']='. Fp'\n",
    "diccionario[',']=', Fc'\n",
    "diccionario[';']='; Fx'\n",
    "diccionario[':']=': Fd'\n",
    "diccionario['(']='( Fpa'\n",
    "diccionario[')']=') Fpt'\n",
    "diccionario['[']='[ Fca'\n",
    "diccionario[']']='] Fct'\n",
    "\n",
    "\n",
    "segmentador= nltk.data.load(\"spanish.pickle\")\n",
    "tokenitzador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'noticia.txt',word_tokenizer=tokenitzador,sent_tokenizer=segmentador)\n",
    "\n",
    "for forma in corpus.words():\n",
    "    if forma in diccionario:\n",
    "        info=diccionario[forma]\n",
    "    elif forma.lower() in diccionario:\n",
    "        info=diccionario[forma.lower()]\n",
    "    else:\n",
    "        info=\"DESCONOCIDA\"\n",
    "    print(forma+\" \"+info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún quedan problemas por resolver, como los nombres propios, algún aspecto mal tokenizado, etc. También pueden aparecer palabras desconocidas que son correctas, pero que no estén recogidas en el diccionario morfológico, etc. En próximos apartados de este mismo capítulo iremos presentando soluciones a estas cuestiones.\n",
    "\n",
    "## 5.3. Etiquetado morfosintáctico\n",
    "\n",
    "### 5.3.1. Introducción\n",
    "\n",
    "En este módulo estudiaremos la tarea llamada etiquetado morfosintáctico (en inglés part-of-speech tagging o POS-tagging). Esta tarea consiste en asignar a cada palabra de un texto una categoría gramatical y otra información adicional (como pueden ser varias subcategorías, el lema asociado, etc.) Esta es una tarea fundamental en el procesamiento del lenguaje natural, aunque no está exenta de problemas que no están todavía totalmente resueltos.\n",
    "\n",
    "El lenguaje natural es ambiguo desde muchos puntos de vista, y también lo es en el morfosintáctico. Una determinada forma (como por ejemplo casa) puede tener varias interpretaciones morfosintácticas, puede ser un sustantivo común femenino singular (con lema casa) y también una forma de presente o de imperativo del verbo casar. Los etiquetadores morfosintácticos deberán intentar dar la interpretación adecuada según el contexto donde aparece una palabra; por lo tanto deberán desambiguar las diferentes posibilidades.\n",
    "\n",
    "En el módulo veremos varias técnicas que nos permitirán etiquetar textos desde el punto de vista morfosintáctico y que intentarán desambiguar (con mayor o menor éxito) las diferentes posibilidades.\n",
    "\n",
    "\n",
    "### 5.3.2. Etiquetado morfosintáctico vs. análisis morfológico\n",
    "\n",
    "En el apartado anterior hemos estudiado la función denominada análisis morfológico, que consiste en asignar a cada palabra de un texto todas los posibles análisis morfológicos. \n",
    "\n",
    "Un analizador morfosintáctico ofrece la misma salida, pero desambiguada, es decir, elige una de las posibilidades de cada palabra.\n",
    "\n",
    "El etiquetado morfosintáctico es una tarea básica para muchas tareas de procesamiento del lenguaje natural. Si queremos hacer un análisis sintáctico de una oración, un paso previo es conocer la categoría gramatical de cada palabra. Disponer de textos etiquetados a escala morfosintáctica es interesante para muchos estudios. Podemos saber cuáles son los sustantivos más utilizados en un corpus, ver todas las apariciones de un verbo independientemente de la forma concreta, etc. El etiquetado morfosintáctico también se utiliza para extraer los términos más relevantes de un determinado documento o conjunto de documentos. Estas técnicas también se utilizan para la clasificación de documentos y recuperación de información.\n",
    "\n",
    "### 5.3.4. Etiquetador para el inglés\n",
    "\n",
    "NLTK proporciona un etiquetador para el inglés que funciona bastante bien y que se puede usar fácilmente de manera directa, como en el programa-5-6.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "oracion=\"They refuse to permit us to obtain the refuse permit\"\n",
    "palabras = nltk.tokenize.word_tokenize(oracion)\n",
    "analisis=nltk.pos_tag(palabras)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que tener en cuenta que la palabra permit la ha etiquetado correctamente como verbo y como sustantivo. Las etiquetas utilizadas son las propias de un etiquetario (tagset), o conjunto de etiquetas, determinado. En este caso concreto se utilizan el tagset WSJ (Wall Street Journal), que es el siguiente:\n",
    "\n",
    "Tag - Function\n",
    "\n",
    "CC - coordinating conjunction\n",
    "\n",
    "CD - cardinal number\n",
    "\n",
    "DT - determiner\n",
    "\n",
    "EX - existential there\n",
    "\n",
    "FW - foreign word\n",
    "\n",
    "IN - preposition\n",
    "\n",
    "JJ - adjective\n",
    "\n",
    "JJR - adjective, comparative\n",
    "\n",
    "JJS - adjective, superlative\n",
    "\n",
    "MD - modal\n",
    "\n",
    "NN - non-plural common noun\n",
    "\n",
    "NNP - non-plural proper noun\n",
    "\n",
    "NNPS - plural proper noun\n",
    "\n",
    "NNS - plural common noun\n",
    "\n",
    "of - the word of\n",
    "\n",
    "PDT - pre-determiner\n",
    "\n",
    "POS - posessive\n",
    "\n",
    "PRP - pronoun\n",
    "\n",
    "puncf - final punctuation (period, question mark and exclamation mark)\n",
    "\n",
    "punc - other punction\n",
    "\n",
    "RB - adverb\n",
    "\n",
    "RBR - adverb, comparative\n",
    "\n",
    "RBS - adverb, superlative\n",
    "\n",
    "RP - particle\n",
    "\n",
    "TO - the word to\n",
    "\n",
    "UH - interjection\n",
    "\n",
    "VB - verb, base form\n",
    "\n",
    "VBD - verb, past tense\n",
    "\n",
    "VBG - verb, gerund or present participle\n",
    "\n",
    "VBN - verb, past participle\n",
    "\n",
    "VBP - verb, non-3rd person\n",
    "\n",
    "VBZ - verb, 3rd person\n",
    "\n",
    "WDT - wh-determiner\n",
    "\n",
    "WP - wh-pronoun\n",
    "\n",
    "WRB - wh-adverb\n",
    "\n",
    "sym - symbol\n",
    "\n",
    "2 - ambiguously labelled\n",
    "\n",
    "Como veremos cuando empezamos a construir etiquetadores para otras lenguas, como el español, las etiquetas que utilizaremos serán diferentes. Hay una propuesta de etiquetario universal (universal tagset). A continuación podemos observar este etiquetario.\n",
    "\n",
    "Tag Meaning - English Examples\n",
    "\n",
    "ADJ - adjective - new, good, high, special, big, local\n",
    "\n",
    "ADP - adposition - on, of, at, with, by, into, under\n",
    "\n",
    "ADV - adverb - really, already, still, early, now\n",
    "\n",
    "CONJ - conjunction - and, or, but, if, while, although\n",
    "\n",
    "DET - determiner, article - the, a, some, most, every, no, which\n",
    "\n",
    "NOUN - noun - year, home, costs, time, Africa\n",
    "\n",
    "NUM - numeral - twenty-four, fourth, 1991, 14:24\n",
    "\n",
    "PRT - particle - at, on, out, over per, that, up, with\n",
    "\n",
    "PRON  - pronoun - he, their, her, its, my, I, us\n",
    "\n",
    "VERB - verb - is, say, told, given, playing, would\n",
    "\n",
    ". - punctuation marks-  . , ; !\n",
    "\n",
    "X - other - ersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "NLTK nos permite convertir las etiquetas de un determinado etiquetario a las del etiquetario universal. Lo podemos ver en el programa-5-6b.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "oracion=\"They refuse to permit us to obtain the refuse permit\"\n",
    "palabras = nltk.tokenize.word_tokenize(oracion)\n",
    "analisis=nltk.pos_tag(palabras)\n",
    "for ana in analisis:\n",
    "    forma=ana[0]\n",
    "    etiqueta=ana[1]\n",
    "    universal=nltk.tag.mapping.map_tag('en-ptb', 'universal', etiqueta)\n",
    "    print(forma,etiqueta,universal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.4. Entrenamiento de etiquetadores\n",
    "\n",
    "### 5.4.1. El etiquetador por defecto\n",
    "\n",
    "En este apartado se presenta un etiquetador muy simple, que lo único que hace es etiquetar todas las palabras con una etiqueta determinada, es decir, etiqueta todas las palabras con la misma etiqueta. Para determinar qué etiqueta elegiremos lo que haremos primero es calcular cuál es la etiqueta más frecuente. Para lograr esto haremos uso de corpus ya etiquetados: para el inglés utilizaremos el Brown Corpus y para el español el CESS_ESP.\n",
    "\n",
    "Para el inglés:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tags=[tag for (word,tag) in brown.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para definir un etiquetador por defecto hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=word_tokenize(oracio)\n",
    "default_tagger=nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, etiqueta todas las palabras con “NN”.\n",
    "\n",
    "Para el español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "tags=[tag for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos puede sorprender que la etiqueta más frecuente en español sea la correspondiente a la preposición ya que esperaríamos que fuera también la correspondiente a sustantivo. Lo que pasa es que en el etiquetario español los substantivos tienen diversas posiciones para la categoría y varias subcategoritzacions (género y número). Si modificamos las dos últimas líneas para hacer que el programa mire solo la primera posición de la etiqueta, obtendremos que la categoría más frecuente es 'n' (sustantivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[tag[0] for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos hacer también un etiquetador por defecto del español indicando que la etiqueta por defecto fuese 'n'. Pero realmente preferiríamos indicar la etiqueta completa más frecuente para los sustantivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[tag for (word,tag) in cess_esp.tagged_words()]\n",
    "nltk.FreqDist(tags).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nos da que los nombres comunes masculinos singulares son ligeramente más frecuentes que los nombres comunes femeninos singulares. Ahora podemos definir un etiquetador por defecto para el español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "oracion=\"Mañana por la mañana lloverá.\"\n",
    "tokens=word_tokenize(oracion)\n",
    "default_tagger=nltk.DefaultTagger('ncms000')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya nos podemos imaginar que este etiquetador no funcionará demasiado bien. En el siguiente apartado aprenderemos a evaluar etiquetadores y podremos ver la precisión de este etiquetador. También puede servir para los casos que tengamos una palabra desconocida, ya que le podremos asignar la etiqueta más frecuente (sin contar la de preposición, ya que siendo una categoría cerrada es prácticamente imposible que sea desconocida).\n",
    "\n",
    "### 5.4.2. El etiquetador por unigrames\n",
    "\n",
    "El etiquetador por defecto estudiado en el apartado anterior etiqueta todas las palabras con la etiqueta más frecuente en todo el corpus. En este apartado y los próximos estudiaremos una serie de etiquetadores llamados genéricamente etiquetadores por n-gramas. Un n-grama es una combinación de n elementos. En general estos etiquetadores aprenden a partir del corpus teniendo en cuenta un contexto de n palabras. En el caso del etiquetador para unigramas lo único que tenemos en cuenta es la propia palabra a etiquetar, sin ningún contexto. El etiquetador por unigrames etiquetará cada palabra con la etiqueta más frecuente para esa palabra.\n",
    "\n",
    "Con NLTK es muy sencillo crear un etiquetador por unigramas. Primero lo haremos para el inglés y luego para el español.\n",
    "\n",
    "Para el inglés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents=brown.tagged_sents()\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=nltk.word_tokenize(oracio)\n",
    "unigram_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos en el resultado, vemos que refuser la etiqueta las dos veces que aparece como verbo, ya que esta es la estiqueta más frecuente para esta palabra.\n",
    "\n",
    "\n",
    "Para el español\n",
    "\n",
    "Haremos lo mismo, pero esta vez en un programa (programa-5-7.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.tokenize import word_tokenize\n",
    "tagged_sents=cess_esp.tagged_sents()\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"Mañana por la mañana lloverá.\"\n",
    "tokens=word_tokenize(oracio)\n",
    "analisis=unigram_tagger.tag(tokens)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar etiquetadores también podemos utilizar nuestros propios corpus etiquetados. En el siguiente ejemplo utilizaremos un fragmento del Wikicorpus del español. Si nos fijamos en el formato de este corpus etiquetado veremos que es forma, lema, etiqueta y probabilidad de la etiqueta separados por tabulador. Tendremos que crear un código capaz de leer este corpus y crear las tagged_sents como listas de tagged_words donde cada tagged_word es una tupla forma, etiqueta. Lo podemos hacer con el programa-5-8.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB Y\n",
    "#NO HAYAS SUBIDO TODAVÍA EL ARCHIVO fragmento-wikicorpus-tagged-spa.txt\n",
    "#Te pedirá que selecciones el archivo fragmento-wikicorpus-tagged-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=unigram_tagger.tag(tokens)\n",
    "print(analisi)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3. El etiquetador por bigramas\n",
    "\n",
    "El etiquetador por unigramas no tiene en cuenta el contexto de aparición de las palabras, y las etiqueta siempre con la etiqueta más frecuente para esa palabra. En este apartado vamos a construir un etiquetador por bigramas que tiene en cuenta la propia palabra a etiquetar y la palabra anterior:\n",
    "\n",
    "Para el inglés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tagged_sents=brown.tagged_sents()\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents)\n",
    "oracio=\"they refuse to permit us to obtain the refuse permit\"\n",
    "tokens=word_tokenize(oracio)\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "bigram_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijémonos que ahora puede etiquetar el segundo refuse como nombre, ya que tiene en cuenta el contexto inmediato.\n",
    "\n",
    "Para el español\n",
    "\n",
    "Modificamos el programa-5-8.py para obtener el programa-5-9.py:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos en la salida veremos que no ha podido etiquetar ninguna palabra. Esto se debe a un fenómeno conocido como dispersión de datos. Hay muchos más unigramas que bigramas en un corpus. Si entrenamos un etiquetador por bigramas, necesitaremos un corpus más grande para poder encontrar suficientes evidencias de cada bigrama de la oración a analizar, como no siempre es posible disponer de grandes corpus, se puede recurrir a la técnica conocida como backoff. A esta técnica también se la conoce como combinación de etiquetadores. En el programa-5-10.py combinamos un etiquetador por bigramas con uno por unigramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y como vemos, ha podido etiquetar la mayoría las palabras, ya que con el etiquetador por bigramas no ha podido, pero sí utilizando el de unigramas.\n",
    "\n",
    "\n",
    "\n",
    "### 5.4.5. El etiquetador por trigramas\n",
    "\n",
    "El etiquetador por trigramas etiqueta una palabra teniendo en cuenta el contexto formado por las dos palabras anteriores. En este caso, el problema de la dispersión de datos será aún más pronunciado. En el programa-5-11.py implementamos un etiquetador por trigramas que se combina con uno por bigramas y a su vez por uno por unigramas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(tagged_sents,backoff=bigram_tagger)\n",
    "oracio=\"mañana por la mañana lloverá\"\n",
    "tokens=nltk.tokenize.word_tokenize(oracio)\n",
    "analisi=bigram_tagger.tag(tokens)\n",
    "print(analisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.5. Tratamiento de palabras desconocidas: diccionario morfológico, etiquetador por afijos y etiquetador por defecto\n",
    "\n",
    "En los programas anteriores hemos visto que un etiquetador entrenado con un corpus no será capaz de etiquetar palabras que no aparezcan en el corpus. Para solucionar esto utilizaremos 3 estrategias:\n",
    "\n",
    "- entrenar un etiquetador por unigramas a partir de un diccionario morfológico, que en este caso será el del propio Freeling y que se puede descargar junto con los programas de este capitulo.\n",
    "- entrenar un etiquetador por afijos que lo que hace es utilizar todas las palabras del diccionario morfológico (y en su defecto podrían utilizarse las propias palabras del corpus), para aprender las etiquetas más frecuentes según las terminaciones de las palabras.\n",
    "- y si todo esto falla, usar un etiquetador por defecto, que use la etiqueta más frecuente en nuestro corpus de aprendizaje\n",
    "\n",
    "Vemos primero estos tres entrenamientos por separado y luego lo combinaremos todo en un único etiquetador.\n",
    "\n",
    "\n",
    "Etiquetador por unigrames a partir de un diccionario morfológico\n",
    "\n",
    "Utilizaremos esta parte de código (programa-5-12.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB Y\n",
    "#NO HAYAS SUBIDO TODAVÍA EL ARCHIVO diccionario-freeling-spa.txt\n",
    "#Te pedirá que selecciones el archivo diccionario-freeling-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    cont+=1\n",
    "    if cont==10000:\n",
    "        break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\":\")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    tupla=(forma,etiqueta)\n",
    "    tagged_words.append(tupla)\n",
    "tagged_sents.append(tagged_words)\n",
    "\n",
    "unigram_tagger_diccionari=nltk.UnigramTagger(tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que contamos las líneas y cuando llega a la 10000 paramos el entrenamiento para que no tarde demasiado. Para hacer las pruebas será suficiente y en el momento de hacer el entrenamiento definitivo eliminaremos (comentaremos) estas líneas de código de manera que haga el entrenamiento con todo el diccionario. Este programa no ofrece ninguna salida.\n",
    "\n",
    "\n",
    "\n",
    "Entrenamiento de un etiquetador por afijos utilizando el diccionario morfológico\n",
    "\n",
    "Podemos encontrar la implementación en el programa-5-13.py que es exactamente igual que el anterior pero cambia la última línea y ahora es:\n",
    "\n",
    "\n",
    "```\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents, affix_length=-3, min_stem_length=2)1\n",
    "```\n",
    "\n",
    "\n",
    "que permite entrenar el etiquetador por afijos teniendo en cuenta los afijos con tres caracteres siempre y cuando la raíz que quede tenga 2 o más caracteres. Este programa tampoco devuelve ninguna salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    cont+=1\n",
    "    if cont==10000:\n",
    "        break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\":\")\n",
    "    forma=camps[0]\n",
    "    lema=camps[1]\n",
    "    etiqueta=camps[2]\n",
    "    tupla=(forma,etiqueta)\n",
    "    tagged_words.append(tupla)\n",
    "tagged_sents.append(tagged_words)\n",
    "\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents, affix_length=-3, min_stem_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinación de la etiqueta más frecuente y entrenamiento del etiquetador por defecto\n",
    "\n",
    "El programa-5-14.py nos devuelve las 10 etiquetas más frecuentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "tags=[]\n",
    "for ts in tagged_sents:\n",
    "    for wt in ts:\n",
    "        tags.append(wt[1])\n",
    "\n",
    "mft=nltk.FreqDist(tags).most_common(10)\n",
    "print(\"Etiqueta más frecuente: \",mft)\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NP00000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y volvemos a obtener que la más frecuente (de las categorías abiertas) es el sustantivo, concretamente el nombre propio. Ahora podemos crear el etiquetador por defecto utilizando esta etiqueta\n",
    "\n",
    "Pondremos todo esto junto en el siguiente apartado, y además, aprenderemos a almacenar etiquetadores.\n",
    "\n",
    "## 5.5. Almacenamiento de etiquetadores\n",
    "\n",
    "En el apartado anterior hemos aprendido a entrenar y combinar etiquetadores. Cada vez que queríamos etiquetar una oración, entrenábamos un etiquetador y luego etiquetábamos. Como el entrenamiento es lento, nos interesa entrenar una vez y poder guardar el etiquetador ya entrenado de manera que lo podamos utilizar tantas veces como queramos.\n",
    "\n",
    "En el programa-5-15.py entrenamos y almacenamos un etiquetador. Fíjate en todos los etiquetadores diferentes que entrenamos, como los combinamos, y como finalmente almacenamos el último, que de hecho está combinado con todos los demás. Fíjate también como usamos el módulo pickle. Y también ten en cuenta que hemos comentado las líneas que limitan el número de entradas del diccionario morfológico que utiliza para entrenar. Si ves que tarda mucho en entrenar, vuelve a descomentar estas líneas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "tagged_sents=[]\n",
    "tagged_sents_per_unigrams=[]\n",
    "cont=0\n",
    "for linia in entrada:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            tagged_sents.append(tagged_words)\n",
    "            tagged_sents_per_unigrams.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "if len(tagged_words)>0:\n",
    "    tagged_sents.append(tagged_words)\n",
    "    tagged_sents_per_unigrams.append(tagged_words)\n",
    "    tagged_words=[]\n",
    "        \n",
    "diccionario=codecs.open(\"diccionario-freeling-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "cont=0\n",
    "for linia in diccionario:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "    linia=linia.rstrip()\n",
    "    camps=linia.split(\":\")\n",
    "    if len(camps)>=3:\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "tagged_sents_per_unigrams.append(tagged_words)\n",
    "\n",
    "\n",
    "default_tagger=nltk.DefaultTagger(\"NP00000\")\n",
    "affix_tagger=nltk.AffixTagger(tagged_sents_per_unigrams, affix_length=-3, min_stem_length=2,backoff=default_tagger)\n",
    "unigram_tagger_diccionari=nltk.UnigramTagger(tagged_sents_per_unigrams,backoff=affix_tagger)\n",
    "unigram_tagger=nltk.UnigramTagger(tagged_sents,backoff=unigram_tagger_diccionari)\n",
    "bigram_tagger=nltk.BigramTagger(tagged_sents,backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(tagged_sents,backoff=bigram_tagger)\n",
    "\n",
    "sortida=open('etiquetador-spa.pkl', 'wb')\n",
    "pickle.dump(trigram_tagger, sortida, -1)\n",
    "sortida.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que es importante tener en cuenta en el programa anterior es que estamos proporcionando un corpus y un diccionario. El corpus nos proporciona información de forma y etiqueta dentro del contexto de la oración. En cambio, el diccionario sólo nos da información sobre formas y sus etiquetas de una manera totalmente fuera de contexto, ya que la palabra que aparece en el diccionario antes de otra sólo guarda una relación alfabética. Por este motivo, en todos los entrenamientos que supongan un contexto (bigrama y trigrama) sólo podemos utilizar la información que proviene del corpus. En cambio, para los entrenamientos que no se tenga en cuenta el contexto (afijos y unigramas) podemos utilizar tanto la información que aparece en el corpus como la que aparece en el diccionario. Ten en cuenta que en el programa utilizamos dos listas para almacenar la información que utilizamos para entrenar:\n",
    "\n",
    "- tagged_sents_per_unigrams: donde ponemos la información del corpus y del diccionario.\n",
    "- tagged_sents: donde solo ponemos la información del corpus.\n",
    "\n",
    "Las líneas:\n",
    "\n",
    "for linia in entrada:\n",
    "```\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "```\n",
    "y\n",
    "```\n",
    "cont=0\n",
    "for linia in diccionario:\n",
    "    #cont+=1\n",
    "    #if cont==10000:\n",
    "    #    break\n",
    "```\n",
    "sirven para limitar la información que se carga o bien del corpus o bien del diccionario, o de ambos. Como el programa tarda mucho en ejecutarse, puedes descomentar (quitar el símbolo \"#\") de delante de las líneas.\n",
    "\n",
    "En los archivos de esta unidad encontrarás el etiquetador entrenado (etiquetador-spa.pkl) resultante para que lo podáis utilizar en el siguiente programa sin esperar a que se complete el entrenamiento.\n",
    "\n",
    "Ahora en el archivo etiquetador-spa.pkl tenemos un etiquetador que podemos cargar siempre que queramos de manera muy rápida. Fíjate en el programa-5-16.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "entrada=open('etiquetador-spa.pkl','rb')\n",
    "etiquetador=pickle.load(entrada)\n",
    "entrada.close()\n",
    "\n",
    "oracio=\"Mañana por la mañana lloverá.\"\n",
    "tokenitzador=nltk.tokenize.RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "tokens=tokenitzador.tokenize(oracio)\n",
    "analisis=etiquetador.tag(tokens)\n",
    "print(analisis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Evaluación de etiquetadores\n",
    "\n",
    "En los apartados anteriores hemos aprendido a crear etiquetadores estadísticos. También hemos aprendido a usarlos para etiquetar textos e intuitivamente hemos visto que funcionan bastante bien, aunque pueden etiquetar mal algunas palabras. Cuando desarrollamos un etiquetador, nos interesaría saber qué precisión logramos. NLTK nos proporciona una manera muy sencilla de evaluar etiquetadores. Empezamos evaluando un etiquetador para el inglés (programa-5-17.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents=brown.tagged_sents()\n",
    "print(\"TOTAL ORACIONES:\", len(brown_tagged_sents))\n",
    "train_sents=brown_tagged_sents[:10000]\n",
    "test_sents=brown_tagged_sents[56001:]\n",
    "default_tagger=nltk.DefaultTagger(\"NN\")\n",
    "affix_tagger=nltk.AffixTagger(train_sents, affix_length=-3, min_stem_length=2)\n",
    "unigram_tagger=nltk.UnigramTagger(train_sents, backoff=affix_tagger)\n",
    "bigram_tagger=nltk.BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "trigram_tagger=nltk.TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "precisio=trigram_tagger.evaluate(test_sents)\n",
    "print(\"PRECISION: \",precisio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa primero nos indicará el número total de oraciones del corpus. Fíjate que crea un conjunto de oraciones de entrenamiento (train_sents) con las primeras 10000 oraciones del corpus; y uno de test con las oraciones finales, de la 56001 hasta la final. Después de crear el etiquetador con el método evaluate evalúa la precisión utilizando las oraciones de test. Lo que hace el programa es etiquetar estas oraciones y compararlas con las etiquetas reales. Si ejecuta el programa, obtendrá la siguiente salida:\n",
    "```\n",
    "TOTAL ORACIONES: 57340\n",
    "\n",
    "PRECISION:  0.8931815568586869\n",
    "```\n",
    "Cambia ahora el número de oraciones para entrenar el etiquetador y pon el máximo (sin coger oraciones de test), es decir, 56000. Si\n",
    "\n",
    "ejecutas ahora el programa, la precisión pasa a ser de:\n",
    "```\n",
    "PRECISION: 0.9220420834770611\n",
    "```\n",
    "Vemos que cuanto mayor sea el corpus de entrenamiento, obtendremos una mejor precisión.\n",
    "\n",
    "Vamos a evaluar el etiquetador del español que hemos entrenado y almacenado en el apartado anterior. Lo hacemos en el programa-5-18.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import codecs\n",
    "\n",
    "#carreguem l'etiquetador\n",
    "entrada=open('etiquetador-spa.pkl','rb')\n",
    "etiquetador=pickle.load(entrada)\n",
    "entrada.close()\n",
    "\n",
    "#carreguem les oracions del corpus de test\n",
    "\n",
    "entrada=codecs.open(\"fragmento-wikicorpus-tagged-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "tagged_words=[]\n",
    "test_tagged_sents=[]\n",
    "for linia in entrada:\n",
    "    linia=linia.rstrip()\n",
    "    if linia.startswith(\"<\") or len(linia)==0:\n",
    "        #nova linia\n",
    "        if len(tagged_words)>0:\n",
    "            test_tagged_sents.append(tagged_words)\n",
    "            tagged_words=[]\n",
    "    else:\n",
    "        camps=linia.split(\" \")\n",
    "        forma=camps[0]\n",
    "        lema=camps[1]\n",
    "        etiqueta=camps[2]\n",
    "        tupla=(forma,etiqueta)\n",
    "        tagged_words.append(tupla)\n",
    "\n",
    "precisio=etiquetador.evaluate(test_tagged_sents)\n",
    "print(\"PRECISION: \",precisio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos, utilizamos un fragmento nuevo de wikicorpus ya etiquetado (fragmento-wikicorpus-test-tagged-spa.txt). Primero cargamos el etiquetador almacenado y luego cargamos el nuevo corpus añadiéndolo al test_tagged_sents, que son las que utilizaremos para evaluar con el método evaluate. Este programa nos proporciona la siguiente salida:\n",
    "```\n",
    "PRECISION:  0.9938872748117409\n",
    "```\n",
    "Recordemos que el corpus que estamos usando es un corpus etiquetado automáticamente con Freeling, y por tanto, el 99.3% de precisión que obtenemos no es real. Deberíamos utilizar corpus etiquetados manualmente, o bien etiquetados automáticamente y revisados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
