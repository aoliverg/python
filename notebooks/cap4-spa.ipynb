{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Análisis textual y procesamiento de corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Acceso a los corpus de NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK se distribuye con una gran cantidad de corpus. A pesar de que una gran parte de estos corpus están en inglés, hay también en otras lenguas, cómo el castellano y el catalán. La lista completa y actualizada de los corpus disponibles al NLTK se puede encontrar en el siguiente enlace: http://www.nltk.org/nltk_data/ \n",
    "\n",
    "Por ejemplo, NLTK incluye una selección de textos del Proyecto Gutenberg. Este proyecto recopila libros en formato electrónico (libros en dominio público, es decir, sin derechos de autor y por tanto legalmente descargables). Si queremos cargar este corpus y observar los archivos que contiene, podemos ejecutar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: Si trabajas con Google Colab ejecuta esta celda\n",
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "libros=gutenberg.fileids()\n",
    "print(libros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder a las palabras de un determinado libro de la colección haciendo lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "mobi=nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "print(mobi)\n",
    "print(len(mobi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método words hemos podido acceder a las palabras del libro (que he almacenado a la variable mobi). En la última línea hemos contado las posiciones de la lista mobi, es decir, el número de palabras del libro. En corpus de texto cómo este podemos acceder a:\n",
    "\n",
    "- al texto en bruto, con el método raw\n",
    "- los párrafos, con el método paras\n",
    "- a las oraciones, con el método sents\n",
    "- a las palabras, con el método words\n",
    "\n",
    "En el programa siguiente (programa-4-1.py), podemos observar el funcionamiento de estos métodos (cuando lo ejecutes, para pasar de un nivel al siguiente, pulsa la tecla Enter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: Si trabajas con Google Colab ejecuta esta celda\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"TEXTO EN BRUTO\")\n",
    "a=input()\n",
    "\n",
    "texto_en_bruto=nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "print(texto_en_bruto[0:100000])\n",
    "\n",
    "print(\"PÁRRAFOS\")\n",
    "a=input()\n",
    "\n",
    "parrafos=nltk.corpus.gutenberg.paras(\"melville-moby_dick.txt\")\n",
    "for para in parrafos:\n",
    "    print(para)\n",
    "\n",
    "print(\"ORACIONES\")\n",
    "a=input()\n",
    "\n",
    "oraciones=nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")\n",
    "for oracion in oraciones:\n",
    "    print(oracion)\n",
    "\n",
    "print(\"PALABRAS\")\n",
    "a=input()\n",
    "\n",
    "palabras=nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "for palabra in palabras:\n",
    "    print(palabra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con a=input() el sistema espera que el usuario introduzca algún valor y lo almacena en la variable a (que no utilizamos para nada más). Lo que conseguimos es que el sistema se espere hasta que el usuario pulse la tecla Enter.\n",
    "\n",
    "Por ahora estamos trabajando con un corpus textual plano, sin ningún tipo de análisis ni anotación. Más adelante veremos que NLTK también proporciona corpus con varios niveles de análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Acceso a corpus propios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK proporciona una buena colección de corpus, pero con toda seguridad necesitaremos también trabajar con corpus propios. Para hacer esto podemos utilizar las instrucciones que ya conocemos para abrir y leer archivos de texto (mira el apartado 2.3 del capítulo 2), o bien podremos usar los diferentes lectores de corpus que proporciona NLTK. \n",
    "\n",
    "El lector de corpus más elemental que proporciona el NLTK es el PlaintextCorpusReader, que sirve, como indica su nombre, para leer corpus que estén en formato de texto plano (es decir, solo el texto, sin ningún tipo de anotación). En el programa-4-2.py se puede observar el funcionamiento básico. En este caso también usaremos la novela Moby Dick, descargada del Proyecto Gutenberg y que podemos encontrar en los ficheros de este capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB. \n",
    "#Te pedirá que selecciones el archivo mobi_dick.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "corpus = PlaintextCorpusReader(\".\", 'mobi_dick.txt')\n",
    "\n",
    "for oracion in corpus.sents():\n",
    "    print(oracion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y en la salida tendremos las oraciones, que de hecho, si te fijas, son listas de palabras:\n",
    "\n",
    "```\n",
    "['In', 'a', 'word', ',', 'the', 'whale', 'was', 'seized', 'and', 'sold', ',', 'and', 'his', 'Grace', 'the', 'Duke', 'of', 'Wellington', 'received', 'the', 'money', '.']\n",
    "```\n",
    "\n",
    "El PlaintextCorpusReader carga el fichero de texto y lleva a cabo un proceso de segmentación en párrafos (a partir de saltos de párrafos), en oraciones (usando un segmentador determinado) y en palabras (o tokens, usando un tokenizador determinado). Si no indicamos nada en el momento de llamar a PlaintextCorpusReader utiliza:\n",
    "\n",
    "* Tokenitzador: word_tokenizer=WordPunctTokenizer(),\n",
    "* Segmentador: sent_tokenizer=nltk.data.LazyLoader('tokenizers/punkt/english.pickle') Es decir, asume por defecto que la lengua del corpus es el inglés.\n",
    "\n",
    "Como que el tokenitzador que usa es muy simple y se basa en separar el texto en secuencias de caracteres alfabéticos y no alfabéticos usando la expresión regular \\w+|[^\\w\\s]+ se pueden producir errores, como el siguiente:\n",
    "\n",
    "```\n",
    "['\"', 'Won', \"'\", 't', 'the', 'Duke', 'be', 'content', 'with', 'a', 'quarter', 'or', 'a', 'half', '?\"']\n",
    "```\n",
    "\n",
    "\n",
    "donde won't se ha separado cómo \n",
    "\n",
    "won\n",
    "'\n",
    "t\n",
    "\n",
    "Fíjate ahora que si aplicamos este mismo programa a un corpus en castellano la tokenización no será totalmente correcta. Con este capítulo se distribuye un fragmento del Corpus del Diario Oficial de la Generalitat de Cataluña (corpus DOGC), concretamente el correspondiente a la versión castellana del año 2015. Si cargamos el fichero DOGC-2015-spa.txt, en vez de mobi_dick.txt obtenemos un resultado como el siguiente (mostramos únicamente un fragmento):\n",
    "\n",
    "``\n",
    "['El', 'pleno', 'del', 'Ayuntamiento', 'de', 'Òrrius', 'en', 'la', 'sesión', 'extraordinaria', 'celebrada', 'el', 'día', '20', 'de', 'enero', 'de', '2015', 'aprobó', 'inicialmente', 'el', 'proyecto', 'de', 'obra',...\n",
    "``\n",
    "\n",
    "Para el castellano el segmentador funciona bastante bien. Si probamos ahora con el mismo texto en catalán (DOGC-2015-cat.txt), veremos que se producen algunos problemas:\n",
    "\n",
    "```\n",
    "['El', 'ple', 'de', 'l', \"'\", 'Ajuntament', 'd', \"'\", 'Òrrius', 'a', 'la', 'sessió', 'extraordinària', 'celebrada', 'el', 'dia', '20', 'de', 'gener', 'de', '2015', 'va', 'aprovar', 'inicialment', 'el', 'projecte', 'd', \"'\", 'obra',...\n",
    "```\n",
    "\n",
    "Fijémonos que, por ejemplo, los apóstrofos (') quedan como tokens aislados, separados del artículo o preposición correspondiente.\n",
    "\n",
    "En este mismo capítulo, en los apartados 4.4 y 4.5, tratamos más a fondo la segmentación en unidades léxicas (tokenización) y la segmentación en oraciones. Lo que sí que avanzamos ahora es que al PlaintextCorpusReader se le puede indicar qué tokenizador y segmentador tiene que utilizar. Esto se puede hacer de la siguiente manera (programa-4-3.py)(necesitareis descargar el archivo catalan.pickle que tenéis en la sección de archivos de este capítulo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenizador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "for oracion in corpus.sents():\n",
    "    print(oracion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que hemos definido un segmentador (que cargamos de catalan.pickle) y un tokenizador basado en expresiones regulares, y que usamos estos nuevos elementos cuando creamos el PlaintextCorpusReader. Ahora hemos arreglado el aspecto de los apóstrofos en la tokenización y la salida (mostramos solo un fragmento) es:\n",
    "\n",
    "```\n",
    "['El', 'ple', 'de', \"l'\", 'Ajuntament', \"d'\", 'Òrrius', 'a', 'la', 'sessió', 'extraordinària', 'celebrada', 'el', 'dia', '20', 'de', 'gener', 'de', '2015', 'va', 'aprovar', 'inicialment', 'el', 'projecte', \"d'\", 'obra', '.']\n",
    "```\n",
    "\n",
    "En el apartado 4.4 veremos con más detalle el tema de las expresiones regulares. Comparamos ahora las expresiones regulares del tokenizador por defecto:\n",
    "\n",
    "```\n",
    "\\w+|[^\\w\\s]+\n",
    "```\n",
    "\n",
    "y el que  hemos creado nosotros:\n",
    "\n",
    "```\n",
    "[ldsmLDSM]\\'|\\w+|[^\\w\\s]+\n",
    "```\n",
    "\n",
    "Hemos añadido la parte [ldsmLDSM]\\' que define los tokens formados por: l', d', s', m', L', D', S', M'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Ocurrencias (tokens) y tipos (types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificaremos ligeramente el programa-4-3.py, para que nos dé todas las palabras (o tokens) en vez de todas las oraciones y para que nos proporcione el número total de palabras (programa-4-4.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenizador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "for paraula in corpus.words():\n",
    "    print(paraula)\n",
    "print(\"TOTAL PALABRAS:\",len(corpus.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa está un buen rato mostrando palabras por pantalla y después nos da el número de palabras totales:\n",
    "\n",
    "...\n",
    "5524\n",
    ",\n",
    "de \n",
    "11\n",
    ".\n",
    "12\n",
    ".\n",
    "2009\n",
    "),\n",
    "TOTAL PALABRAS: 2448870\n",
    "\n",
    "Si miramos con detenimiento la lista veremos que muchas de las palabras ocurren más de una vez (en el apartado 4.6 veremos como calcular la frecuencia y la distribución de frecuencias de las palabras). Si ahora lo que queremos es obtener una lista de palabras diferentes, podemos utilizar la instrucción set, como podemos observar en el programa-4-5.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB. \n",
    "#Te pedirá que selecciones el archivo catalan.pickle de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenizador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "\n",
    "ocurrencies=corpus.words()\n",
    "tipus=set(ocurrencies)\n",
    "\n",
    "print(\"OCURRENCIAS:\",len(ocurrencies))\n",
    "print(\"TIPOS:\",len(tipus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nos da a la salida:\n",
    "\n",
    "OCURRENCIAS: 2448870\n",
    "TIPOS: 34872\n",
    "\n",
    "Para resumir podemos decir que las ocurrencias son el número total de palabras que aparecen en el corpus y los tipos el número de palabras diferentes que aparecen en el corpus.\n",
    "\n",
    "Hay que tener en cuenta, no obstante, que no podemos hablar estrictamente de palabras, puesto que también se incluyen los signos de puntuación, las cifras, etc.\n",
    "\n",
    "Podemos calcular un índice de riqueza léxica dividiendo el número de ocurrencias entre el número de tipos. Lo vemos en el programa-4-6.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"catalan.pickle\")\n",
    "tokenizador=RegexpTokenizer('[ldsmLDSM]\\'|\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-cat.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "\n",
    "ocurrencias=corpus.words()\n",
    "tipos=set(ocurrencies)\n",
    "riquezalexica=len(ocurrencies)/len(tipus)\n",
    "\n",
    "print(\"OCURRENCIAS:\",len(ocurrencias))\n",
    "print(\"TIPUS:\",len(tipos))\n",
    "print(\"RIQUEZA LÊXICA:\",round(riquezalexica,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nos da a la salida:\n",
    "\n",
    "OCURRENCIAS: 2448870\n",
    "TIPOS: 34872\n",
    "RIQUEZA LÉXICA: 70.22\n",
    "\n",
    "Lo que indica que cada palabra de media se utiliza 70 veces. Experimenta un poco con diferentes tipos de texto para ver cómo varía este índice de riqueza léxica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segmentación en unidades léxicas o tokenización, consiste en dividir el texto en unidades más pequeñas (que a menudo coinciden con palabras). A pesar de que se trata de una tarea muy básica y necesaria para poder llevar a cabo tareas de análisis más avanzadas, esta tarea presenta numerosos problemas que no son fáciles de solucionar. Hay numerosos trabajos sobre esta área entre los cuales se pueden destacar los trabajos de Grefenstette y Tapanainen (1994) y Mikheev (2002). Sea como sea, actualmente se puede considerar que esta tarea se resuelve de manera satisfactoria y no hay una investigación activa para mejorarla. En este apartado aprenderemos algunas técnicas para segmentar el texto en unidades léxicas e iremos observando los diferentes problemas que aparecen y cómo se pueden solucionar. Las pruebas de los diferentes sistemas las haremos con la oración:\n",
    "\n",
    "El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\n",
    "\n",
    "A pesar de que el resultado deseado de la tokenització puede depender de las tareas concretas, la salida deseada de nuestro sistema tendría que ser algo del siguiente estilo:\n",
    "\n",
    "```\n",
    "['El','Sr.','Martínez','llegará','mañana','de ','Alicante','con','la','R.E.N.F.E','a','las','22.30','h.','y','se ','alojará','en','el ','hotel','de',' la','estación',.']\n",
    "```\n",
    "\n",
    "\n",
    "El primer tokenizador que probaremos utiliza la instrucción split(), que divide una cadena según el separador que se indique, y si no se indica nada, por espacios. Lo podemos ver en el programa-4-7.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "\n",
    "tokens=oracion.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que nos da la siguiente salida, que no es exactamente la que queríamos:\n",
    "\n",
    "```\n",
    "['El', 'Sr.', 'Martínez', 'llegará', 'mañana', 'de', 'Alicante', 'con', 'la', 'R.E.N.F.E.', 'a', 'las', '22.30', 'h.', 'y', 'se', 'alojará', 'en', 'el', 'hotel', 'de', 'la', 'estación.']\n",
    "```\n",
    "\n",
    "NLTK proporciona una serie de tokenitzadores que presentamos a continuación:\n",
    "\n",
    "### WhitespaceTokenizer\n",
    "\n",
    "Separa el texto por espacios en blanco, como lo hemos hecho en el ejemplo anterior. En este caso los espacios en blanco pueden ser los caracteres: espacio en blanco, tabulador y nueva línea. Lo podemos ver en el programa-4-8.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokens=nltk.tokenize.WhitespaceTokenizer().tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que da como salida:\n",
    "\n",
    "``` \n",
    "['El', 'Sr.', 'Martínez', 'llegará', 'mañana', 'de', 'Alicante', 'con', 'la', 'R.E.N.F.E.', 'a', 'las', '22.30', 'h.', 'y', 'se', 'alojará', 'en', 'el', 'hotel', 'de', 'la', 'estación.']\n",
    "```\n",
    "\n",
    "Aprovecho este ejemplo para explicar varias maneras de importar un tokenizador, o cualquier método de una clase determinada. En el caso anterior hemos importado todo el nltk y hemos llamado al método haciendo:\n",
    "\n",
    "```\n",
    "tokens=nltk.tokenize.WhitespaceTokenizer().tokenize(oracion)\n",
    "```\n",
    "\n",
    "Esto también se puede hacer de la siguiente manera alternativa (programa-4-8b.py) (fíjate como accedemos ahora al método tokenize):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokens=WhitespaceTokenizer().tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y también de la siguiente manera, dando un nombre cualquiera al método (programa-4-8c.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer as tokenitzador\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokens=tokenitzador().tokenize(oracio)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceTokenizer\n",
    "\n",
    "Es igual que el anterior, pero en este caso el único carácter que se tiene en cuenta es el de espacio en blanco (\" \"). Equivale a split(\" \"). En nuestro ejemplo la salida seria exactamente la misma por lo que no es necesario proporcionar ni el código ni el programa.\n",
    "\n",
    "\n",
    "### TreebankWordTokenizzer\n",
    "\n",
    "Este tokenizador utiliza las convenciones del Penn Treebank corpus, qué es un corpus anotado del inglés creado en 1980 a partir de artículos del Wall Street Journal. Cómo que es para el inglés, no funcionará del todo bien para otras lengua y por este motivo en el ejemplo pongo una oración del inglés (programa-4-9.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as tokenitzador\n",
    "\n",
    "oracion=\"We need to conduct an assessment to learn whether a student's dificulties are because he or she can't or won't complete assignments.\"\n",
    "tokens=tokenitzador().tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y la salida será:\n",
    "\n",
    "``` \n",
    "['We', 'need', 'to', 'conduct', 'an', 'assessment', 'to', 'learn', 'whether', 'a', 'student', \"'s\", 'dificulties', 'are', 'because', 'he', 'or', 'she', 'ca', \"n't\", 'or', 'wo', \"n't\", 'complete', 'assignments', '.']\n",
    "``` \n",
    "\n",
    "Este tokenitzador se usa bastante para el inglés, y por este motivo se ha creado una función específica que hace de wrapper (wrapper function) para simplificar su uso (programa-4-9b.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "oracion=\"We need to conduct an assessment to learn whether a student's dificulties are because he or she can't or won't complete assignments.\"\n",
    "tokens=word_tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que proporciona exactamente la misma salida.\n",
    "\n",
    "\n",
    "### RegexpTokenizer\n",
    "\n",
    "En el apartado anterior (4.3. Ocurrencias (tokens) y tipos (types)) ya vimos un ejemplo del tokenizador por expresiones regulares. Este es un tipo de tokenizador que nos permite un control total sobre el proceso de tokenización. Claro que para sacar el máximo provecho hay que dominar las expresiones regulares de Python. Podéis encontrar una explicación detallada en Regular Expression HOWTO y también un buen resumen a Pyschools.com (http://doc.pyschools.com/html/regex.html). Otro buen recurso para trabajar con expresiones regulares es la web http://regexr.com/.  \n",
    "\n",
    "Veamos ahora algunos ejemplos, empezando por el programa-4-10.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokenizador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "tokens=tokenizador.tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nos ofrece la siguiente salida:\n",
    "\n",
    "```\n",
    "['El', 'Sr', '.', 'Martínez', 'llegará', 'mañana', 'de', 'Alicante', 'con', 'la', 'R', '.', 'E', '.', 'N', '.', 'F', '.', 'E', '.', 'a', 'las', '22', '.', '30', 'h', '.', 'y', 'se', 'alojará', 'en', 'el', 'hotel', 'de', 'la', 'estación', '.']\n",
    "```\n",
    "\n",
    "Como podemos observar, nos separa Sr del punto (.) y realmente querríamos tener Sr. como token, y lo mismo pasa con h.. También tokeniza incorrectamente R.E.N.F.E. Podríamos solucionar esto modificando la expresión regular (programa-4-10b.py) de forma que añadimos estas dos unidades como tokens.\n",
    "\n",
    "tokenizador=RegexpTokenizer('Sr\\.|h\\.|R\\.E\\.N.\\F\\.E\\.|\\w+|[^\\w\\s]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokenizador=RegexpTokenizer('Sr\\.|h\\.|R\\.E\\.N\\.F\\.E\\.|\\w+|[^\\w\\s]+')\n",
    "tokens=tokenizador.tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que nos devolvería la tokenización correcta de estas dos unidades:\n",
    "\n",
    "```\n",
    "['El', 'Sr.', 'Martínez', 'llegará', 'mañana', 'de', 'Alicante', 'con', 'la', 'R.E.N.F.E.', 'a', 'las', '22', '.', '30', 'h.', 'y', 'se', 'alojará', 'en', 'el', 'hotel', 'de', 'la', 'estación', '.']\n",
    "```\n",
    "\n",
    "El hecho de añadir una lista de abreviaturas frecuentes es bastante habitual, pero no podemos esperar tener una lista suficiente completa de acrónimos. Por este motivo, tenemos que intentar expresar los acrónimos de una manera más general, cómo por ejemplo la que presentamos al programa-4-10c.py\n",
    "\n",
    "``` \n",
    "tokenizador=RegexpTokenizer('Sr\\.|h\\.|([A-Z]\\.){1,}|\\w+|[^\\w\\s]+')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokenizador=RegexpTokenizer('Sr\\.|h\\.|[A-Z\\.]{2,}|\\w+|[^\\w\\s]+')\n",
    "tokens=tokenizador.tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "que nos ofrece la misma salida pero que ahora contempla cualquier acrónimo tipo R.E.N.F.E.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "oracion=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.\"\n",
    "tokenizador=RegexpTokenizer('\\s+',gaps=True)\n",
    "tokens=tokenizador.tokenize(oracion)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que da la siguiente salida (que coincide con la del tokenizador por espacios en blanco):\n",
    "\n",
    "```\n",
    "['El', 'Sr.', 'Martínez', 'llegará', 'mañana', 'de', 'Alicante', 'con', 'la', 'R.E.N.F.E.', 'a', 'las', '22.30', 'h.', 'y', 'se', 'alojará', 'en', 'el', 'hotel', 'de', 'la', 'estación.']\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Segmentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sección anterior hemos visto como separar el texto en unidad léxicas, proceso que también recibe el nombre de tokenización. En esta sección veremos cómo podemos separar un párrafo en segmentos, que son unidades parecidas a oraciones. Este proceso recibe el nombre de segmentación.\n",
    "\n",
    "En esta sección usaremos dos segmentos de prueba, uno que no presentará grandes problemas:\n",
    "\n",
    "Hoy hace un día muy bonito. Mañana Albert irá a comer en casa.\n",
    "\n",
    "y otro que sí que presenta numerosos problemas.\n",
    "\n",
    "El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación. El día siguiente visitará al Dr. Rovira a la Avda. Tibidabo. La vuelta la hará en avión en el vuelo AF.352.\n",
    "\n",
    "### PunktSentenceTokenizer\n",
    "\n",
    "Este es un segmentador sencillo que básicamente segmenta por puntos. Para ver cómo funciona probamos el programa-4-11.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "parrafo1=\"Hoy hace un día muy bonito. Mañana Albert irá a comer en casa.\"\n",
    "\n",
    "parrafo2=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación. El día siguiente visitará al Dr. Rovira en la Avda. Tibidabo. La vuelta la hará en avión en el vuelo AF.352.\"\n",
    "\n",
    "segmentador=PunktSentenceTokenizer()\n",
    "segmentos1=segmentador.tokenize(parrafo1)\n",
    "print(segmentos1)\n",
    "segmentos2=segmentador.tokenize(parrafo2)\n",
    "print(segmentos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que proporciona la siguiente salida:\n",
    "\n",
    "```\n",
    "['Hoy hace un día muy bonito.', 'Mañana Albert irá a comer en casa.']\n",
    "['El Sr.', 'Martínez llegará mañana de Alicante con la R.E.N.F.E.', 'a las 22.30 h. y se alojará en el hotel de la estación.', 'El día siguiente visitará al Dr.', 'Rovira a la Avda.', 'Tibidabo.', 'La vuelta la hará en avión en el vuelo AF.352.']\n",
    "```\n",
    "\n",
    "### sent_tokenize()\n",
    "\n",
    "El proceso de segmentación también se puede hacer con sent_tokenizer(), que llama a una instancia especial del PunktSenteceTokenizer que ha sido entrenada y que funciona bastante bien para varias lenguas europeas. Se trata de una implementación del algoritmo de Kiss and Strunk (2006). Lo podemos ver en el programa-4-11b.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "parrafo1=\"Hoy hace un día muy bonito. Mañana Albert irá a comer en casa.\"\n",
    "\n",
    "parrafo2=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación. El día siguiente visitará al Dr. Rovira en la Avda. Tibidabo. La vuelta la hará en avión en el vuelo AF.352.\"\n",
    "\n",
    "segmentos1=sent_tokenize(parrafo1)\n",
    "print(segmentos1)\n",
    "segmentos2=sent_tokenize(parrafo2)\n",
    "print(segmentos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nos ofrece una salida mucho mejor, a pesar de que no perfecta:\n",
    "\n",
    "```\n",
    "['Hoy hace un día muy bonito.', 'Mañana Albert irá a comer en casa.']\n",
    "['El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E.', 'a las 22.30 h. y se alojará en el hotel de la estación.', 'El día siguiente visitará al Dr. Rovira en la Avda.', 'Tibidabo.', 'La vuelta la hará en avión en el vuelo AF.352.']\n",
    "```\n",
    "\n",
    "### Cargar un segmentador concreto para una lengua determinada\n",
    "\n",
    "Con los datos del NLTK se distribuyen una serie de segmentadores para lenguas determinadas. Concretamente se distribuyen los siguientes: checo, finlandés, noruego, español, danés, francés, polaco, sueco, holandés, alemán, portugués, turco, inglés, griego, estonio, italiano, esloveno. No se distribuye uno para el catalán, pero en esta misma sección aprenderemos a crear uno específico para el catalán. En el programa-4-12.py cargamos un de específico para el inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "parrafo=\"Today Mr. Smith and Ms. Johanson will meet at St. Patrick church.\"\n",
    "\n",
    "segmentador=nltk.data.load(\"tokenizers/punkt/PY3/english.pickle\")\n",
    "segmentos=segmentador.tokenize(parrafo)\n",
    "print(segmentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que proporciona la salida:\n",
    "\n",
    "```\n",
    "['Today Mr. Smith and Ms. Johanson will meet at St. Patrick church.']\n",
    "```\n",
    "\n",
    "### Entrenamiento de un segmentador\n",
    "\n",
    "El algoritmo de Kiss and Strunk (2006) permite entrenar un segmentador a partir de texto sin ningún tipo de anotación. NLTK implementa este algoritmo. Vamos a entrenar un segmentador para el castellano (de hecho crear un spanish.pickle, aunque ya se distribuye uno con NLTK) a partir del corpus de DOGC correspondiente al año 2015. El programa-4-13.py implementa este aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize.punkt\n",
    "import pickle\n",
    "import codecs\n",
    "segmentador = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "texto = codecs.open(\"DOGC-2015-spa.txt\",\"r\",\"utf8\").read()\n",
    "segmentador.train(texto)\n",
    "out = open(\"spanish.pickle\",\"wb\")\n",
    "pickle.dump(segmentador, out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segmentador almacenado en spanish.pickle lo podemos utilizar programa-4-12b.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "segmentador=nltk.data.load(\"spanish.pickle\")\n",
    "\n",
    "parrafo1=\"Hoy hace un día muy bonito. Mañana Albert irá a comer en casa.\"\n",
    "\n",
    "parrafo2=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación. El día siguiente visitará al Dr. Rovira en la Avda. Tibidabo. La vuelta la hará en avión en el vuelo AF.352.\"\n",
    "\n",
    "segmentos1=segmentador.tokenize(parrafo1)\n",
    "print(segmentos1)\n",
    "segmentos2=segmentador.tokenize(parrafo2)\n",
    "print(segmentos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que ofrece la siguiente salida:\n",
    "\n",
    "``` \n",
    "['Hoy hace un día muy bonito.', 'Mañana Albert irá a comer en casa.']\n",
    "['El Sr.', 'Martínez llegará mañana de Alicante con la R.E.N.F.E.', 'a las 22.30 h. y se alojará en el hotel de la estación.', 'El día siguiente visitará al Dr.', 'Rovira en la Avda.', 'Tibidabo.', 'La vuelta la hará en avión en el vuelo AF.352.']\n",
    "```\n",
    "\n",
    "¿Funciona del todo correctamente el segmentador que hemos entrenado?\n",
    "\n",
    "#### Personalitzar el segmentador\n",
    "\n",
    "Podemos personalizar el segmentador entrenado para añadir nuevas abreviaturas o acrónimos que no han sido detectados en el proceso de entrenamiento. Lo podemos hacer específicamente para un programa determinado, como en el programa-4-12c.py, donde cargamos el spanish.pickle que hemos entrenado y añadimos R.E.N.F.E. (fíjate que lo hacemos en minúsculas y sin el punto final):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "segmentador=nltk.data.load(\"spanish.pickle\")\n",
    "abreviaturas_extra = ['r.e.n.f.e']\n",
    "segmentador._params.abbrev_types.update(abreviaturas_extra)\n",
    "\n",
    "parrafo1=\"Hoy hace un día muy bonito. Mañana Albert irá a comer en casa.\"\n",
    "\n",
    "parrafo2=\"El Sr. Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación. El día siguiente visitará al Dr. Rovira en la Avda. Tibidabo. La vuelta la hará en avión en el vuelo AF.352.\"\n",
    "\n",
    "segmentos1=segmentador.tokenize(parrafo1)\n",
    "print(segmentos1)\n",
    "segmentos2=segmentador.tokenize(parrafo2)\n",
    "print(segmentos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora la salida ya está mejor segmentada:\n",
    "\n",
    "```\n",
    "['El Sr.', 'Martínez llegará mañana de Alicante con la R.E.N.F.E. a las 22.30 h. y se alojará en el hotel de la estación.', 'El día siguiente visitará al Dr.', 'Rovira en la Avda.', 'Tibidabo.', 'La vuelta la hará en avión en el vuelo AF.352.']\n",
    "```\n",
    "\n",
    "En los ficheros adjuntos puedes encontrar una lista de abreviaciones. Ahora lo que haremos será cargar el spanish.pickle que hemos entrenado y modificarlo añadiendo la lista de abreviaturas y acrónimos del fichero. Grabaremos este nuevo segmentador como spanish-mod.pickle. (programa-4-14.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: EJECUTA ESTA CELDA SOLO EN EL CASO QUE ESTÉS TRABAJANDO CON GOOGLE COLAB. \n",
    "#Te pedirá que selecciones el archivo abr-spa.txt de tu ordenador y lo subirá al entorno de Google Colab.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import codecs\n",
    "import pickle\n",
    "segmentador=nltk.data.load(\"spanish.pickle\")\n",
    "archivo_abreviaturas=codecs.open(\"abr-spa.txt\",\"r\",encoding=\"utf-8\")\n",
    "abreviaturas_extra =[]\n",
    "for abreviatura in archivo_abreviaturas.readlines():\n",
    "    abreviatura=abreviatura.rstrip()\n",
    "    abreviaturas_extra.append(abreviatura)\n",
    "segmentador._params.abbrev_types.update(abreviaturas_extra)\n",
    "out = open(\"spanish-mod.pickle\",\"wb\")\n",
    "pickle.dump(segmentador, out)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifica el programa-4-12b.py para que cargue este nuevo segmentador y verifica si funciona correctamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Frecuencias y distribución de frecuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado aprenderemos a hacer algunos cálculos sencillos sobre corpus: frecuencias absolutas y frecuencias relativas, distribuciones de frecuencia y a encontrar las colocaciones más frecuentes de un corpus.\n",
    "\n",
    "### 4.6.1. Frecuencia absoluta\n",
    "\n",
    "Entendemos por frecuencia absoluta el número total de veces que aparece una determinada unidad léxica en nuestro corpus.\n",
    "\n",
    "El cálculo de la frecuencia absoluta de una palabra es sencillo: podemos utilizar un diccionario para poner como clave las palabras e ir incrementando el valor del diccionario cada vez que aparece la palabra. En el siguiente programa (programa-4-15.py) podemos ver una implementación sencilla de esta idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "segmentador= nltk.data.load(\"spanish-mod.pickle\")\n",
    "tokenizador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-spa.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "\n",
    "frecuencia={}\n",
    "\n",
    "for palabra in corpus.words():\n",
    "    frecuencia[palabra]=frecuencia.get(palabra,0)+1\n",
    "\n",
    "for clave in frecuencia.keys():\n",
    "    print(frecuencia[clave],clave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y por pantalla nos mostrará las palabras y las frecuencias, pero de una manera desordenada, puesto que los diccionarios de Python son estructuras de datos sin un orden determinado.\n",
    "\n",
    "NLTK proporciona una función FreqDist que facilita mucho el cálculo de frecuencias. Veamos su uso en el programa-4-16.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "segmentador= nltk.data.load(\"spanish-mod.pickle\")\n",
    "tokenizador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-spa.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "\n",
    "frequencia=FreqDist(corpus.words())\n",
    "\n",
    "for mc in frequencia.most_common():\n",
    "    print(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí que nos proporciona las palabras y la frecuencia de cada palabra ordenada de más frecuente a menos frecuente. Cómo que hay muchas, podemos modificar fácilmente el programa porque nos proporcione las 25 más frecuentes, cambiando la línea:\n",
    "\n",
    "`for mc in frequencia.most_common():`\n",
    "\n",
    "por\n",
    "\n",
    "`for mc in frequencia.most_common(25):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "segmentador= nltk.data.load(\"spanish-mod.pickle\")\n",
    "tokenizador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-spa.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "\n",
    "frequencia=FreqDist(corpus.words())\n",
    "\n",
    "for mc in frequencia.most_common(25):\n",
    "    print(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2. Frecuencia relativa\n",
    "\n",
    "La frecuencia absoluta de una palabra en un determinado corpus no nos da información real sobre si la palabra es muy frecuente o no, porque esto dependerá del tamaño del corpus. Que una palabra aparezca, digamos, 22 veces en nuestro corpus, no nos dice nada, puesto que si el corpus es muy grande quizás este valor de frecuencia sea pequeño.\n",
    "\n",
    "La frecuencia relativa de una palabra en un corpus es el número a veces que aparece dividida por el número total de palabras en el corpus.\n",
    "\n",
    "FreqDist nos facilita mucho el cálculo de la frecuencia relativa puesto que la podemos consultar con el método freq(). Lo podemos ver en el programa-4-17.py, que es una modificación del anterior. Ahora guardamos en el fichero frequencias.txt las palabras ordenadas por frecuencia y mostramos la frecuencia absoluta y la relativa de cada palabra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import FreqDist\n",
    "import codecs\n",
    "\n",
    "segmentador= nltk.data.load(\"spanish-mod.pickle\")\n",
    "tokenizador=RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "\n",
    "corpus = PlaintextCorpusReader(\".\", 'DOGC-2015-spa.txt',word_tokenizer=tokenizador,sent_tokenizer=segmentador)\n",
    "\n",
    "frecuencia=FreqDist(corpus.words())\n",
    "\n",
    "salida=codecs.open(\"frecuencias.txt\",\"w\",encoding=\"utf-8\")\n",
    "\n",
    "for mc in frecuencia.most_common():\n",
    "    palabra=mc[0]\n",
    "    frecuencia_absoluta=mc[1]\n",
    "    frecuencia_relativa=frecuencia.freq(palabra)\n",
    "    cadena=str(frecuencia_absoluta)+\"\\t\"+str(frecuencia_relativa)+\"\\t\"+palabra\n",
    "    print(cadena)\n",
    "    salida.write(cadena+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la salida (mostramos solo las 25 primeras):\n",
    "\n",
    "\n",
    "1779837\t0.07246859684771821\tde\n",
    "\n",
    "1031818\t0.042011938543933466\t/\n",
    "\n",
    "986574\t0.040169764684317016\t,\n",
    "\n",
    "921036\t0.03750129172853188\t.\n",
    "668298\t0.027210704315134695\tla\n",
    "\n",
    "421836\t0.017175653174899757\ty\n",
    "\n",
    "394394\t0.016058313084377378\t:\n",
    "\n",
    "386949\t0.015755179312278437\tel\n",
    "\n",
    "377317\t0.015362998722237202\t-\n",
    "\n",
    "349183\t0.014217482866732621\ten\n",
    "\n",
    "302520\t0.012317532402333312\tdel\n",
    "\n",
    "290548\t0.011830075381571926\t2015\n",
    "\n",
    "278339\t0.011332968568468372\tque\n",
    "\n",
    "242310\t0.009865996550341745\ta\n",
    "\n",
    "217715\t0.008864576117195547\tlos\n",
    "\n",
    "200890\t0.00817952229374831\tlas\n",
    "\n",
    "189023\t0.007696340497442317\t0\n",
    "\n",
    "181949\t0.007408312518419092\tse\n",
    "\n",
    "136654\t0.005564062121210024\t08\n",
    "\n",
    "136191\t0.005545210417182917\tCIR\n",
    "\n",
    "134874\t0.005491586887585294\t1\n",
    "\n",
    "124829\t0.005082590414686186\tpor\n",
    "\n",
    "122338\t0.004981165804034949\t)\n",
    "\n",
    "119835\t0.004879252596303095\t2\n",
    "\n",
    "114850\t0.004676281225730467\tcon\n",
    "\n",
    "\n",
    "### 4.6.3. La ley de Zipf\n",
    "\n",
    "La Ley de Zipf afirma que dado un corpus, la frecuencia de una palabra es inversamente proporcional a su posición a la tabla de frecuencias (rank).\n",
    "\n",
    "Con su ley, Zipf (1949) afirma que hay una constante k que se puede calcular multiplicando la frecuencia de cualquier palabra por su posición en la tabla (rank) (k = f · r).\n",
    "\n",
    "En el programa siguiente (programa-4-18.py) evaluamos la ley de Zipf con las 50 palabras más frecuentes del corpus Cess_esp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATENCIÓN: Si trabajas con Google Colab ejecuta esta celda\n",
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "palabras=cess_esp.words()\n",
    "frecdist=nltk.FreqDist(palabras)\n",
    "salida=codecs.open(\"salida.txt\",\"w\", encoding=\"utf-8\")\n",
    "\n",
    "posicion=0\n",
    "p = re.compile('\\w+')\n",
    "for mc in frecdist.most_common(50):\n",
    "    if p.match(mc[0]):\n",
    "        posicion+=1\n",
    "        frec=mc[1]\n",
    "        fxr=posicion*frec\n",
    "        cadena=mc[0]+\"\\t\"+str(frec)+\"\\t\"+str(posicion)+\"\\t\"+str(fxr)\n",
    "        print(cadena)\n",
    "        salida.write(cadena+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el fichero salida.txt podemos observar las palabras con su frecuencia, posición y resultado del producto de la frecuencia y la posición (que tiende a ser constante):\n",
    "\n",
    "de\t10234\t1\t10234\n",
    "\n",
    "la\t6412\t2\t12824\n",
    "\n",
    "que\t5552\t3\t16656\n",
    "\n",
    "el\t5199\t4\t20796\n",
    "\n",
    "en\t4340\t5\t21700\n",
    "\n",
    "y\t4235\t6\t25410\n",
    "\n",
    "los\t2963\t7\t20741\n",
    "\n",
    "a\t2953\t8\t23624\n",
    "\n",
    "del\t2257\t9\t20313\n",
    "\n",
    "se\t1884\t10\t18840\n",
    "\n",
    "las\t1832\t11\t20152\n",
    "\n",
    "un\t1815\t12\t21780\n",
    "\n",
    "con\t1494\t13\t19422\n",
    "\n",
    "por\t1456\t14\t20384\n",
    "\n",
    "una\t1396\t15\t20940\n",
    "\n",
    "su\t1291\t16\t20656\n",
    "\n",
    "para\t1258\t17\t21386\n",
    "\n",
    "no\t1232\t18\t22176\n",
    "\n",
    "al\t984\t19\t18696\n",
    "\n",
    "es\t911\t20\t18220\n",
    "\n",
    "El\t812\t21\t17052\n",
    "\n",
    "ha\t705\t22\t15510\n",
    "\n",
    "como\t696\t23\t16008\n",
    "\n",
    "lo\t652\t24\t15648\n",
    "\n",
    "más\t648\t25\t16200\n",
    "\n",
    "La\t510\t26\t13260\n",
    "\n",
    "sus\t493\t27\t13311\n",
    "\n",
    "o\t435\t28\t12180\n",
    "\n",
    "pero\t357\t29\t10353\n",
    "\n",
    "hoy\t348\t30\t10440\n",
    "\n",
    "entre\t315\t31\t9765\n",
    "\n",
    "dos\t306\t32\t9792\n",
    "\n",
    "sobre\t303\t33\t9999\n",
    "\n",
    "En\t301\t34\t10234\n",
    "\n",
    "le\t300\t35\t10500\n",
    "\n",
    "años\t291\t36\t10476\n",
    "\n",
    "este\t278\t37\t10286\n",
    "\n",
    "han\t271\t38\t10298\n",
    "\n",
    "también\t254\t39\t9906\n",
    "\n",
    "fue\t244\t40\t9760\n",
    "\n",
    "si\t227\t41\t9307\n",
    "\n",
    "Los\t226\t42\t9492\n",
    "\n",
    "A pesar de que la ley solo muestra una tendencia y no es exacta, recalca el hecho que en un corpus hay muy pocas palabras muy frecuentes y muchas palabras poco frecuentes. En el programa siguiente (programa-4-19.py) graficamos este fenómeno con un subconjunto del corpus Cess_esp de 1.000 palabras. Cómo podemos observar al gráfico el principio mencionado se cumple, es decir, que muy pocas palabras aparecen muchas veces y que muchas aparecen pocas veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "import pylab\n",
    "palabras=cess_esp.words()\n",
    "palabras1=palabras[:1000]\n",
    "freqdist=nltk.FreqDist(palabras1)\n",
    "freqdist.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
